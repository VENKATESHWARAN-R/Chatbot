{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required modules.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the required data files\n",
    "# Reading the movie details meta data\n",
    "with open('./data/movie_titles_metadata.txt', 'r', encoding='utf-8', errors='ignore') as mtm:\n",
    "    movie_titles = mtm.read().split('\\n')\n",
    "\n",
    "# Reading the conversation meta data\n",
    "with open('./data/movie_conversations.txt', 'r', encoding='utf-8', errors='ignore') as mc:\n",
    "    movie_conversations = mc.read().split('\\n')\n",
    "\n",
    "# Reading the conversation lines\n",
    "with open('./data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore') as ml:\n",
    "    movie_lines = ml.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dictionary for all data \n",
    "# Prepare dictionary for movie meta data\n",
    "movie_title_list = []\n",
    "for line in movie_titles:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_title_info = {}\n",
    "    movie_info = line.split(' +++$+++ ')\n",
    "    movie_title_info['movie_id'] = movie_info[0].strip()\n",
    "    movie_title_info['name'] = movie_info[1].strip()\n",
    "    movie_title_info['year'] = movie_info[2].strip()\n",
    "    movie_title_info['rating'] = movie_info[3].strip()\n",
    "    movie_title_info['genre'] = movie_info[-1][2:-2].strip().split(\"', '\") # this is for splitting the genres from ['comedy', 'romance'] to a list\n",
    "    movie_title_list.append(movie_title_info)\n",
    "\n",
    "# Prepare dictionary for movie convo meta data\n",
    "movie_conversation_list = []\n",
    "for line in movie_conversations:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_conversation_info = {}\n",
    "    conversation_info = line.split(' +++$+++ ')\n",
    "    movie_conversation_info['speaker1'] = conversation_info[0].strip()\n",
    "    movie_conversation_info['speaker2'] = conversation_info[1].strip()\n",
    "    movie_conversation_info['movie_id'] = conversation_info[2].strip()\n",
    "    movie_conversation_info['line_ids'] = conversation_info[-1][2:-2].strip().split(\"', '\")# this is for splitting the conversation info from ['L198', 'L199'] to a list\n",
    "    movie_conversation_list.append(movie_conversation_info)\n",
    "\n",
    "# Prepare dictionary for movie dialogues\n",
    "movie_lines_list = []\n",
    "for line in movie_lines:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_line_info = {}\n",
    "    line_info = line.split(' +++$+++ ')\n",
    "    movie_line_info['line_id'] = line_info[0].strip()\n",
    "    movie_line_info['speaker'] = line_info[1].strip()\n",
    "    movie_line_info['movie_id'] = line_info[2].strip()\n",
    "    movie_line_info['character'] = line_info[3].strip()\n",
    "    movie_line_info['dialogue'] = line_info[-1].strip()\n",
    "    movie_lines_list.append(movie_line_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for all the above dicts for better processing\n",
    "movie_title_df = pd.DataFrame.from_dict(movie_title_list)\n",
    "movie_conversation_df = pd.DataFrame.from_dict(movie_conversation_list)\n",
    "movie_lines_df = pd.DataFrame.from_dict(movie_lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available genres from the whole dataset \n",
    "genres = movie_title_df['genre'].to_numpy()\n",
    "genre_set = set()\n",
    "for genre_list in genres:\n",
    "    for genre in genre_list:\n",
    "        if genre:\n",
    "            genre_set.add(genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count of movies in each genres and storing the movies with respect to their genres in the dictionary\n",
    "genre_dict = {}\n",
    "for genre_name in genre_set:\n",
    "    genre_dict[genre_name] = []\n",
    "for movie, genre_list in movie_title_df[['movie_id', 'genre']].to_numpy():\n",
    "    for genre in genre_list:\n",
    "        if genre:\n",
    "            genre_dict[genre].append(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make conversation line dictionary for preparing the final dataset\n",
    "dialogue_ids = movie_lines_df['line_id'].to_numpy()\n",
    "dialogue_lines = movie_lines_df['dialogue'].to_numpy()\n",
    "dialogue_dict = {}\n",
    "for dialogue_id, dialogue_line in zip(dialogue_ids, dialogue_lines):\n",
    "    dialogue_dict[dialogue_id] = dialogue_line\n",
    "\n",
    "#len(dialogue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare final/actual dictionary for creating the chat bot\n",
    "# This dictionary will have the conversation wise data.\n",
    "conversation_data_dict = {}\n",
    "conversation_data_dict['movie_id'] = []\n",
    "conversation_data_dict['input'] = []\n",
    "conversation_data_dict['target'] = []\n",
    "for movie_id, convo_list in movie_conversation_df[['movie_id', 'line_ids']].to_numpy():\n",
    "    for convos in range(len(convo_list)-1):\n",
    "        conversation_data_dict['movie_id'].append(movie_id)\n",
    "        conversation_data_dict['input'].append(dialogue_dict[convo_list[convos]])\n",
    "        conversation_data_dict['target'].append(dialogue_dict[convo_list[convos+1]])\n",
    "\n",
    "# Prepare dataframe from the dictionary for better access\n",
    "conversation_data_df = pd.DataFrame.from_dict(conversation_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for data cleaning\n",
    "def clean_text(input_text: str, add_tags: bool = False, start_tag: str = 'START_ ', end_tag: str = ' _END', \n",
    "                remove_punc: bool = True, remove_symbols: str = '[^0-9a-z #+_]', ignore_words: list = [], \n",
    "                remove_numbers: bool = True, replace_word_from: list = [], replace_word_to: list = []):\n",
    "    \"\"\"\n",
    "    Input: input_text (string), add_tags (optional - bool), start_tag (optional - string), end_tag (optional - string), \n",
    "            remove_punc (optional - bool), remove_symbols (optional - string), ignore_words (optional - list), remove_numbers (optional - bool),\n",
    "            replace_word_from (optional - bool), replace_word_to (optional - bool)\n",
    "    Output: cleaned text (string)\n",
    "    description:\n",
    "        This function will clean the input text given by removong the bad symbols, numbers, punctuations, extra spaces... and return back the cleaned text\n",
    "        if the add_tags value is True (it's False by default) it will add the start tag and end tags at the start and end of the text\n",
    "        we can also define the start_tag and end_tag values\n",
    "    \"\"\"\n",
    "    def remove_punctuation(text: str):\n",
    "        punctuation_list = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in punctuation_list)\n",
    "\n",
    "    def remove_bad_symbols(text: str, symbols: str):\n",
    "        bad_symbols = re.compile(symbols)\n",
    "        return bad_symbols.sub(' ', text)\n",
    "\n",
    "    def remove_extra_space(text: str):\n",
    "        extra_space = re.compile(' +')\n",
    "        return extra_space.sub(' ', text)\n",
    "\n",
    "    def remove_ignore_words(text: str, ignore_words_list: list):\n",
    "        for word in ignore_words_list:\n",
    "            text = text.replace(word, \" \")\n",
    "        return text\n",
    "    \n",
    "    def remove_digits(text:str):\n",
    "        remove_digit = str.maketrans('', '', string.digits)\n",
    "        return text.translate(remove_digit)\n",
    "\n",
    "    def replace_words(text: str, replace_word_list_from: list, replace_word_list_to: list):\n",
    "        for from_word, to_word in zip(replace_word_list_from, replace_word_list_to):\n",
    "            text = text.replace(str(from_word).lower(), str(to_word).lower())\n",
    "        return text\n",
    "\n",
    "    def add_start_end_tags(text: str):\n",
    "        return 'START_ ' + text + ' _END'\n",
    "\n",
    "    input_text = input_text.lower()\n",
    "    input_text = replace_words(input_text, replace_word_from, replace_word_to) if replace_word_from and (len(replace_word_from) == len(replace_word_to)) else input_text\n",
    "    input_text = remove_ignore_words(input_text, ignore_words) if ignore_words else input_text\n",
    "    input_text = remove_digits(input_text) if remove_numbers else input_text\n",
    "    input_text = remove_punctuation(input_text) if remove_punc else input_text\n",
    "    input_text = remove_bad_symbols(input_text, remove_symbols) if remove_symbols else input_text\n",
    "    input_text = add_start_end_tags(input_text) if add_tags else input_text\n",
    "    input_text = remove_extra_space(input_text)\n",
    "    return input_text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_data_df['input'] = conversation_data_df['input'].apply(clean_text)\n",
    "conversation_data_df['target'] = conversation_data_df['target'].apply(clean_text, add_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the comedy movies\n",
    "comedy_movies_list = genre_dict['comedy']\n",
    "\n",
    "# filter only the comedy movies from total dataframe\n",
    "comedy_movie_line_df = conversation_data_df[conversation_data_df['movie_id'].isin(comedy_movies_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61668, 6852, 61668, 6852)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting data for training and validation\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(comedy_movie_line_df['input'].to_numpy(),\n",
    "                                                                            comedy_movie_line_df['target'].to_numpy(),\n",
    "                                                                            test_size=0.1,\n",
    "                                                                            random_state=42)\n",
    "len(train_inputs), len(test_inputs), len(train_targets), len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters text vectorizer & creating text vectorizer \n",
    "max_vocab_length = 10000\n",
    "max_length = 20\n",
    "text_vectorizer = layers.experimental.preprocessing.TextVectorization(\n",
    "                    max_tokens=max_vocab_length,\n",
    "                    output_mode=\"int\",\n",
    "                    output_sequence_length=max_length,\n",
    "                    standardize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapting the training data for preparing the final dictionary\n",
    "text_vectorizer.adapt(comedy_movie_line_df['target'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output text to vectors for training the model\n",
    "train_vector_targets = text_vectorizer(train_targets)\n",
    "test_vector_targets = text_vectorizer(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing generator function for fetching dataset\n",
    "def batch_data_generator(x_vec, y_vec, vocab_list: list, batch_size: int = 128, ):\n",
    "    while True:\n",
    "        for i in range(0, len(x_vec), batch_size):\n",
    "            encoder_input_data = x_vec[i:i+batch_size]\n",
    "            decoder_input_data = np.zeros((batch_size, y_vec[0].shape[0]), dtype=int) #y_vec[i:i+batch_size]\n",
    "            decoder_target_data = np.zeros((batch_size, y_vec[0].shape[0], len(vocab_list)), dtype=int) #y_vec[i:i+batch_size] #tf.zeros((batch_size, max_length, max_vocab_length), dtype=tf.float32)\n",
    "            start_index = vocab_list.index('START_')\n",
    "            unk_index = vocab_list.index('[UNK]')\n",
    "            end_index = vocab_list.index('_END')\n",
    "            all_zero = np.zeros(len(vocab_list))\n",
    "            end_vector = np.zeros(len(vocab_list))\n",
    "            end_vector[end_index] = 1\n",
    "            for j, target_vector in enumerate(y_vec[i:i+batch_size]):\n",
    "                closing_index = np.where(target_vector.numpy() == end_index)[0].size\n",
    "                max_index = len(target_vector.numpy()) - 1\n",
    "                if closing_index:\n",
    "                    max_index = np.where(target_vector.numpy() == end_index)[0][0]\n",
    "                vector_length = len(target_vector.numpy()) -1\n",
    "                for t, idx in enumerate(target_vector.numpy()):\n",
    "                    if idx == end_index:\n",
    "                        decoder_input_data[j][t] = 0\n",
    "                    else:\n",
    "                        decoder_input_data[j][t] = idx\n",
    "                    if t == max_index:\n",
    "                        if idx == end_index:\n",
    "                            decoder_target_data[j][t-1][0] = 1\n",
    "                        else:\n",
    "                            decoder_target_data[j][t-1][idx] = 1\n",
    "                    elif idx == unk_index:\n",
    "                        decoder_target_data[j][t-1][0] = 1\n",
    "                    elif t > 0:\n",
    "                        decoder_target_data[j][t-1][idx] = 1\n",
    "                    if t == vector_length:\n",
    "                        decoder_target_data[j][t][idx] = 1\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating emmbedding object\n",
    "embedding_output_dimension = 128\n",
    "enc_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=embedding_output_dimension,\n",
    "                                #input_length=max_length,\n",
    "                                mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "lstm_units = 128\n",
    "encoder_inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "encoder_vector = text_vectorizer(encoder_inputs)\n",
    "enc_emd = enc_embedding(encoder_vector)\n",
    "encoder_lstm = layers.LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emd)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer for decoder\n",
    "dec_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=embedding_output_dimension, # 128\n",
    "                                #input_length=max_length,\n",
    "                                mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "#decoder_vector = text_vectorizer(decoder_inputs)\n",
    "dec_emb = dec_embedding(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(max_vocab_length, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model_train = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = batch_data_generator(train_inputs, train_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=32)\n",
    "test_batch = batch_data_generator(test_inputs, test_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 18s 186ms/step - loss: 3.4420 - mse: 9.9140e-05 - val_loss: 2.8186 - val_mse: 9.8118e-05\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 11s 176ms/step - loss: 2.9150 - mse: 9.8155e-05 - val_loss: 2.9573 - val_mse: 9.8141e-05\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 12s 184ms/step - loss: 2.8787 - mse: 9.8070e-05 - val_loss: 2.6833 - val_mse: 9.7823e-05\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 13s 202ms/step - loss: 2.7493 - mse: 9.7767e-05 - val_loss: 2.6622 - val_mse: 9.7729e-05\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 13s 200ms/step - loss: 2.7237 - mse: 9.7673e-05 - val_loss: 2.8735 - val_mse: 9.7771e-05\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 12s 192ms/step - loss: 2.8371 - mse: 9.7706e-05 - val_loss: 2.7447 - val_mse: 9.7551e-05\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 12s 184ms/step - loss: 2.7876 - mse: 9.7598e-05 - val_loss: 2.5960 - val_mse: 9.7295e-05\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 12s 182ms/step - loss: 2.7407 - mse: 9.7479e-05 - val_loss: 2.7267 - val_mse: 9.7494e-05\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 12s 182ms/step - loss: 2.6599 - mse: 9.7291e-05 - val_loss: 2.5843 - val_mse: 9.7364e-05\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 12s 183ms/step - loss: 2.7884 - mse: 9.7355e-05 - val_loss: 2.7207 - val_mse: 9.7357e-05\n"
     ]
    }
   ],
   "source": [
    "model_train_history = model_train.fit(training_batch,\n",
    "                                        steps_per_epoch=64,\n",
    "                                        epochs=10,\n",
    "                                        validation_data=test_batch,\n",
    "                                        validation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder at test time\n",
    "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = layers.Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = layers.Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '_END', 'START_', 'you']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = text_vectorizer.get_vocabulary()\n",
    "vocab_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0][0] = vocab_list.index('START_')\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
    "        print(output_tokens[0, -1, :].shape)\n",
    "        print(output_tokens)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :], axis=0)\n",
    "        print(sampled_token_index)\n",
    "        sampled_char = vocab_list[sampled_token_index]\n",
    "        print(sampled_char)\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        if len(decoded_sentence.split())>19:\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0][0] = sampled_token_index\n",
    "        states_value = [h,c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = batch_data_generator(train_inputs, train_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=1)\n",
    "(input_seq, actual_out), encoded_out = next(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['what are you talking about'], dtype=object),\n",
       " array([[   3,   18,  216,   43,    6,  183,   11,  119,    4,  655,   47,\n",
       "            1, 2376,   49,    1, 1531,   11,  245,    0,    0]]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq, actual_out, encoded_out[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what are you talking about',\n",
       " 'START_ im talking about the kind of people you hang out withabout growing up assuming responsibility of yourself _END')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0], train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "[[[1.1956852e-02 6.9580469e-03 3.4358453e-07 ... 1.4406443e-06\n",
      "   1.0840726e-06 2.0768932e-06]]]\n",
      "5\n",
      "i\n",
      "(10000,)\n",
      "[[[2.6507201e-02 2.4037773e-02 8.6131274e-07 ... 3.0394147e-06\n",
      "   2.1279711e-06 4.1855092e-06]]]\n",
      "17\n",
      "dont\n",
      "(10000,)\n",
      "[[[4.4840828e-02 2.6194606e-02 8.4241356e-07 ... 3.0599788e-06\n",
      "   2.1489086e-06 4.3231953e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.4639330e-01 3.2070532e-02 3.9693265e-07 ... 1.6630485e-06\n",
      "   1.3764676e-06 2.8903789e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.5843211e-01 3.3758163e-02 4.3604513e-07 ... 1.7288272e-06\n",
      "   1.5386161e-06 3.2223982e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.6896784e-01 3.4326278e-02 4.4550981e-07 ... 1.7166493e-06\n",
      "   1.6044052e-06 3.3416552e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7313072e-01 3.4797058e-02 4.5445955e-07 ... 1.7192851e-06\n",
      "   1.6485881e-06 3.4258392e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7558481e-01 3.5110567e-02 4.5856763e-07 ... 1.7166554e-06\n",
      "   1.6704472e-06 3.4667341e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7704785e-01 3.5318192e-02 4.6077656e-07 ... 1.7138287e-06\n",
      "   1.6825920e-06 3.4892787e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7798473e-01 3.5456069e-02 4.6193114e-07 ... 1.7111554e-06\n",
      "   1.6894112e-06 3.5018736e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7860799e-01 3.5549853e-02 4.6254317e-07 ... 1.7088828e-06\n",
      "   1.6933654e-06 3.5091527e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7903644e-01 3.5615258e-02 4.6287033e-07 ... 1.7070411e-06\n",
      "   1.6957302e-06 3.5135074e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7933817e-01 3.5661861e-02 4.6304720e-07 ... 1.7055826e-06\n",
      "   1.6971907e-06 3.5162150e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7955518e-01 3.5695855e-02 4.6314528e-07 ... 1.7044380e-06\n",
      "   1.6981298e-06 3.5179692e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7971452e-01 3.5721041e-02 4.6320096e-07 ... 1.7035509e-06\n",
      "   1.6987487e-06 3.5191408e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7983304e-01 3.5740033e-02 4.6323336e-07 ... 1.7028561e-06\n",
      "   1.6991721e-06 3.5199539e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7992258e-01 3.5754569e-02 4.6325280e-07 ... 1.7023153e-06\n",
      "   1.6994703e-06 3.5205280e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.7999128e-01 3.5765834e-02 4.6326440e-07 ... 1.7018890e-06\n",
      "   1.6996847e-06 3.5209584e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.8004441e-01 3.5774671e-02 4.6327168e-07 ... 1.7015506e-06\n",
      "   1.6998428e-06 3.5212795e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n",
      "(10000,)\n",
      "[[[1.8008596e-01 3.5781644e-02 4.6327654e-07 ... 1.7012828e-06\n",
      "   1.6999658e-06 3.5215241e-06]]]\n",
      "0\n",
      "\n",
      "(10000,)\n",
      "[[[1.13544374e-04 1.21419747e-04 9.72123744e-05 ... 1.00053177e-04\n",
      "   9.90620319e-05 9.91349079e-05]]]\n",
      "1\n",
      "[UNK]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' i dont  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e3e6d922a4a2b8bdfb835fcfd6c913bd8c71bd3fc5f1ecc522edf4d040dd891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
