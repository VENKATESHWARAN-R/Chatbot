{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot KATTA V1\n",
    "This chat bot is a general purpose chat bot which is trained on movies conversations.\n",
    "* This is a try 1 with a base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required modules.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "* Get the movie details\n",
    "* Get the conversation details\n",
    "* Get the conversation lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the required data files\n",
    "# Reading the movie details meta data\n",
    "with open('./data/movie_titles_metadata.txt', 'r', encoding='utf-8', errors='ignore') as mtm:\n",
    "    movie_titles = mtm.read().split('\\n')\n",
    "\n",
    "# Reading the conversation meta data\n",
    "with open('./data/movie_conversations.txt', 'r', encoding='utf-8', errors='ignore') as mc:\n",
    "    movie_conversations = mc.read().split('\\n')\n",
    "\n",
    "# Reading the conversation lines\n",
    "with open('./data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore') as ml:\n",
    "    movie_lines = ml.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dictionary for all data \n",
    "# Prepare dictionary for movie meta data\n",
    "movie_title_list = []\n",
    "for line in movie_titles:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_title_info = {}\n",
    "    movie_info = line.split(' +++$+++ ')\n",
    "    movie_title_info['movie_id'] = movie_info[0].strip()\n",
    "    movie_title_info['name'] = movie_info[1].strip()\n",
    "    movie_title_info['year'] = movie_info[2].strip()\n",
    "    movie_title_info['rating'] = movie_info[3].strip()\n",
    "    movie_title_info['genre'] = movie_info[-1][2:-2].strip().split(\"', '\") # this is for splitting the genres from ['comedy', 'romance'] to a list\n",
    "    movie_title_list.append(movie_title_info)\n",
    "\n",
    "# Prepare dictionary for movie convo meta data\n",
    "movie_conversation_list = []\n",
    "for line in movie_conversations:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_conversation_info = {}\n",
    "    conversation_info = line.split(' +++$+++ ')\n",
    "    movie_conversation_info['speaker1'] = conversation_info[0].strip()\n",
    "    movie_conversation_info['speaker2'] = conversation_info[1].strip()\n",
    "    movie_conversation_info['movie_id'] = conversation_info[2].strip()\n",
    "    movie_conversation_info['line_ids'] = conversation_info[-1][2:-2].strip().split(\"', '\")# this is for splitting the conversation info from ['L198', 'L199'] to a list\n",
    "    movie_conversation_list.append(movie_conversation_info)\n",
    "\n",
    "# Prepare dictionary for movie dialogues\n",
    "movie_lines_list = []\n",
    "for line in movie_lines:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_line_info = {}\n",
    "    line_info = line.split(' +++$+++ ')\n",
    "    movie_line_info['line_id'] = line_info[0].strip()\n",
    "    movie_line_info['speaker'] = line_info[1].strip()\n",
    "    movie_line_info['movie_id'] = line_info[2].strip()\n",
    "    movie_line_info['character'] = line_info[3].strip()\n",
    "    movie_line_info['dialogue'] = line_info[-1].strip()\n",
    "    movie_lines_list.append(movie_line_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for all the above dicts for better processing\n",
    "movie_title_df = pd.DataFrame.from_dict(movie_title_list)\n",
    "movie_conversation_df = pd.DataFrame.from_dict(movie_conversation_list)\n",
    "movie_lines_df = pd.DataFrame.from_dict(movie_lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available genres from the whole dataset \n",
    "genres = movie_title_df['genre'].to_numpy()\n",
    "genre_set = set()\n",
    "for genre_list in genres:\n",
    "    for genre in genre_list:\n",
    "        if genre:\n",
    "            genre_set.add(genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count of movies in each genres and storing the movies with respect to their genres in the dictionary\n",
    "genre_dict = {}\n",
    "for genre_name in genre_set:\n",
    "    genre_dict[genre_name] = []\n",
    "for movie, genre_list in movie_title_df[['movie_id', 'genre']].to_numpy():\n",
    "    for genre in genre_list:\n",
    "        if genre:\n",
    "            genre_dict[genre].append(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable & Data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies available in this FAMILY genre 17\n",
      "Movies available in this MYSTERY genre 102\n",
      "Movies available in this ACTION genre 168\n",
      "Movies available in this CRIME genre 147\n",
      "Movies available in this HISTORY genre 21\n",
      "Movies available in this DOCUMENTARY genre 3\n",
      "Movies available in this BIOGRAPHY genre 25\n",
      "Movies available in this THRILLER genre 269\n",
      "Movies available in this COMEDY genre 162\n",
      "Movies available in this FANTASY genre 78\n",
      "Movies available in this HORROR genre 99\n",
      "Movies available in this ADVENTURE genre 116\n",
      "Movies available in this DRAMA genre 320\n",
      "Movies available in this ADULT genre 1\n",
      "Movies available in this MUSIC genre 13\n",
      "Movies available in this SCI-FI genre 120\n",
      "Movies available in this WAR genre 23\n",
      "Movies available in this ANIMATION genre 18\n",
      "Movies available in this ROMANCE genre 132\n",
      "Movies available in this SPORT genre 8\n",
      "Movies available in this SHORT genre 5\n",
      "Movies available in this MUSICAL genre 8\n",
      "Movies available in this FILM-NOIR genre 4\n",
      "Movies available in this WESTERN genre 12\n"
     ]
    }
   ],
   "source": [
    "for genre_name, genre_lists in genre_dict.items():\n",
    "    print(f'Movies available in this {genre_name.upper()} genre', len(genre_lists))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['m0', list(['comedy', 'romance'])],\n",
       "       ['m1', list(['adventure', 'biography', 'drama', 'history'])],\n",
       "       ['m2', list(['action', 'crime', 'drama', 'thriller'])],\n",
       "       ['m3', list(['adventure', 'mystery', 'sci-fi'])],\n",
       "       ['m4', list(['action', 'comedy', 'crime', 'drama', 'thriller'])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_df[['movie_id', 'genre']].head().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action',\n",
       " 'adult',\n",
       " 'adventure',\n",
       " 'animation',\n",
       " 'biography',\n",
       " 'comedy',\n",
       " 'crime',\n",
       " 'documentary',\n",
       " 'drama',\n",
       " 'family',\n",
       " 'fantasy',\n",
       " 'film-noir',\n",
       " 'history',\n",
       " 'horror',\n",
       " 'music',\n",
       " 'musical',\n",
       " 'mystery',\n",
       " 'romance',\n",
       " 'sci-fi',\n",
       " 'short',\n",
       " 'sport',\n",
       " 'thriller',\n",
       " 'war',\n",
       " 'western'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>[comedy, romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m1</td>\n",
       "      <td>1492: conquest of paradise</td>\n",
       "      <td>1992</td>\n",
       "      <td>6.20</td>\n",
       "      <td>[adventure, biography, drama, history]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m2</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>2001</td>\n",
       "      <td>6.10</td>\n",
       "      <td>[action, crime, drama, thriller]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_id  ...                                   genre\n",
       "0       m0  ...                       [comedy, romance]\n",
       "1       m1  ...  [adventure, biography, drama, history]\n",
       "2       m2  ...        [action, crime, drama, thriller]\n",
       "\n",
       "[3 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_id    object\n",
       "name        object\n",
       "year        object\n",
       "rating      object\n",
       "genre       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list ['comedy', 'romance'] is <class 'list'>\n",
      "list ['adventure', 'biography', 'drama', 'history'] is <class 'list'>\n",
      "list ['action', 'crime', 'drama', 'thriller'] is <class 'list'>\n",
      "list ['adventure', 'mystery', 'sci-fi'] is <class 'list'>\n",
      "list ['action', 'comedy', 'crime', 'drama', 'thriller'] is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(movie_title_df['genre'][:5]):\n",
    "    print(\"list\", l, \"is\", type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker1</th>\n",
       "      <th>speaker2</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>line_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>[L194, L195, L196, L197]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>[L198, L199]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>[L200, L201, L202, L203]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker1 speaker2 movie_id                  line_ids\n",
       "0       u0       u2       m0  [L194, L195, L196, L197]\n",
       "1       u0       u2       m0              [L198, L199]\n",
       "2       u0       u2       m0  [L200, L201, L202, L203]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversation_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker1    object\n",
       "speaker2    object\n",
       "movie_id    object\n",
       "line_ids    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversation_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list ['L194', 'L195', 'L196', 'L197'] is <class 'list'>\n",
      "list ['L198', 'L199'] is <class 'list'>\n",
      "list ['L200', 'L201', 'L202', 'L203'] is <class 'list'>\n",
      "list ['L204', 'L205', 'L206'] is <class 'list'>\n",
      "list ['L207', 'L208'] is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(movie_conversation_df['line_ids'][:5]):\n",
    "    print(\"list\", l, \"is\", type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>character</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  line_id speaker movie_id character      dialogue\n",
       "0   L1045      u0       m0    BIANCA  They do not!\n",
       "1   L1044      u2       m0   CAMERON   They do to!\n",
       "2    L985      u0       m0    BIANCA    I hope so."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['m0', list(['L194', 'L195', 'L196', 'L197'])],\n",
       "       ['m0', list(['L198', 'L199'])],\n",
       "       ['m0', list(['L200', 'L201', 'L202', 'L203'])]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test check\n",
    "movie_conversation_df[['movie_id', 'line_ids']].head(3).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines_df['dialogue'][movie_lines_df['line_id']=='L194'].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare final datasets\n",
    "Prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make conversation line dictionary for preparing the final dataset\n",
    "dialogue_ids = movie_lines_df['line_id'].to_numpy()\n",
    "dialogue_lines = movie_lines_df['dialogue'].to_numpy()\n",
    "dialogue_dict = {}\n",
    "for dialogue_id, dialogue_line in zip(dialogue_ids, dialogue_lines):\n",
    "    dialogue_dict[dialogue_id] = dialogue_line\n",
    "\n",
    "#len(dialogue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare final/actual dictionary for creating the chat bot\n",
    "# This dictionary will have the conversation wise data.\n",
    "conversation_data_dict = {}\n",
    "conversation_data_dict['movie_id'] = []\n",
    "conversation_data_dict['input'] = []\n",
    "conversation_data_dict['target'] = []\n",
    "for movie_id, convo_list in movie_conversation_df[['movie_id', 'line_ids']].to_numpy():\n",
    "    for convos in range(len(convo_list)-1):\n",
    "        conversation_data_dict['movie_id'].append(movie_id)\n",
    "        conversation_data_dict['input'].append(dialogue_dict[convo_list[convos]])\n",
    "        conversation_data_dict['target'].append(dialogue_dict[convo_list[convos+1]])\n",
    "\n",
    "# Prepare dataframe from the dictionary for better access\n",
    "conversation_data_df = pd.DataFrame.from_dict(conversation_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable & Data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m0</td>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m0</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m0</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m0</td>\n",
       "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
       "      <td>Forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m0</td>\n",
       "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
       "      <td>Cameron.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_id  ...                                             target\n",
       "0       m0  ...  Well, I thought we'd start with pronunciation,...\n",
       "1       m0  ...  Not the hacking and gagging and spitting part....\n",
       "2       m0  ...  Okay... then how 'bout we try out some French ...\n",
       "3       m0  ...                                         Forget it.\n",
       "4       m0  ...                                           Cameron.\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[1,2,3,4], [3,4,5,6]])\n",
    "np.unique(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the created dataframes for future accesses.\n",
    "conversation_data_df.to_csv('./data/processed_data/conversation_data.csv')\n",
    "movie_conversation_df.to_csv('./data/processed_data/movie_conversation.csv')\n",
    "movie_lines_df.to_csv('./data/processed_data/movie_lines.csv')\n",
    "movie_title_df.to_csv('./data/processed_data/movie_title.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for data cleaning\n",
    "def clean_text(input_text: str, add_tags: bool = False, start_tag: str = 'START_ ', end_tag: str = ' _END', \n",
    "                remove_punc: bool = True, remove_symbols: str = '[^0-9a-z #+_]', ignore_words: list = [], \n",
    "                remove_numbers: bool = True, replace_word_from: list = [], replace_word_to: list = []):\n",
    "    \"\"\"\n",
    "    Input: input_text (string), add_tags (optional - bool), start_tag (optional - string), end_tag (optional - string), \n",
    "            remove_punc (optional - bool), remove_symbols (optional - string), ignore_words (optional - list), remove_numbers (optional - bool),\n",
    "            replace_word_from (optional - bool), replace_word_to (optional - bool)\n",
    "    Output: cleaned text (string)\n",
    "    description:\n",
    "        This function will clean the input text given by removong the bad symbols, numbers, punctuations, extra spaces... and return back the cleaned text\n",
    "        if the add_tags value is True (it's False by default) it will add the start tag and end tags at the start and end of the text\n",
    "        we can also define the start_tag and end_tag values\n",
    "    \"\"\"\n",
    "    def remove_punctuation(text: str):\n",
    "        punctuation_list = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in punctuation_list)\n",
    "\n",
    "    def remove_bad_symbols(text: str, symbols: str):\n",
    "        bad_symbols = re.compile(symbols)\n",
    "        return bad_symbols.sub(' ', text)\n",
    "\n",
    "    def remove_extra_space(text: str):\n",
    "        extra_space = re.compile(' +')\n",
    "        return extra_space.sub(' ', text)\n",
    "\n",
    "    def remove_ignore_words(text: str, ignore_words_list: list):\n",
    "        for word in ignore_words_list:\n",
    "            text = text.replace(word, \" \")\n",
    "        return text\n",
    "    \n",
    "    def remove_digits(text:str):\n",
    "        remove_digit = str.maketrans('', '', string.digits)\n",
    "        return text.translate(remove_digit)\n",
    "\n",
    "    def replace_words(text: str, replace_word_list_from: list, replace_word_list_to: list):\n",
    "        for from_word, to_word in zip(replace_word_list_from, replace_word_list_to):\n",
    "            text = text.replace(str(from_word).lower(), str(to_word).lower())\n",
    "        return text\n",
    "\n",
    "    def add_start_end_tags(text: str):\n",
    "        return 'START_ ' + text + ' _END'\n",
    "\n",
    "    input_text = input_text.lower()\n",
    "    input_text = replace_words(input_text, replace_word_from, replace_word_to) if replace_word_from and (len(replace_word_from) == len(replace_word_to)) else input_text\n",
    "    input_text = remove_ignore_words(input_text, ignore_words) if ignore_words else input_text\n",
    "    input_text = remove_digits(input_text) if remove_numbers else input_text\n",
    "    input_text = remove_punctuation(input_text) if remove_punc else input_text\n",
    "    input_text = remove_bad_symbols(input_text, remove_symbols) if remove_symbols else input_text\n",
    "    input_text = add_start_end_tags(input_text) if add_tags else input_text\n",
    "    input_text = remove_extra_space(input_text)\n",
    "    return input_text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_data_df['input'] = conversation_data_df['input'].apply(clean_text)\n",
    "conversation_data_df['target'] = conversation_data_df['target'].apply(clean_text, add_tags=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable & Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Hi There buddy! can't you speak? ... okay here's my number 786797655765. ring me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi there buddy cannot you speak okay heres my number ring me'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(test_sentence, replace_word_from=[\"can't\"], replace_word_to=['Cannot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m0</td>\n",
       "      <td>can we make this quick roxanne korrine and and...</td>\n",
       "      <td>START_ well i thought wed start with pronuncia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m0</td>\n",
       "      <td>well i thought wed start with pronunciation if...</td>\n",
       "      <td>START_ not the hacking and gagging and spittin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m0</td>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "      <td>START_ okay then how bout we try out some fren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m0</td>\n",
       "      <td>youre asking me out thats so cute whats your n...</td>\n",
       "      <td>START_ forget it _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m0</td>\n",
       "      <td>no no its my fault we didnt have a proper intr...</td>\n",
       "      <td>START_ cameron _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_id                                              input  \\\n",
       "0       m0  can we make this quick roxanne korrine and and...   \n",
       "1       m0  well i thought wed start with pronunciation if...   \n",
       "2       m0  not the hacking and gagging and spitting part ...   \n",
       "3       m0  youre asking me out thats so cute whats your n...   \n",
       "4       m0  no no its my fault we didnt have a proper intr...   \n",
       "\n",
       "                                              target  \n",
       "0  START_ well i thought wed start with pronuncia...  \n",
       "1  START_ not the hacking and gagging and spittin...  \n",
       "2  START_ okay then how bout we try out some fren...  \n",
       "3                              START_ forget it _END  \n",
       "4                                START_ cameron _END  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "* first we will build model only with the comedy movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 \n",
    "This model will only include comedy movies.\n",
    "This model architecture is inspired from https://medium.com/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161\n",
    "\n",
    "This is a Encoder - decoder model\n",
    "* Encoder will be trained with input data\n",
    "* Decoder will be trained using target data (differently - during traing)\n",
    "* During predictions the layers from decoder is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the comedy movies\n",
    "comedy_movies_list = genre_dict['comedy']\n",
    "\n",
    "# filter only the comedy movies from total dataframe\n",
    "comedy_movie_line_df = conversation_data_df[conversation_data_df['movie_id'].isin(comedy_movies_list)][:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 3), (221616, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_movie_line_df.shape, conversation_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters text vectorizer & creating text vectorizer \n",
    "max_vocab_length = 10000\n",
    "max_length = 20\n",
    "text_vectorizer = layers.experimental.preprocessing.TextVectorization(\n",
    "                    max_tokens=max_vocab_length,\n",
    "                    output_mode=\"int\",\n",
    "                    output_sequence_length=max_length,\n",
    "                    standardize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapting the training data for preparing the final dictionary\n",
    "text_vectorizer.adapt(comedy_movie_line_df['target'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating emmbedding object\n",
    "embedding_output_dimension = 128\n",
    "enc_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=embedding_output_dimension,\n",
    "                                #input_length=max_length,\n",
    "                                mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "lstm_units = 64\n",
    "encoder_inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "encoder_vector = text_vectorizer(encoder_inputs)\n",
    "enc_emd = enc_embedding(encoder_vector)\n",
    "encoder_lstm = layers.LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emd)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer for decoder\n",
    "dec_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=embedding_output_dimension, # 128\n",
    "                                #input_length=max_length,\n",
    "                                mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "#decoder_vector = text_vectorizer(decoder_inputs)\n",
    "dec_emb = dec_embedding(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(max_vocab_length, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model_train = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_5 (TextVect  (None, 20)          0           ['input_14[0][0]']               \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 20, 128)      1280000     ['text_vectorization_5[3][0]']   \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, None, 128)    1280000     ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)                 [(None, 64),         49408       ['embedding_12[0][0]']           \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 [(None, None, 64),   49408       ['embedding_13[0][0]',           \n",
      "                                 (None, 64),                      'lstm_13[0][1]',                \n",
      "                                 (None, 64)]                      'lstm_13[0][2]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, None, 10000)  650000      ['lstm_14[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,308,816\n",
      "Trainable params: 3,308,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 3000, 27000, 3000)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting data for training and validation\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(comedy_movie_line_df['input'].to_numpy(),\n",
    "                                                                            comedy_movie_line_df['target'].to_numpy(),\n",
    "                                                                            test_size=0.1,\n",
    "                                                                            random_state=42)\n",
    "len(train_inputs), len(test_inputs), len(train_targets), len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output text to vectors for training the model\n",
    "train_vector_targets = text_vectorizer(train_targets)\n",
    "test_vector_targets = text_vectorizer(test_targets)\n",
    "\n",
    "#vector_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27000,), TensorShape([27000, 20]))"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape, train_vector_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable & data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yeah well dont let it get out', 'pretty advanced isnt it',\n",
       "       'what do you think of me', ..., 'five bills',\n",
       "       'sir before you boot me i just want to explain i mean okay you got a goatguy with a hook for a head',\n",
       "       'that was your people magazine with the letters cut out wasnt it'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'START_ hi there _END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "array([[  3, 341,  69,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer([test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_\n",
      "hi\n",
      "Hello hi\n",
      "there\n",
      "Hello there\n",
      "Hello _END\n"
     ]
    }
   ],
   "source": [
    "for t, word in enumerate(test_text.split()):\n",
    "    if t<len(test_text.split()) -1:\n",
    "        print(word)\n",
    "    if t>0:\n",
    "        print('Hello', word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 20])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[:128].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([    2,    18,   216,    43,     6,   183,    11,   119,     4,\n",
       "         653,    47, 10117,  2374,    49, 13934,  1529,    11,   245,\n",
       "           3,     0], dtype=int64)>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([363,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer('hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61668"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vector_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary()[67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9]\n",
    "a[0] = 10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(61668, 20), dtype=int64, numpy=\n",
       "array([[   2,   18,  216, ...,  245,    3,    0],\n",
       "       [   2,  152,   52, ...,    0,    0,    0],\n",
       "       [   2, 3149,  143, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,    9,   34, ...,    0,    0,    0],\n",
       "       [   2,  285,   31, ...,    0,    0,    0],\n",
       "       [   2, 6499,    3, ...,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 15000)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for x in train_vector_targets:\n",
    "    temp = tf.one_hot(x, depth=max_vocab_length)\n",
    "    break\n",
    "\n",
    "\n",
    "np.array(temp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([27000, 20])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train_vector_targets[0][:6].numpy() == 0)[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train_vector_targets[0].numpy() == 0)[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train_vector_targets[0].numpy() == 3)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[0].numpy().size -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '_END', 'START_', 'you']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary().index('START_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([  3,  43, 106,  27, 479,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 43, 106,  27, 479,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0], dtype=int64),\n",
       " (19,))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[0].numpy()[1:], train_vector_targets[0].numpy()[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 43, 106,  27, 479,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0], dtype=int64),\n",
       " (20,))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(train_vector_targets[0].numpy()[1:], [0]), np.append(train_vector_targets[0].numpy()[1:], [0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = text_vectorizer.get_vocabulary().index('START_')\n",
    "end_index = text_vectorizer.get_vocabulary().index('_END')\n",
    "start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '_END', 'START_', 'you']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  43, 106,  27, 479,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int64)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train_vector_targets[0].numpy() == end_index)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vector_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'TensorShape([20])' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20248/1399184467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_vector_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_vector_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret 'TensorShape([20])' as a data type"
     ]
    }
   ],
   "source": [
    "train_vector_targets[0].shape\n",
    "np.zeros(2, train_vector_targets[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing generator function for fetching dataset\n",
    "def batch_data_generator(x_vec, y_vec, vocab_list: list, batch_size: int = 128, ):\n",
    "    while True:\n",
    "        for i in range(0, len(x_vec), batch_size):\n",
    "            encoder_input_data = x_vec[i:i+batch_size]\n",
    "            decoder_input_data = np.zeros((batch_size, y_vec[0].shape[0]), dtype=int) #y_vec[i:i+batch_size]\n",
    "            decoder_target_data = np.zeros((batch_size, y_vec[0].shape[0], len(vocab_list)), dtype=int) #y_vec[i:i+batch_size] #tf.zeros((batch_size, max_length, max_vocab_length), dtype=tf.float32)\n",
    "            start_index = vocab_list.index('START_')\n",
    "            end_index = vocab_list.index('_END')\n",
    "            all_zero = np.zeros(len(vocab_list))\n",
    "            end_vector = np.zeros(len(vocab_list))\n",
    "            end_vector[end_index] = 1\n",
    "            for j, target_vector in enumerate(y_vec[i:i+batch_size]):\n",
    "                #print(target_vector, j)\n",
    "                #print(decoder_target_data[j].shape)\n",
    "                #print(np.append(decoder_target_data[j][1:], [0]))\n",
    "                #print(decoder_target_data[j])\n",
    "                closing_index = np.where(target_vector.numpy() == end_index)[0].size\n",
    "                max_index = len(target_vector.numpy()) - 1\n",
    "                if closing_index:\n",
    "                    max_index = np.where(target_vector.numpy() == end_index)[0][0]\n",
    "                vector_length = len(target_vector.numpy()) -1\n",
    "                #print(vector_length)\n",
    "                #print('max_index', max_index)\n",
    "                for t, idx in enumerate(target_vector.numpy()):\n",
    "                    #print(idx)\n",
    "                    if idx == end_index:\n",
    "                        #print('end ', t, idx)\n",
    "                        decoder_input_data[j][t] = 0\n",
    "                    else:\n",
    "                        decoder_input_data[j][t] = idx\n",
    "                    if t == max_index:\n",
    "                        #print(t)\n",
    "                        decoder_target_data[j][t-1][idx] = 1\n",
    "                    elif t > 0:\n",
    "                        decoder_target_data[j][t-1][idx] = 1\n",
    "                    if t == vector_length:\n",
    "                        decoder_target_data[j][t][idx] = 1\n",
    "                    #decoder_target_data[j] = np.append(decoder_target_data[j][1:], all_zero)\n",
    "                #print(decoder_input_data[j])\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "            #return ([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variable & Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 3), (1, 43), (2, 106), (3, 27), (4, 479), (5, 2), (6, 0))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = 5\n",
    "(0,3), (1,43), (2,106), (3,27), (4,479), (5, 2), (6,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "([test_in,text_in1], test_out) = batch_data_generator(train_inputs[:2], train_vector_targets[:2],vocab_list=text_vectorizer.get_vocabulary(), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1,), (1, 20), (1, 20, 10000))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_in.shape, text_in1.shape, test_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yeah well dont let it get out'], dtype=object)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "array([[  3,  43, 106,  27, 479,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_targets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3,  43, 106,  27, 479,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 10000)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2], dtype=int64),)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(test_out[0][4] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset for training and validation\n",
    "#train_data_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
    "#train_lables_dataset = tf.data.Dataset.from_tensor_slices(train_vector_targets)\n",
    "#train_dataset = tf.data.Dataset.zip((train_data_dataset, train_lables_dataset))\n",
    "#train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#test_data_dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_targets))\n",
    "#test_lables_dataset = tf.data.Dataset.from_tensor_slices(test_vector_targets)\n",
    "#test_dataset = tf.data.Dataset.zip((test_data_dataset, test_lables_dataset))\n",
    "#test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None,), (None,)), (None, 20)), types: ((tf.string, tf.string), tf.int64)>,\n",
       " <PrefetchDataset shapes: (((None,), (None,)), (None, 20)), types: ((tf.string, tf.string), tf.int64)>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = batch_data_generator(train_inputs, train_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=32)\n",
    "test_batch = batch_data_generator(test_inputs, test_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "64/64 [==============================] - 12s 187ms/step - loss: 2.5583 - mse: 9.6297e-05 - val_loss: 2.7016 - val_mse: 9.6638e-05\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 2.5348 - mse: 9.6246e-05"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "   Incompatible shapes: [128,256] vs. [56,256]\n\t [[{{node add}}]]\n\t [[model_7/lstm_14/PartitionedCall]] [Op:__inference_test_function_424058]\n\nFunction call stack:\ntest_function -> test_function -> test_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20248/3083094415.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_train_history = model_train.fit(training_batch,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                         validation_steps=16)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:    Incompatible shapes: [128,256] vs. [56,256]\n\t [[{{node add}}]]\n\t [[model_7/lstm_14/PartitionedCall]] [Op:__inference_test_function_424058]\n\nFunction call stack:\ntest_function -> test_function -> test_function\n"
     ]
    }
   ],
   "source": [
    "model_train_history = model_train.fit(training_batch,\n",
    "                                        steps_per_epoch=64,\n",
    "                                        epochs=5,\n",
    "                                        validation_data=test_batch,\n",
    "                                        validation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder at test time\n",
    "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = layers.Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = layers.Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = text_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '_END', 'START_', 'you', 'i', 'the', 'to', 'a', 'it']"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0][0] = vocab_list.index('START_')\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
    "        print(output_tokens.shape)\n",
    "        print(output_tokens)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = vocab_list[sampled_token_index]\n",
    "        print(sampled_char)\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        if (sampled_char=='_END') or len(decoded_sentence)>19:\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0][0] = sampled_token_index\n",
    "        states_value = [h,c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yeah well dont let it get out', 'pretty advanced isnt it',\n",
       "       'what do you think of me', 'the poor thing six years',\n",
       "       'are you waiting for a bus',\n",
       "       'im going to change for dinner ill see you shortly', 'for what',\n",
       "       'why is it', 'having fun',\n",
       "       'yes so do i but i think he is a little oldfashioned like a puff harold'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah well dont let it get out'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' _END'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([train_inputs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "434398e44183469e607e2a6981795099df29b4cddf5938cb48c3354932d31cfd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
