{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries for data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str, data_location: str = './data/'):\n",
    "    \"\"\"\n",
    "    This function will be loading the data from the filenames which are given as input and return the list of lines from the data file\n",
    "    input: file_name -> str, data_location -> str = ./data/ by default\n",
    "    output: lines -> list data lines list from the input file\n",
    "    \"\"\"\n",
    "    def fix_dir(dir_name: str):\n",
    "        if dir_name[-1] == '/':\n",
    "            return dir_name\n",
    "        return dir_name + '/'\n",
    "    \n",
    "    data_file = fix_dir(data_location) + file_name\n",
    "    with open(data_file, 'r', encoding='utf-8', errors='ignore') as dfile:\n",
    "        lines = dfile.read().split('\\n')\n",
    "    \n",
    "    print(f'Data read from {data_file} and converted into {len(lines)} lines')\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(movie_titles: list, movie_conversations: list, movie_lines: list):\n",
    "    \"\"\"\n",
    "    This function prepares data dictionary for each files it outputs list of dictionaries for all the major datasets \n",
    "    inputs: movie_titles -> list, movie_conversations -> list, movie_lines -> list\n",
    "    outputs: movie_title_list -> list(dict), movie_conversation_list -> list(dict), movie_lines_list -> list(dict)\n",
    "    \"\"\"\n",
    "    # Prepare dictionary for movie meta data\n",
    "    movie_title_list = []\n",
    "    for line in movie_titles:\n",
    "        if not line:\n",
    "            continue # for identifying and ignoring empty lines\n",
    "        movie_title_info = {}\n",
    "        movie_info = line.split(' +++$+++ ')\n",
    "        movie_title_info['movie_id'] = movie_info[0].strip()\n",
    "        movie_title_info['name'] = movie_info[1].strip()\n",
    "        movie_title_info['year'] = movie_info[2].strip()\n",
    "        movie_title_info['rating'] = movie_info[3].strip()\n",
    "        movie_title_info['genre'] = movie_info[-1][2:-2].strip().split(\"', '\") # this is for splitting the genres from ['comedy', 'romance'] to a list\n",
    "        movie_title_list.append(movie_title_info)\n",
    "\n",
    "    # Prepare dictionary for movie convo meta data\n",
    "    movie_conversation_list = []\n",
    "    for line in movie_conversations:\n",
    "        if not line:\n",
    "            continue # for identifying and ignoring empty lines\n",
    "        movie_conversation_info = {}\n",
    "        conversation_info = line.split(' +++$+++ ')\n",
    "        movie_conversation_info['speaker1'] = conversation_info[0].strip()\n",
    "        movie_conversation_info['speaker2'] = conversation_info[1].strip()\n",
    "        movie_conversation_info['movie_id'] = conversation_info[2].strip()\n",
    "        movie_conversation_info['line_ids'] = conversation_info[-1][2:-2].strip().split(\"', '\")# this is for splitting the conversation info from ['L198', 'L199'] to a list\n",
    "        movie_conversation_list.append(movie_conversation_info)\n",
    "\n",
    "    # Prepare dictionary for movie dialogues\n",
    "    movie_lines_list = []\n",
    "    for line in movie_lines:\n",
    "        if not line:\n",
    "            continue # for identifying and ignoring empty lines\n",
    "        movie_line_info = {}\n",
    "        line_info = line.split(' +++$+++ ')\n",
    "        movie_line_info['line_id'] = line_info[0].strip()\n",
    "        movie_line_info['speaker'] = line_info[1].strip()\n",
    "        movie_line_info['movie_id'] = line_info[2].strip()\n",
    "        movie_line_info['character'] = line_info[3].strip()\n",
    "        movie_line_info['dialogue'] = line_info[-1].strip()\n",
    "        movie_lines_list.append(movie_line_info)\n",
    "\n",
    "    return movie_title_list, movie_conversation_list, movie_lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_dict(data_dict_list: list):\n",
    "    \"\"\"\n",
    "    This function converts the list of dictionaries into pandas dataframe\n",
    "    input: data_dict_list -> list(dict)\n",
    "    output: pandas dataframe prepared from the list\n",
    "    \"\"\"\n",
    "    return pd.DataFrame.from_dict(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre_dict(movie_title_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This line takes the input as movie titles pandas dataframe and prepares the genre dict\n",
    "    input: movie_title_df -> pandas.DataFrame\n",
    "    output: genre_dict -> dict the output will have the dictionary with keys as genre and values as list of movies from that genre\n",
    "    \"\"\"\n",
    "    # Get the list of available genres from the whole dataset \n",
    "    genres = movie_title_df['genre'].to_numpy()\n",
    "    genre_set = set()\n",
    "    for genre_list in genres:\n",
    "        for genre in genre_list:\n",
    "            if genre:\n",
    "                genre_set.add(genre)\n",
    "    \n",
    "    # Checking the count of movies in each genres and storing the movies with respect to their genres in the dictionary\n",
    "    genre_dict = {}\n",
    "    for genre_name in genre_set:\n",
    "        genre_dict[genre_name] = []\n",
    "    for movie, genre_list in movie_title_df[['movie_id', 'genre']].to_numpy():\n",
    "        for genre in genre_list:\n",
    "            if genre:\n",
    "              genre_dict[genre].append(movie)\n",
    "    \n",
    "    print('Genre dictionary prepared')\n",
    "\n",
    "    return genre_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_conversations(movie_lines_df: pd.DataFrame, movie_conversation_df: pd.DataFrame, only_start: bool = False):\n",
    "    \"\"\"\n",
    "    This line takes the input as movie lines pandas dataframe and prepares the genre dict\n",
    "    input: movie_lines_df -> pandas.DataFrame, movie_conversation_df -> pandas.DataFrame\n",
    "    output: dialogue_dict -> dict dictionary with line_id as key and respective line as value, conversation_data_df -> pandas.DataFrame will have question and answers dataframe\n",
    "    \"\"\"\n",
    "    # Make conversation line dictionary for preparing the final dataset\n",
    "    dialogue_ids = movie_lines_df['line_id'].to_numpy()\n",
    "    dialogue_lines = movie_lines_df['dialogue'].to_numpy()\n",
    "    dialogue_dict = {}\n",
    "    for dialogue_id, dialogue_line in zip(dialogue_ids, dialogue_lines):\n",
    "        dialogue_dict[dialogue_id] = dialogue_line\n",
    "\n",
    "    # prepare final/actual dictionary for creating the chat bot\n",
    "    # This dictionary will have the conversation wise data.\n",
    "    conversation_data_dict = {}\n",
    "    conversation_data_dict['movie_id'] = []\n",
    "    conversation_data_dict['input'] = []\n",
    "    conversation_data_dict['target'] = []\n",
    "    for movie_id, convo_list in movie_conversation_df[['movie_id', 'line_ids']].to_numpy():\n",
    "        for convos in range(len(convo_list)-1):\n",
    "            conversation_data_dict['movie_id'].append(movie_id)\n",
    "            conversation_data_dict['input'].append(dialogue_dict[convo_list[convos]])\n",
    "            conversation_data_dict['target'].append(dialogue_dict[convo_list[convos+1]])\n",
    "            if only_start:\n",
    "              break\n",
    "\n",
    "    # Prepare dataframe from the dictionary for better access\n",
    "    conversation_data_df = pd.DataFrame.from_dict(conversation_data_dict)\n",
    "    print('Conversations prepared')\n",
    "    \n",
    "    return dialogue_dict, conversation_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for data cleaning\n",
    "def clean_text(input_text: str, add_tags: bool = False, start_tag: str = 'START_ ', end_tag: str = ' _END', \n",
    "                remove_punc: bool = True, remove_symbols: str = '[^0-9a-z #+_]', ignore_words: list = [], \n",
    "                remove_numbers: bool = True, replace_word_from: list = [], replace_word_to: list = []):\n",
    "    \"\"\"\n",
    "    Input: input_text (string), add_tags (optional - bool), start_tag (optional - string), end_tag (optional - string), \n",
    "            remove_punc (optional - bool), remove_symbols (optional - string), ignore_words (optional - list), remove_numbers (optional - bool),\n",
    "            replace_word_from (optional - bool), replace_word_to (optional - bool)\n",
    "    Output: cleaned text (string)\n",
    "    description:\n",
    "        This function will clean the input text given by removong the bad symbols, numbers, punctuations, extra spaces... and return back the cleaned text\n",
    "        if the add_tags value is True (it's False by default) it will add the start tag and end tags at the start and end of the text\n",
    "        we can also define the start_tag and end_tag values\n",
    "    \"\"\"\n",
    "    def replace_common_words(text: str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(\"i'm\", \"i am\", text)\n",
    "        text = re.sub(\"he's\", \"he is\", text)\n",
    "        text = re.sub(\"she's\", \"she is\", text)\n",
    "        text = re.sub(\"that's\", \"that is\", text)\n",
    "        text = re.sub(\"what's\", \"what is\", text)\n",
    "        text = re.sub(\"where's\", \"where is\", text)\n",
    "        text = re.sub(\"'ll\", \" will\", text)\n",
    "        text = re.sub(\"'ve\", \" have\", text)\n",
    "        text = re.sub(\"'re\", \" are\", text)\n",
    "        text = re.sub(\"'d\", \" would\", text)\n",
    "        text = re.sub(\"n't\", \" not\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text: str):\n",
    "        punctuation_list = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in punctuation_list)\n",
    "\n",
    "    def remove_bad_symbols(text: str, symbols: str):\n",
    "        bad_symbols = re.compile(symbols)\n",
    "        return bad_symbols.sub(' ', text)\n",
    "\n",
    "    def remove_extra_space(text: str):\n",
    "        extra_space = re.compile(' +')\n",
    "        return extra_space.sub(' ', text)\n",
    "\n",
    "    def remove_ignore_words(text: str, ignore_words_list: list):\n",
    "        for word in ignore_words_list:\n",
    "            text = text.replace(word, \" \")\n",
    "        return text\n",
    "    \n",
    "    def remove_digits(text:str):\n",
    "        remove_digit = str.maketrans('', '', string.digits)\n",
    "        return text.translate(remove_digit)\n",
    "\n",
    "    def replace_words(text: str, replace_word_list_from: list, replace_word_list_to: list):\n",
    "        for from_word, to_word in zip(replace_word_list_from, replace_word_list_to):\n",
    "            text = text.replace(str(from_word).lower(), str(to_word).lower())\n",
    "        return text\n",
    "\n",
    "    def add_start_end_tags(text: str):\n",
    "        return start_tag + text + end_tag\n",
    "\n",
    "    input_text = input_text.lower()\n",
    "    input_text = replace_common_words(input_text)\n",
    "    input_text = replace_words(input_text, replace_word_from, replace_word_to) if replace_word_from and (len(replace_word_from) == len(replace_word_to)) else input_text\n",
    "    input_text = remove_ignore_words(input_text, ignore_words) if ignore_words else input_text\n",
    "    input_text = remove_digits(input_text) if remove_numbers else input_text\n",
    "    input_text = remove_punctuation(input_text) if remove_punc else input_text\n",
    "    input_text = remove_bad_symbols(input_text, remove_symbols) if remove_symbols else input_text\n",
    "    input_text = add_start_end_tags(input_text) if add_tags else input_text\n",
    "    input_text = remove_extra_space(input_text)\n",
    "    #print('Data cleaning done')\n",
    "    \n",
    "    return input_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_long(conversation_data_df: pd.DataFrame, min_q_length: int = 2, max_q_length: int = 25, min_a_length: int = 2, max_a_length: int = 25):\n",
    "    \"\"\"\n",
    "    This function takes list of input dialogues and list of target dialogues and returns only the dialogues with given length\n",
    "    input: conversation_data_df -> pandas.DataFrame\n",
    "    output: filtered_conversation_df -> pandas.DataFrame\n",
    "    \"\"\"\n",
    "    movie_id_seq, qseq, aseq = conversation_data_df['movie_id'].to_numpy(), conversation_data_df['input'].to_numpy(), conversation_data_df['target'].to_numpy()\n",
    "    conversation_data_dict = {}\n",
    "    conversation_data_dict['movie_id'], conversation_data_dict['input'], conversation_data_dict['target'] = [], [], []\n",
    "    raw_data_len = len(movie_id_seq)\n",
    "\n",
    "    for i in range(raw_data_len):\n",
    "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
    "        if qlen >= min_q_length and qlen <= max_q_length:\n",
    "            if alen >= min_a_length and alen <= max_a_length:\n",
    "                conversation_data_dict['movie_id'].append(movie_id_seq[i])\n",
    "                conversation_data_dict['input'].append(qseq[i])\n",
    "                conversation_data_dict['target'].append(aseq[i])\n",
    "    \n",
    "    filt_data_len = len(conversation_data_dict['movie_id'])\n",
    "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
    "    print(f'{filtered}% filtered from original data')\n",
    "\n",
    "    return pd.DataFrame.from_dict(conversation_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorize_filter_unk(conversation_data_df: pd.DataFrame, Vectorizer: TextVectorization, unk: str = '[UNK]', test_split: float = 0.2, seed: int = 42):\n",
    "    \"\"\"\n",
    "    This function takes list of input dialogues and list of target dialogues and returns only the dialogues with less unknown tokens\n",
    "    input: conversation_data_df -> pandas.DataFrame, vectorizer object\n",
    "    output: training_data -> dict data needed for training, testing_data -> data needed for testing\n",
    "    \"\"\"\n",
    "    def remove_start_tag(input_with_start_tag: str):\n",
    "        return ' '.join(input_with_start_tag.split()[1:])\n",
    "\n",
    "    movie_id_seq, qseq, aseq = conversation_data_df['movie_id'].to_numpy(), conversation_data_df['input'].to_numpy(), conversation_data_df['target'].to_numpy()\n",
    "    training_data = {}\n",
    "    testing_data = {}\n",
    "    training_data['input'], training_data['target'], training_data['input_vectors'], training_data['target_vectors'] = [], [], [], []\n",
    "    testing_data['input'], testing_data['target'], testing_data['input_vectors'], testing_data['target_vectors'] = [], [], [], []\n",
    "\n",
    "    raw_data_len = len(movie_id_seq)\n",
    "    vocab_list = Vectorizer.get_vocabulary()\n",
    "    unk_index = vocab_list.index(unk)\n",
    "\n",
    "    train_inputs, test_inputs, train_targets, test_targets = train_test_split(qseq, aseq, test_size=test_split, random_state=seed)\n",
    "    \n",
    "    start_tag_removed_train_targets = [remove_start_tag(target) for target in train_targets]\n",
    "    start_tag_removed_test_targets = [remove_start_tag(target) for target in test_targets]\n",
    "\n",
    "    train_vectorized_inputs, train_vectorized_targets = Vectorizer(train_inputs), Vectorizer(start_tag_removed_train_targets)\n",
    "    test_vectorized_inputs, test_vectorized_targets = Vectorizer(test_inputs), Vectorizer(start_tag_removed_test_targets)\n",
    "\n",
    "    for idx, (input_tensor, target_tensor) in enumerate(zip(train_vectorized_inputs, train_vectorized_targets)):\n",
    "        input_list = list(input_tensor.numpy())\n",
    "        target_list = list(target_tensor.numpy())\n",
    "        unknown_count_q = input_list.count(unk_index)\n",
    "        unknown_count_a = target_list.count(unk_index)\n",
    "        if unknown_count_a <=1 :\n",
    "            if unknown_count_q > 0:\n",
    "                temp_list = list(filter(lambda num: num != 0, input_list)) # This list will have the inputs without zeros padded\n",
    "                if unknown_count_q/len(temp_list) > 0.2:\n",
    "                    continue\n",
    "            training_data['input'].append(train_inputs[idx])\n",
    "            training_data['target'].append(train_targets[idx])\n",
    "            training_data['input_vectors'].append(input_tensor)\n",
    "            training_data['target_vectors'].append(target_tensor)\n",
    "        \n",
    "    testing_data['input'], testing_data['target'] = test_inputs, test_targets \n",
    "    testing_data['input_vectors'], testing_data['target_vectors'] = test_vectorized_inputs, test_vectorized_targets\n",
    "\n",
    "    print(f'Training data points: {len(train_inputs)}')\n",
    "    print(f'Test data points: {len(test_inputs)}')\n",
    "    filt_data_len = len(training_data['input'])\n",
    "    filtered = int((len(train_inputs) - filt_data_len)*100/len(train_inputs))\n",
    "    print(f'{filtered}% filtered from training data points')\n",
    "    print(f'After unknown token filters training data points: {filt_data_len}')\n",
    "\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(vocab_list, decoder_model_function, encoder_model: Model, input_text: str = 'hi', next_word: str = 'START_', clean_text = clean_text, max_length: int = 19, multi_layer: bool = True):\n",
    "    \"\"\"\n",
    "    This function takes inputs as follows and returns the model response.\n",
    "    input: vocab_list -> this is the list of voicabulary used in the model,\n",
    "            model_function -> this is a reference functions in wich the decoder model is defined, \n",
    "            encoder_model -> this is the encoder model which need to be used for input text encoding, \n",
    "            input_text -> this is the input phrase for which the model create the response the default value if 'hi', \n",
    "            next_word -> this is the trigger or start word for the decoder model, the default value is 'START_',\n",
    "            clean_text -> this is a referance of the function which need to be used for cleaning the text the default is 'clean_text' function written or imported in this python file,\n",
    "            max_length -> max length of the bot response defaults to 19\n",
    "            multi_layer -> if the model single layer then this has to be False by default it is True\n",
    "    output: bot_response -> this is the predicted response of the bot\n",
    "    \"\"\"\n",
    "    states_list = []\n",
    "    input_text = clean_text(input_text)\n",
    "    if multi_layer:\n",
    "        encoder_output = encoder_model.predict([input_text])\n",
    "    else:\n",
    "        encoder_output = [encoder_model.predict([input_text])]\n",
    "    for states in encoder_output:\n",
    "        states_list.append([tf.constant(states[0]), tf.constant(states[1])])\n",
    "    stop_condition = True\n",
    "    bot_response = \"\"\n",
    "    states = states_list\n",
    "    while stop_condition:\n",
    "        next_word, states = decoder_model_function(next_word, states, vocab_list)\n",
    "        if next_word == '_END' or len(bot_response.split()) > max_length:\n",
    "            break\n",
    "        bot_response += next_word + ' '\n",
    "    return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59274f66ba6d87f8881ad97d5dea530dfc64a2944c69a18ecfbd70818c31a586"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
