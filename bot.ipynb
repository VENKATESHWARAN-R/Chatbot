{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required modules.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the required data files\n",
    "# Reading the movie details meta data\n",
    "with open('./data/movie_titles_metadata.txt', 'r', encoding='utf-8', errors='ignore') as mtm:\n",
    "    movie_titles = mtm.read().split('\\n')\n",
    "\n",
    "# Reading the conversation meta data\n",
    "with open('./data/movie_conversations.txt', 'r', encoding='utf-8', errors='ignore') as mc:\n",
    "    movie_conversations = mc.read().split('\\n')\n",
    "\n",
    "# Reading the conversation lines\n",
    "with open('./data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore') as ml:\n",
    "    movie_lines = ml.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dictionary for all data \n",
    "# Prepare dictionary for movie meta data\n",
    "movie_title_list = []\n",
    "for line in movie_titles:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_title_info = {}\n",
    "    movie_info = line.split(' +++$+++ ')\n",
    "    movie_title_info['movie_id'] = movie_info[0].strip()\n",
    "    movie_title_info['name'] = movie_info[1].strip()\n",
    "    movie_title_info['year'] = movie_info[2].strip()\n",
    "    movie_title_info['rating'] = movie_info[3].strip()\n",
    "    movie_title_info['genre'] = movie_info[-1][2:-2].strip().split(\"', '\") # this is for splitting the genres from ['comedy', 'romance'] to a list\n",
    "    movie_title_list.append(movie_title_info)\n",
    "\n",
    "# Prepare dictionary for movie convo meta data\n",
    "movie_conversation_list = []\n",
    "for line in movie_conversations:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_conversation_info = {}\n",
    "    conversation_info = line.split(' +++$+++ ')\n",
    "    movie_conversation_info['speaker1'] = conversation_info[0].strip()\n",
    "    movie_conversation_info['speaker2'] = conversation_info[1].strip()\n",
    "    movie_conversation_info['movie_id'] = conversation_info[2].strip()\n",
    "    movie_conversation_info['line_ids'] = conversation_info[-1][2:-2].strip().split(\"', '\")# this is for splitting the conversation info from ['L198', 'L199'] to a list\n",
    "    movie_conversation_list.append(movie_conversation_info)\n",
    "\n",
    "# Prepare dictionary for movie dialogues\n",
    "movie_lines_list = []\n",
    "for line in movie_lines:\n",
    "    if not line:\n",
    "        continue # for identifying and ignoring empty lines\n",
    "    movie_line_info = {}\n",
    "    line_info = line.split(' +++$+++ ')\n",
    "    movie_line_info['line_id'] = line_info[0].strip()\n",
    "    movie_line_info['speaker'] = line_info[1].strip()\n",
    "    movie_line_info['movie_id'] = line_info[2].strip()\n",
    "    movie_line_info['character'] = line_info[3].strip()\n",
    "    movie_line_info['dialogue'] = line_info[-1].strip()\n",
    "    movie_lines_list.append(movie_line_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for all the above dicts for better processing\n",
    "movie_title_df = pd.DataFrame.from_dict(movie_title_list)\n",
    "movie_conversation_df = pd.DataFrame.from_dict(movie_conversation_list)\n",
    "movie_lines_df = pd.DataFrame.from_dict(movie_lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available genres from the whole dataset \n",
    "genres = movie_title_df['genre'].to_numpy()\n",
    "genre_set = set()\n",
    "for genre_list in genres:\n",
    "    for genre in genre_list:\n",
    "        if genre:\n",
    "            genre_set.add(genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count of movies in each genres and storing the movies with respect to their genres in the dictionary\n",
    "genre_dict = {}\n",
    "for genre_name in genre_set:\n",
    "    genre_dict[genre_name] = []\n",
    "for movie, genre_list in movie_title_df[['movie_id', 'genre']].to_numpy():\n",
    "    for genre in genre_list:\n",
    "        if genre:\n",
    "            genre_dict[genre].append(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make conversation line dictionary for preparing the final dataset\n",
    "dialogue_ids = movie_lines_df['line_id'].to_numpy()\n",
    "dialogue_lines = movie_lines_df['dialogue'].to_numpy()\n",
    "dialogue_dict = {}\n",
    "for dialogue_id, dialogue_line in zip(dialogue_ids, dialogue_lines):\n",
    "    dialogue_dict[dialogue_id] = dialogue_line\n",
    "\n",
    "#len(dialogue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare final/actual dictionary for creating the chat bot\n",
    "# This dictionary will have the conversation wise data.\n",
    "conversation_data_dict = {}\n",
    "conversation_data_dict['movie_id'] = []\n",
    "conversation_data_dict['input'] = []\n",
    "conversation_data_dict['target'] = []\n",
    "for movie_id, convo_list in movie_conversation_df[['movie_id', 'line_ids']].to_numpy():\n",
    "    for convos in range(len(convo_list)-1):\n",
    "        conversation_data_dict['movie_id'].append(movie_id)\n",
    "        conversation_data_dict['input'].append(dialogue_dict[convo_list[convos]])\n",
    "        conversation_data_dict['target'].append(dialogue_dict[convo_list[convos+1]])\n",
    "\n",
    "# Prepare dataframe from the dictionary for better access\n",
    "conversation_data_df = pd.DataFrame.from_dict(conversation_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for data cleaning\n",
    "def clean_text(input_text: str, add_tags: bool = False, start_tag: str = 'START_ ', end_tag: str = ' _END', \n",
    "                remove_punc: bool = True, remove_symbols: str = '[^0-9a-z #+_]', ignore_words: list = [], \n",
    "                remove_numbers: bool = True, replace_word_from: list = [], replace_word_to: list = []):\n",
    "    \"\"\"\n",
    "    Input: input_text (string), add_tags (optional - bool), start_tag (optional - string), end_tag (optional - string), \n",
    "            remove_punc (optional - bool), remove_symbols (optional - string), ignore_words (optional - list), remove_numbers (optional - bool),\n",
    "            replace_word_from (optional - bool), replace_word_to (optional - bool)\n",
    "    Output: cleaned text (string)\n",
    "    description:\n",
    "        This function will clean the input text given by removong the bad symbols, numbers, punctuations, extra spaces... and return back the cleaned text\n",
    "        if the add_tags value is True (it's False by default) it will add the start tag and end tags at the start and end of the text\n",
    "        we can also define the start_tag and end_tag values\n",
    "    \"\"\"\n",
    "    def remove_punctuation(text: str):\n",
    "        punctuation_list = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in punctuation_list)\n",
    "\n",
    "    def remove_bad_symbols(text: str, symbols: str):\n",
    "        bad_symbols = re.compile(symbols)\n",
    "        return bad_symbols.sub(' ', text)\n",
    "\n",
    "    def remove_extra_space(text: str):\n",
    "        extra_space = re.compile(' +')\n",
    "        return extra_space.sub(' ', text)\n",
    "\n",
    "    def remove_ignore_words(text: str, ignore_words_list: list):\n",
    "        for word in ignore_words_list:\n",
    "            text = text.replace(word, \" \")\n",
    "        return text\n",
    "    \n",
    "    def remove_digits(text:str):\n",
    "        remove_digit = str.maketrans('', '', string.digits)\n",
    "        return text.translate(remove_digit)\n",
    "\n",
    "    def replace_words(text: str, replace_word_list_from: list, replace_word_list_to: list):\n",
    "        for from_word, to_word in zip(replace_word_list_from, replace_word_list_to):\n",
    "            text = text.replace(str(from_word).lower(), str(to_word).lower())\n",
    "        return text\n",
    "\n",
    "    def add_start_end_tags(text: str):\n",
    "        return 'START_ ' + text + ' _END'\n",
    "\n",
    "    input_text = input_text.lower()\n",
    "    input_text = replace_words(input_text, replace_word_from, replace_word_to) if replace_word_from and (len(replace_word_from) == len(replace_word_to)) else input_text\n",
    "    input_text = remove_ignore_words(input_text, ignore_words) if ignore_words else input_text\n",
    "    input_text = remove_digits(input_text) if remove_numbers else input_text\n",
    "    input_text = remove_punctuation(input_text) if remove_punc else input_text\n",
    "    input_text = remove_bad_symbols(input_text, remove_symbols) if remove_symbols else input_text\n",
    "    input_text = add_start_end_tags(input_text) if add_tags else input_text\n",
    "    input_text = remove_extra_space(input_text)\n",
    "    return input_text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_data_df['input'] = conversation_data_df['input'].apply(clean_text)\n",
    "conversation_data_df['target'] = conversation_data_df['target'].apply(clean_text, add_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the comedy movies\n",
    "comedy_movies_list = genre_dict['comedy']\n",
    "\n",
    "# filter only the comedy movies from total dataframe\n",
    "comedy_movie_line_df = conversation_data_df[conversation_data_df['movie_id'].isin(comedy_movies_list)][:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 3000, 27000, 3000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting data for training and validation\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(comedy_movie_line_df['input'].to_numpy(),\n",
    "                                                                            comedy_movie_line_df['target'].to_numpy(),\n",
    "                                                                            test_size=0.1,\n",
    "                                                                            random_state=42)\n",
    "len(train_inputs), len(test_inputs), len(train_targets), len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters text vectorizer & creating text vectorizer \n",
    "max_vocab_length = 10000\n",
    "max_length = 20\n",
    "text_vectorizer = layers.experimental.preprocessing.TextVectorization(\n",
    "                    max_tokens=max_vocab_length,\n",
    "                    output_mode=\"int\",\n",
    "                    output_sequence_length=max_length,\n",
    "                    standardize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapting the training data for preparing the final dictionary\n",
    "text_vectorizer.adapt(comedy_movie_line_df['target'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output text to vectors for training the model\n",
    "train_vector_targets = text_vectorizer(train_targets)\n",
    "test_vector_targets = text_vectorizer(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing generator function for fetching dataset\n",
    "def batch_data_generator(x_vec, y_vec, vocab_list: list, batch_size: int = 128, ):\n",
    "    while True:\n",
    "        for i in range(0, len(x_vec), batch_size):\n",
    "            encoder_input_data = x_vec[i:i+batch_size]\n",
    "            decoder_input_data = np.zeros((batch_size, y_vec[0].shape[0]), dtype=int) #y_vec[i:i+batch_size]\n",
    "            decoder_target_data = np.zeros((batch_size, y_vec[0].shape[0], len(vocab_list)), dtype=int) #y_vec[i:i+batch_size] #tf.zeros((batch_size, max_length, max_vocab_length), dtype=tf.float32)\n",
    "            start_index = vocab_list.index('START_')\n",
    "            end_index = vocab_list.index('_END')\n",
    "            all_zero = np.zeros(len(vocab_list))\n",
    "            end_vector = np.zeros(len(vocab_list))\n",
    "            end_vector[end_index] = 1\n",
    "            for j, target_vector in enumerate(y_vec[i:i+batch_size]):\n",
    "                closing_index = np.where(target_vector.numpy() == end_index)[0].size\n",
    "                max_index = len(target_vector.numpy()) - 1\n",
    "                if closing_index:\n",
    "                    max_index = np.where(target_vector.numpy() == end_index)[0][0]\n",
    "                vector_length = len(target_vector.numpy()) -1\n",
    "                for t, idx in enumerate(target_vector.numpy()):\n",
    "                    if idx == end_index:\n",
    "                        decoder_input_data[j][t] = 0\n",
    "                    else:\n",
    "                        decoder_input_data[j][t] = idx\n",
    "                    if t == max_index:\n",
    "                        decoder_target_data[j][t-1][idx] = 1\n",
    "                    elif t > 0:\n",
    "                        decoder_target_data[j][t-1][idx] = 1\n",
    "                    if t == vector_length:\n",
    "                        decoder_target_data[j][t][idx] = 1\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating emmbedding object\n",
    "embedding_output_dimension = 128\n",
    "enc_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=embedding_output_dimension,\n",
    "                                #input_length=max_length,\n",
    "                                mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "lstm_units = 64\n",
    "encoder_inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "encoder_vector = text_vectorizer(encoder_inputs)\n",
    "enc_emd = enc_embedding(encoder_vector)\n",
    "encoder_lstm = layers.LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emd)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer for decoder\n",
    "dec_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=embedding_output_dimension, # 128\n",
    "                                #input_length=max_length,\n",
    "                                mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "#decoder_vector = text_vectorizer(decoder_inputs)\n",
    "dec_emb = dec_embedding(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(max_vocab_length, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model_train = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = batch_data_generator(train_inputs, train_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=32)\n",
    "test_batch = batch_data_generator(test_inputs, test_vector_targets, vocab_list=text_vectorizer.get_vocabulary(), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "64/64 [==============================] - 19s 182ms/step - loss: 3.8101 - mse: 9.9706e-05 - val_loss: 2.8841 - val_mse: 9.8679e-05\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 2.9227 - mse: 9.8579e-05 - val_loss: 3.0055 - val_mse: 9.8542e-05\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 11s 174ms/step - loss: 2.8741 - mse: 9.8480e-05 - val_loss: 2.8596 - val_mse: 9.8435e-05\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 10s 162ms/step - loss: 2.8334 - mse: 9.8417e-05 - val_loss: 2.8684 - val_mse: 9.8461e-05\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 10s 160ms/step - loss: 2.8527 - mse: 9.8368e-05 - val_loss: 2.7143 - val_mse: 9.8299e-05\n"
     ]
    }
   ],
   "source": [
    "model_train_history = model_train.fit(training_batch,\n",
    "                                        steps_per_epoch=64,\n",
    "                                        epochs=5,\n",
    "                                        validation_data=test_batch,\n",
    "                                        validation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder at test time\n",
    "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = layers.Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = layers.Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '_END', 'START_', 'you']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = text_vectorizer.get_vocabulary()\n",
    "vocab_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0][0] = vocab_list.index('START_')\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
    "        print(output_tokens.shape)\n",
    "        print(output_tokens)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print(sampled_token_index)\n",
    "        sampled_char = vocab_list[sampled_token_index]\n",
    "        print(sampled_char)\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        if (sampled_char=='_END') or len(decoded_sentence)>19:\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0][0] = sampled_token_index\n",
    "        states_value = [h,c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yeah well dont let it get out', 'pretty advanced isnt it',\n",
       "       'what do you think of me', 'the poor thing six years',\n",
       "       'are you waiting for a bus'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 10000)\n",
      "[[[4.7443319e-07 2.0884672e-02 1.3164012e-01 ... 4.4962533e-07\n",
      "   4.6176430e-07 4.0685825e-07]]]\n",
      "2\n",
      "_END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' _END'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([train_inputs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e3e6d922a4a2b8bdfb835fcfd6c913bd8c71bd3fc5f1ecc522edf4d040dd891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
