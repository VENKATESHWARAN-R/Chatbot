{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Katta_bot_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dzjvhk7pLxqj",
        "PHbTXC31aZHm",
        "ew85nvMjZ2u3",
        "7HzkZ6bJbnAb",
        "-HdhtwIkc036",
        "9JB_TWEPik-O",
        "X-E9ebch5Dqd",
        "kxAh4W5H-vbp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KATTA bot local\n",
        "\n",
        "This bot code is a replica of the local code"
      ],
      "metadata": {
        "id": "_0tlGbQjLisE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENV setup\n",
        "\n",
        "Setup libraries and dependent data files "
      ],
      "metadata": {
        "id": "Dzjvhk7pLxqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pArwJ2WhD-o",
        "outputId": "f5fb2c60-7559-4546-b73a-b46db2c8b667"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Z7HWgNBLIbr"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries for data preparation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up data utilities\n",
        "\n",
        "Creating the required functions for preparing the data"
      ],
      "metadata": {
        "id": "Q81GXw8yT43k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_name: str, data_location: str = './data/'):\n",
        "    \"\"\"\n",
        "    This function will be loading the data from the filenames which are given as input and return the list of lines from the data file\n",
        "    input: file_name -> str, data_location -> str = ./data/ by default\n",
        "    output: lines -> list data lines list from the input file\n",
        "    \"\"\"\n",
        "    def fix_dir(dir_name: str):\n",
        "        if dir_name[-1] == '/':\n",
        "            return dir_name\n",
        "        return dir_name + '/'\n",
        "    \n",
        "    data_file = fix_dir(data_location) + file_name\n",
        "    with open(data_file, 'r', encoding='utf-8', errors='ignore') as dfile:\n",
        "        lines = dfile.read().split('\\n')\n",
        "    \n",
        "    print(f'Data read from {data_file} and converted into {len(lines)} lines')\n",
        "\n",
        "    return lines"
      ],
      "metadata": {
        "id": "I6bF_nJ9PviO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(movie_titles: list, movie_conversations: list, movie_lines: list):\n",
        "    \"\"\"\n",
        "    This function prepares data dictionary for each files it outputs list of dictionaries for all the major datasets \n",
        "    inputs: movie_titles -> list, movie_conversations -> list, movie_lines -> list\n",
        "    outputs: movie_title_list -> list(dict), movie_conversation_list -> list(dict), movie_lines_list -> list(dict)\n",
        "    \"\"\"\n",
        "    # Prepare dictionary for movie meta data\n",
        "    movie_title_list = []\n",
        "    for line in movie_titles:\n",
        "        if not line:\n",
        "            continue # for identifying and ignoring empty lines\n",
        "        movie_title_info = {}\n",
        "        movie_info = line.split(' +++$+++ ')\n",
        "        movie_title_info['movie_id'] = movie_info[0].strip()\n",
        "        movie_title_info['name'] = movie_info[1].strip()\n",
        "        movie_title_info['year'] = movie_info[2].strip()\n",
        "        movie_title_info['rating'] = movie_info[3].strip()\n",
        "        movie_title_info['genre'] = movie_info[-1][2:-2].strip().split(\"', '\") # this is for splitting the genres from ['comedy', 'romance'] to a list\n",
        "        movie_title_list.append(movie_title_info)\n",
        "\n",
        "    # Prepare dictionary for movie convo meta data\n",
        "    movie_conversation_list = []\n",
        "    for line in movie_conversations:\n",
        "        if not line:\n",
        "            continue # for identifying and ignoring empty lines\n",
        "        movie_conversation_info = {}\n",
        "        conversation_info = line.split(' +++$+++ ')\n",
        "        movie_conversation_info['speaker1'] = conversation_info[0].strip()\n",
        "        movie_conversation_info['speaker2'] = conversation_info[1].strip()\n",
        "        movie_conversation_info['movie_id'] = conversation_info[2].strip()\n",
        "        movie_conversation_info['line_ids'] = conversation_info[-1][2:-2].strip().split(\"', '\")# this is for splitting the conversation info from ['L198', 'L199'] to a list\n",
        "        movie_conversation_list.append(movie_conversation_info)\n",
        "\n",
        "    # Prepare dictionary for movie dialogues\n",
        "    movie_lines_list = []\n",
        "    for line in movie_lines:\n",
        "        if not line:\n",
        "            continue # for identifying and ignoring empty lines\n",
        "        movie_line_info = {}\n",
        "        line_info = line.split(' +++$+++ ')\n",
        "        movie_line_info['line_id'] = line_info[0].strip()\n",
        "        movie_line_info['speaker'] = line_info[1].strip()\n",
        "        movie_line_info['movie_id'] = line_info[2].strip()\n",
        "        movie_line_info['character'] = line_info[3].strip()\n",
        "        movie_line_info['dialogue'] = line_info[-1].strip()\n",
        "        movie_lines_list.append(movie_line_info)\n",
        "\n",
        "    return movie_title_list, movie_conversation_list, movie_lines_list"
      ],
      "metadata": {
        "id": "6NZcLq9cUM4z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_from_dict(data_dict_list: list):\n",
        "    \"\"\"\n",
        "    This function converts the list of dictionaries into pandas dataframe\n",
        "    input: data_dict_list -> list(dict)\n",
        "    output: pandas dataframe prepared from the list\n",
        "    \"\"\"\n",
        "    return pd.DataFrame.from_dict(data_dict_list)"
      ],
      "metadata": {
        "id": "NU_HFQpJUPNt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_genre_dict(movie_title_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    This line takes the input as movie titles pandas dataframe and prepares the genre dict\n",
        "    input: movie_title_df -> pandas.DataFrame\n",
        "    output: genre_dict -> dict the output will have the dictionary with keys as genre and values as list of movies from that genre\n",
        "    \"\"\"\n",
        "    # Get the list of available genres from the whole dataset \n",
        "    genres = movie_title_df['genre'].to_numpy()\n",
        "    genre_set = set()\n",
        "    for genre_list in genres:\n",
        "        for genre in genre_list:\n",
        "            if genre:\n",
        "                genre_set.add(genre)\n",
        "    \n",
        "    # Checking the count of movies in each genres and storing the movies with respect to their genres in the dictionary\n",
        "    genre_dict = {}\n",
        "    for genre_name in genre_set:\n",
        "        genre_dict[genre_name] = []\n",
        "    for movie, genre_list in movie_title_df[['movie_id', 'genre']].to_numpy():\n",
        "        for genre in genre_list:\n",
        "            if genre:\n",
        "              genre_dict[genre].append(movie)\n",
        "    \n",
        "    print('Genre dictionary prepared')\n",
        "\n",
        "    return genre_dict"
      ],
      "metadata": {
        "id": "Mg8QPq23URgo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_conversations(movie_lines_df: pd.DataFrame, movie_conversation_df: pd.DataFrame, only_start: bool = False):\n",
        "    \"\"\"\n",
        "    This line takes the input as movie lines pandas dataframe and prepares the genre dict\n",
        "    input: movie_lines_df -> pandas.DataFrame, movie_conversation_df -> pandas.DataFrame\n",
        "    output: dialogue_dict -> dict dictionary with line_id as key and respective line as value, conversation_data_df -> pandas.DataFrame will have question and answers dataframe\n",
        "    \"\"\"\n",
        "    # Make conversation line dictionary for preparing the final dataset\n",
        "    dialogue_ids = movie_lines_df['line_id'].to_numpy()\n",
        "    dialogue_lines = movie_lines_df['dialogue'].to_numpy()\n",
        "    dialogue_dict = {}\n",
        "    for dialogue_id, dialogue_line in zip(dialogue_ids, dialogue_lines):\n",
        "        dialogue_dict[dialogue_id] = dialogue_line\n",
        "\n",
        "    # prepare final/actual dictionary for creating the chat bot\n",
        "    # This dictionary will have the conversation wise data.\n",
        "    conversation_data_dict = {}\n",
        "    conversation_data_dict['movie_id'] = []\n",
        "    conversation_data_dict['input'] = []\n",
        "    conversation_data_dict['target'] = []\n",
        "    for movie_id, convo_list in movie_conversation_df[['movie_id', 'line_ids']].to_numpy():\n",
        "        for convos in range(len(convo_list)-1):\n",
        "            conversation_data_dict['movie_id'].append(movie_id)\n",
        "            conversation_data_dict['input'].append(dialogue_dict[convo_list[convos]])\n",
        "            conversation_data_dict['target'].append(dialogue_dict[convo_list[convos+1]])\n",
        "            if only_start:\n",
        "              break\n",
        "\n",
        "    # Prepare dataframe from the dictionary for better access\n",
        "    conversation_data_df = pd.DataFrame.from_dict(conversation_data_dict)\n",
        "    print('Conversations prepared')\n",
        "    \n",
        "    return dialogue_dict, conversation_data_df"
      ],
      "metadata": {
        "id": "xJWwWG28UTx0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function for data cleaning\n",
        "def clean_text(input_text: str, add_tags: bool = False, start_tag: str = 'START_ ', end_tag: str = ' _END', \n",
        "                remove_punc: bool = True, remove_symbols: str = '[^0-9a-z #+_]', ignore_words: list = [], \n",
        "                remove_numbers: bool = True, replace_word_from: list = [], replace_word_to: list = []):\n",
        "    \"\"\"\n",
        "    Input: input_text (string), add_tags (optional - bool), start_tag (optional - string), end_tag (optional - string), \n",
        "            remove_punc (optional - bool), remove_symbols (optional - string), ignore_words (optional - list), remove_numbers (optional - bool),\n",
        "            replace_word_from (optional - bool), replace_word_to (optional - bool)\n",
        "    Output: cleaned text (string)\n",
        "    description:\n",
        "        This function will clean the input text given by removong the bad symbols, numbers, punctuations, extra spaces... and return back the cleaned text\n",
        "        if the add_tags value is True (it's False by default) it will add the start tag and end tags at the start and end of the text\n",
        "        we can also define the start_tag and end_tag values\n",
        "    \"\"\"\n",
        "    def replace_common_words(text: str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(\"i'm\", \"i am\", text)\n",
        "        text = re.sub(\"he's\", \"he is\", text)\n",
        "        text = re.sub(\"she's\", \"she is\", text)\n",
        "        text = re.sub(\"that's\", \"that is\", text)\n",
        "        text = re.sub(\"what's\", \"what is\", text)\n",
        "        text = re.sub(\"where's\", \"where is\", text)\n",
        "        text = re.sub(\"'ll\", \" will\", text)\n",
        "        text = re.sub(\"'ve\", \" have\", text)\n",
        "        text = re.sub(\"'re\", \" are\", text)\n",
        "        text = re.sub(\"'d\", \" would\", text)\n",
        "        text = re.sub(\"n't\", \" not\", text)\n",
        "        return text\n",
        "\n",
        "    def remove_punctuation(text: str):\n",
        "        punctuation_list = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in punctuation_list)\n",
        "\n",
        "    def remove_bad_symbols(text: str, symbols: str):\n",
        "        bad_symbols = re.compile(symbols)\n",
        "        return bad_symbols.sub(' ', text)\n",
        "\n",
        "    def remove_extra_space(text: str):\n",
        "        extra_space = re.compile(' +')\n",
        "        return extra_space.sub(' ', text)\n",
        "\n",
        "    def remove_ignore_words(text: str, ignore_words_list: list):\n",
        "        for word in ignore_words_list:\n",
        "            text = text.replace(word, \" \")\n",
        "        return text\n",
        "    \n",
        "    def remove_digits(text:str):\n",
        "        remove_digit = str.maketrans('', '', string.digits)\n",
        "        return text.translate(remove_digit)\n",
        "\n",
        "    def replace_words(text: str, replace_word_list_from: list, replace_word_list_to: list):\n",
        "        for from_word, to_word in zip(replace_word_list_from, replace_word_list_to):\n",
        "            text = text.replace(str(from_word).lower(), str(to_word).lower())\n",
        "        return text\n",
        "\n",
        "    def add_start_end_tags(text: str):\n",
        "        return start_tag + text + end_tag\n",
        "\n",
        "    input_text = input_text.lower()\n",
        "    input_text = replace_common_words(input_text)\n",
        "    input_text = replace_words(input_text, replace_word_from, replace_word_to) if replace_word_from and (len(replace_word_from) == len(replace_word_to)) else input_text\n",
        "    input_text = remove_ignore_words(input_text, ignore_words) if ignore_words else input_text\n",
        "    input_text = remove_digits(input_text) if remove_numbers else input_text\n",
        "    input_text = remove_punctuation(input_text) if remove_punc else input_text\n",
        "    input_text = remove_bad_symbols(input_text, remove_symbols) if remove_symbols else input_text\n",
        "    input_text = add_start_end_tags(input_text) if add_tags else input_text\n",
        "    input_text = remove_extra_space(input_text)\n",
        "    #print('Data cleaning done')\n",
        "    \n",
        "    return input_text.strip()"
      ],
      "metadata": {
        "id": "t5BaV0yJUWBm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_short_long(conversation_data_df: pd.DataFrame, min_q_length: int = 2, max_q_length: int = 25, min_a_length: int = 2, max_a_length: int = 25):\n",
        "    \"\"\"\n",
        "    This function takes list of input dialogues and list of target dialogues and returns only the dialogues with given length\n",
        "    input: conversation_data_df -> pandas.DataFrame\n",
        "    output: filtered_conversation_df -> pandas.DataFrame\n",
        "    \"\"\"\n",
        "    movie_id_seq, qseq, aseq = conversation_data_df['movie_id'].to_numpy(), conversation_data_df['input'].to_numpy(), conversation_data_df['target'].to_numpy()\n",
        "    conversation_data_dict = {}\n",
        "    conversation_data_dict['movie_id'], conversation_data_dict['input'], conversation_data_dict['target'] = [], [], []\n",
        "    raw_data_len = len(movie_id_seq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= min_q_length and qlen <= max_q_length:\n",
        "            if alen >= min_a_length and alen <= max_a_length:\n",
        "                conversation_data_dict['movie_id'].append(movie_id_seq[i])\n",
        "                conversation_data_dict['input'].append(qseq[i])\n",
        "                conversation_data_dict['target'].append(aseq[i])\n",
        "    \n",
        "    filt_data_len = len(conversation_data_dict['movie_id'])\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(f'{filtered}% filtered from original data')\n",
        "\n",
        "    return pd.DataFrame.from_dict(conversation_data_dict)"
      ],
      "metadata": {
        "id": "xsMgpAMAYeW8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_vectorize_filter_unk(conversation_data_df: pd.DataFrame, Vectorizer: TextVectorization, unk: str = '[UNK]', test_split: float = 0.2, seed: int = 42):\n",
        "    \"\"\"\n",
        "    This function takes list of input dialogues and list of target dialogues and returns only the dialogues with less unknown tokens\n",
        "    input: conversation_data_df -> pandas.DataFrame, vectorizer object\n",
        "    output: training_data -> dict data needed for training, testing_data -> data needed for testing\n",
        "    \"\"\"\n",
        "    def remove_start_tag(input_with_start_tag: str):\n",
        "        return ' '.join(input_with_start_tag.split()[1:])\n",
        "\n",
        "    movie_id_seq, qseq, aseq = conversation_data_df['movie_id'].to_numpy(), conversation_data_df['input'].to_numpy(), conversation_data_df['target'].to_numpy()\n",
        "    training_data = {}\n",
        "    testing_data = {}\n",
        "    training_data['input'], training_data['target'], training_data['input_vectors'], training_data['target_vectors'] = [], [], [], []\n",
        "    testing_data['input'], testing_data['target'], testing_data['input_vectors'], testing_data['target_vectors'] = [], [], [], []\n",
        "\n",
        "    raw_data_len = len(movie_id_seq)\n",
        "    vocab_list = Vectorizer.get_vocabulary()\n",
        "    unk_index = vocab_list.index(unk)\n",
        "\n",
        "    train_inputs, test_inputs, train_targets, test_targets = train_test_split(qseq, aseq, test_size=test_split, random_state=seed)\n",
        "    \n",
        "    start_tag_removed_train_targets = [remove_start_tag(target) for target in train_targets]\n",
        "    start_tag_removed_test_targets = [remove_start_tag(target) for target in test_targets]\n",
        "\n",
        "    train_vectorized_inputs, train_vectorized_targets = Vectorizer(train_inputs), Vectorizer(start_tag_removed_train_targets)\n",
        "    test_vectorized_inputs, test_vectorized_targets = Vectorizer(test_inputs), Vectorizer(start_tag_removed_test_targets)\n",
        "\n",
        "    for idx, (input_tensor, target_tensor) in enumerate(zip(train_vectorized_inputs, train_vectorized_targets)):\n",
        "        input_list = list(input_tensor.numpy())\n",
        "        target_list = list(target_tensor.numpy())\n",
        "        unknown_count_q = input_list.count(unk_index)\n",
        "        unknown_count_a = target_list.count(unk_index)\n",
        "        if unknown_count_a <=1 :\n",
        "            if unknown_count_q > 0:\n",
        "                temp_list = list(filter(lambda num: num != 0, input_list)) # This list will have the inputs without zeros padded\n",
        "                if unknown_count_q/len(temp_list) > 0.2:\n",
        "                    continue\n",
        "            training_data['input'].append(train_inputs[idx])\n",
        "            training_data['target'].append(train_targets[idx])\n",
        "            training_data['input_vectors'].append(input_tensor)\n",
        "            training_data['target_vectors'].append(target_tensor)\n",
        "        \n",
        "    testing_data['input'], testing_data['target'] = test_inputs, test_targets \n",
        "    testing_data['input_vectors'], testing_data['target_vectors'] = test_vectorized_inputs, test_vectorized_targets\n",
        "\n",
        "    print(f'Training data points: {len(train_inputs)}')\n",
        "    print(f'Test data points: {len(test_inputs)}')\n",
        "    filt_data_len = len(training_data['input'])\n",
        "    filtered = int((len(train_inputs) - filt_data_len)*100/len(train_inputs))\n",
        "    print(f'{filtered}% filtered from training data points')\n",
        "    print(f'After unknown token filters training data points: {filt_data_len}')\n",
        "\n",
        "    return training_data, testing_data"
      ],
      "metadata": {
        "id": "uBkM_XSTYp2P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction_old(vocab_list, decoder_model_function, encoder_model: Model, input_text: str = 'hi', next_word: str = 'START_', clean_text = clean_text, max_length: int = 19, multi_layer: bool = True):\n",
        "    \"\"\"\n",
        "    This function takes inputs as follows and returns the model response.\n",
        "    input: vocab_list -> this is the list of voicabulary used in the model,\n",
        "            model_function -> this is a reference functions in wich the decoder model is defined, \n",
        "            encoder_model -> this is the encoder model which need to be used for input text encoding, \n",
        "            input_text -> this is the input phrase for which the model create the response the default value if 'hi', \n",
        "            next_word -> this is the trigger or start word for the decoder model, the default value is 'START_',\n",
        "            clean_text -> this is a referance of the function which need to be used for cleaning the text the default is 'clean_text' function written or imported in this python file,\n",
        "            max_length -> max length of the bot response defaults to 19\n",
        "            multi_layer -> if the model single layer then this has to be False by default it is True\n",
        "    output: bot_response -> this is the predicted response of the bot\n",
        "    \"\"\"\n",
        "    states_list = []\n",
        "    input_text = clean_text(input_text)\n",
        "    if multi_layer:\n",
        "        encoder_output = encoder_model.predict([input_text])\n",
        "    else:\n",
        "        encoder_output = [encoder_model.predict([input_text])]\n",
        "    for states in encoder_output:\n",
        "        states_list.append([tf.constant(states[0]), tf.constant(states[1])])\n",
        "    stop_condition = True\n",
        "    bot_response = \"\"\n",
        "    states = states_list\n",
        "    while stop_condition:\n",
        "        _, next_word, states = decoder_model_function(next_word, states, vocab_list)\n",
        "        if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "            break\n",
        "        bot_response += next_word + ' '\n",
        "    return bot_response"
      ],
      "metadata": {
        "id": "Xmur94DQPP2v"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(vocab_list, decoder_model_function, encoder_model: Model, input_text: str = 'hi', \n",
        "                    next_word: str = 'START_', clean_text = clean_text, max_length: int = 19, \n",
        "                    multi_layer: bool = True, go_beam: bool = False, top_values: int = 3, top_percentage: float = 50.0):\n",
        "    \"\"\"\n",
        "    This function takes inputs as follows and returns the model response.\n",
        "    input: vocab_list -> this is the list of voicabulary used in the model,\n",
        "            model_function -> this is a reference functions in wich the decoder model is defined, \n",
        "            encoder_model -> this is the encoder model which need to be used for input text encoding, \n",
        "            input_text -> this is the input phrase for which the model create the response the default value if 'hi', \n",
        "            next_word -> this is the trigger or start word for the decoder model, the default value is 'START_',\n",
        "            clean_text -> this is a referance of the function which need to be used for cleaning the text the default is 'clean_text' function written or imported in this python file,\n",
        "            max_length -> max length of the bot response defaults to 19\n",
        "            multi_layer -> if the model single layer then this has to be False. by default it is True\n",
        "            go_beam -> if more variety output is needed we can set this to true. by default it is False\n",
        "            top_values -> number of top values to consider from the output. defaults to 3 this works only if go_beam = True\n",
        "            top_percentage -> prbability of the different words to consider. defaults to 50. this also works only if go_beam = True\n",
        "    output: bot_response -> this is the predicted response of the bot\n",
        "    \"\"\"\n",
        "    def get_random_word(decoder_output: np.ndarray, vocab_list: list = vocab_list, top_values:int = top_values, top_percentage: float = top_percentage):\n",
        "      \"\"\"\n",
        "      Instead of getting the same word as output for given input this function will help to give the output with different combination of words.\n",
        "      This will get the probability distribution of the model as input.\n",
        "      from the probability distribution of outputs insetad of choosing the word with maximum probability this function will choose top words \n",
        "      with maximum probabilities and randomly chooses one word from that. for considering the top values this function uses the variable/value top_values=3 by default.\n",
        "      from the top words this will consider only the words with the given percentage top_percentage from the maximum percentage.\n",
        "      for example if the top 3 percentages and their words are [(80, hi), (50, hello), (20, welcome)] and if top_percentage = 50\n",
        "      the max_prob = 80\n",
        "      prob_percentage_value = (80/100) * 50 => 40\n",
        "      this will consider the words with probability more than 40 so the choise of words would be [(80, hi), (50, hello)]\n",
        "      from this list of words the function will pick a random word everytime\n",
        "      \"\"\"\n",
        "      word_prob_match = zip(decoder_output, vocab_list)\n",
        "      top_prob_list = sorted(word_prob_match, reverse=True)[:top_values]\n",
        "      max_prob = top_prob_list[0][0]\n",
        "      prob_percentage_val = (max_prob/100) * top_percentage\n",
        "      beam_word_list, beam_prob_list = [], []\n",
        "      for word_prob in top_prob_list:\n",
        "        word_probability = word_prob[0]\n",
        "        if word_probability > prob_percentage_val:\n",
        "          beam_prob_list.append(word_prob[0])\n",
        "          beam_word_list.append(word_prob[1])\n",
        "      next_word_idx = np.random.randint(low=0, high=len(beam_prob_list))\n",
        "      next_word = beam_word_list[next_word_idx]\n",
        "      return next_word\n",
        "\n",
        "    states_list = []\n",
        "    input_text = clean_text(input_text)\n",
        "    if multi_layer:\n",
        "        encoder_output = encoder_model.predict([input_text])\n",
        "    else:\n",
        "        encoder_output = [encoder_model.predict([input_text])]\n",
        "    for states in encoder_output:\n",
        "        states_list.append([tf.constant(states[0]), tf.constant(states[1])])\n",
        "    stop_condition = True\n",
        "    bot_response = \"\"\n",
        "    states = states_list\n",
        "    while stop_condition:\n",
        "        decoder_output, next_word, states = decoder_model_function(next_word, states, vocab_list)\n",
        "        if go_beam:\n",
        "          next_word = get_random_word(decoder_output=decoder_output, vocab_list=vocab_list, top_percentage=top_percentage)\n",
        "        if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "            break\n",
        "        bot_response += next_word + ' '\n",
        "    return bot_response"
      ],
      "metadata": {
        "id": "m7zBhuqk27Cy"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable setup\n",
        "\n",
        "Setting up the variable values for the entire program"
      ],
      "metadata": {
        "id": "PHbTXC31aZHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the variable for preparing the model\n",
        "only_start = False\n",
        "max_vocab_length = 15000\n",
        "max_length = 20\n",
        "test_split = 0.2\n",
        "random_seed = 42\n",
        "data_subset = -1\n",
        "subset = 'comedy'\n",
        "embedding_output_dimension = 128\n",
        "lstm_units = 400\n",
        "stacked_lstm_units = 256\n",
        "dropout_rate = 0.2\n",
        "epoch = 50\n",
        "sparse_loss_fun = 'sparse_categorical_crossentropy'\n",
        "one_hot_loss_fuc = 'categorical_crossentropy'"
      ],
      "metadata": {
        "id": "BixzsI92afMU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "preparing the datasets for model creation"
      ],
      "metadata": {
        "id": "ew85nvMjZ2u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data \n",
        "# Load the movie details meta data\n",
        "movie_titles = load_data(file_name='movie_titles_metadata.txt', data_location='/content/drive/MyDrive/Chatbot/data/')\n",
        "\n",
        "# Load the conversation meta data\n",
        "movie_conversations = load_data(file_name='movie_conversations.txt', data_location='/content/drive/MyDrive/Chatbot/data/')\n",
        "\n",
        "# Load the conversation lines\n",
        "movie_lines = load_data(file_name='movie_lines.txt', data_location='/content/drive/MyDrive/Chatbot/data/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJn9j9sKYwlF",
        "outputId": "dbd96fd9-2765-47d9-f44e-e657668abeaf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data read from /content/drive/MyDrive/Chatbot/data/movie_titles_metadata.txt and converted into 618 lines\n",
            "Data read from /content/drive/MyDrive/Chatbot/data/movie_conversations.txt and converted into 83098 lines\n",
            "Data read from /content/drive/MyDrive/Chatbot/data/movie_lines.txt and converted into 304714 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dictionary for all data\n",
        "movie_title_list, movie_conversation_list, movie_lines_list = prepare_data(movie_titles=movie_titles, movie_conversations=movie_conversations, movie_lines=movie_lines)"
      ],
      "metadata": {
        "id": "nTHy4B45aGbG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataframe from  the dictionary\n",
        "movie_title_df = dataframe_from_dict(data_dict_list=movie_title_list)\n",
        "movie_conversation_df = dataframe_from_dict(data_dict_list=movie_conversation_list)\n",
        "movie_lines_df = dataframe_from_dict(data_dict_list=movie_lines_list)"
      ],
      "metadata": {
        "id": "GEGdcrAFam0X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare genre dictionary\n",
        "genre_dict = get_genre_dict(movie_title_df=movie_title_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8e19NAzasQd",
        "outputId": "c868e4a3-8ad0-4cd4-cc65-76747d91e982"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre dictionary prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dialogue dict for final dataset\n",
        "dialogue_dict, conversation_data_df = prepare_conversations(movie_lines_df=movie_lines_df, movie_conversation_df=movie_conversation_df, only_start=only_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q9lPdXia03A",
        "outputId": "acca5574-5ca7-4146-8852-57e5c187531f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do cleaning of the text data\n",
        "conversation_data_df['input'] = conversation_data_df['input'].apply(clean_text)\n",
        "conversation_data_df['target'] = conversation_data_df['target'].apply(clean_text, add_tags=True)"
      ],
      "metadata": {
        "id": "-SP5oxjya5bD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering data which are not in appropriate length\n",
        "filtered_conversation_df = filter_short_long(conversation_data_df=conversation_data_df, \n",
        "                                                        min_q_length=2, max_q_length=20, \n",
        "                                                        min_a_length=2, max_a_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAvHdAKEbKnI",
        "outputId": "ed52fdf4-df07-4a8c-bb05-562737eef793"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33% filtered from original data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Vectorizer"
      ],
      "metadata": {
        "id": "7HzkZ6bJbnAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text vectorizer object\n",
        "Vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                output_mode=\"int\",\n",
        "                                output_sequence_length=max_length,\n",
        "                                standardize=None)"
      ],
      "metadata": {
        "id": "imiSJBHAbXKh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the text vectorizer for the dataset\n",
        "Vectorizer.adapt(filtered_conversation_df['target'].to_numpy())"
      ],
      "metadata": {
        "id": "BlD4ITMTbtSh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = Vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "exs1F0fPbyLM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4_ocymL14vS",
        "outputId": "218499a1-98d9-49a2-c0a1-993e9669ad92"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare traing and test datasets from subset of data"
      ],
      "metadata": {
        "id": "-HdhtwIkc036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only the comedy movies data\n",
        "subset_movies_list = genre_dict[subset]\n",
        "\n",
        "subset_movie_line_df = filtered_conversation_df[filtered_conversation_df['movie_id'].isin(subset_movies_list)][:data_subset]"
      ],
      "metadata": {
        "id": "pIV_XNJJcT23"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for datasets also remove the sentences with most unknown tokens\n",
        "training_data, testing_data = split_vectorize_filter_unk(conversation_data_df=subset_movie_line_df, Vectorizer=Vectorizer, test_split=test_split, seed=random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K4o4bvOd0ok",
        "outputId": "3c4f5dbc-7095-4649-fca1-65a0f3162491"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data points: 36973\n",
            "Test data points: 9244\n",
            "5% filtered from training data points\n",
            "After unknown token filters training data points: 35071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing datasets\n",
        "train_inputs = np.array(training_data['input'])\n",
        "train_targets = np.array(training_data['target'])\n",
        "#train_vector_targets = tf.keras.utils.to_categorical(np.array(training_data['target_vectors']), max_vocab_length)\n",
        "train_vector_targets = tf.expand_dims(tf.constant(np.array(training_data['target_vectors'])), axis=-1)\n",
        "\n",
        "test_inputs = np.array(testing_data['input'])\n",
        "test_targets = np.array(testing_data['target'])\n",
        "#test_vector_targets = tf.keras.utils.to_categorical(np.array(testing_data['target_vectors']), max_vocab_length)\n",
        "test_vector_targets = tf.expand_dims(tf.constant(np.array(testing_data['target_vectors'])), axis=-1)"
      ],
      "metadata": {
        "id": "KdkMNBGleYXX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataset for training and validation\n",
        "train_data_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
        "train_lables_dataset = tf.data.Dataset.from_tensor_slices(train_vector_targets)\n",
        "train_dataset = tf.data.Dataset.zip((train_data_dataset, train_lables_dataset))\n",
        "train_dataset = train_dataset.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_data_dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_targets))\n",
        "test_lables_dataset = tf.data.Dataset.from_tensor_slices(test_vector_targets)\n",
        "test_dataset = tf.data.Dataset.zip((test_data_dataset, test_lables_dataset))\n",
        "test_dataset = test_dataset.batch(128).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "3GMCmqe1ek3l"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare traing and test datasets from all available data"
      ],
      "metadata": {
        "id": "ryH64-D95uv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for datasets also remove the sentences with most unknown tokens\n",
        "training_data_all, testing_data_all = split_vectorize_filter_unk(conversation_data_df=filtered_conversation_df, Vectorizer=Vectorizer, test_split=test_split, seed=random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3g0Lwxs6Len",
        "outputId": "2ad1eaee-5ba6-4c54-8abf-c8670da23022"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data points: 118565\n",
            "Test data points: 29642\n",
            "4% filtered from training data points\n",
            "After unknown token filters training data points: 112655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing datasets\n",
        "train_inputs_all = np.array(training_data_all['input'])\n",
        "train_targets_all = np.array(training_data_all['target'])\n",
        "#train_vector_targets_all = tf.keras.utils.to_categorical(np.array(training_data_all['target_vectors']), max_vocab_length)\n",
        "train_vector_targets_all = tf.expand_dims(tf.constant(np.array(training_data_all['target_vectors'])), axis=-1)\n",
        "\n",
        "test_inputs_all = np.array(testing_data_all['input'])\n",
        "test_targets_all = np.array(testing_data_all['target'])\n",
        "#test_vector_targets_all = tf.keras.utils.to_categorical(np.array(testing_data_all['target_vectors']), max_vocab_length)\n",
        "test_vector_targets_all = tf.expand_dims(tf.constant(np.array(testing_data_all['target_vectors'])), axis=-1)"
      ],
      "metadata": {
        "id": "v7oQ9gz16Sjt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataset for training and validation\n",
        "train_data_dataset_all = tf.data.Dataset.from_tensor_slices((train_inputs_all, train_targets_all))\n",
        "train_lables_dataset_all = tf.data.Dataset.from_tensor_slices(train_vector_targets_all)\n",
        "train_dataset_all = tf.data.Dataset.zip((train_data_dataset_all, train_lables_dataset_all))\n",
        "train_dataset_all = train_dataset_all.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_data_dataset_all = tf.data.Dataset.from_tensor_slices((test_inputs_all, test_targets_all))\n",
        "test_lables_dataset_all = tf.data.Dataset.from_tensor_slices(test_vector_targets_all)\n",
        "test_dataset_all = tf.data.Dataset.zip((test_data_dataset_all, test_lables_dataset_all))\n",
        "test_dataset_all = test_dataset_all.batch(128).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "D2qcdqzt6jzv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "YeKD2Bp_YduR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "KTfvtk92YcVJ",
        "outputId": "0e029fb1-38c5-46fc-e597-2db415209d4d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-85ea980a1e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfail_here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'fail_here' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation"
      ],
      "metadata": {
        "id": "NNT3ppX8ezDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 with single layer LSTM\n",
        "\n",
        "* Input for this model is string (question, answer first word(START_))\n",
        "* Output will be the probability of the next word\n",
        "* This model has single layer of LSTM Units"
      ],
      "metadata": {
        "id": "Xq8zCUfve1wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating embedding object for encoder and decoder models\n",
        "EncoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='encoder_embedding_layer')\n",
        "\n",
        "DecoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='decoder_embedding_layer')"
      ],
      "metadata": {
        "id": "A44HSmIfflsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vector = Vectorizer(EncoderInput)\n",
        "\n",
        "DecoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vector = Vectorizer(DecoderInput)"
      ],
      "metadata": {
        "id": "smxZFfyJevw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoder\n",
        "encoder_embeddings = EncoderEmbeddingLayer(encoder_vector)\n",
        "EncoderLstmLayer = layers.LSTM(lstm_units, return_state=True, name='Encoder_LSTM')\n",
        "encoder_lstm_outputs, state_h, state_c = EncoderLstmLayer(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "GPRBWfWAfmir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Decoder\n",
        "decoder_embeddings = DecoderEmbeddingLayer(decoder_vector)\n",
        "DecoderLstmLayer = layers.LSTM(lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
        "decoder_lstm_outputs, _, _ = DecoderLstmLayer(decoder_embeddings, initial_state=encoder_states)\n",
        "DecoderDenseLayer = layers.Dense(max_vocab_length, activation='softmax', name='Decoder_dense')\n",
        "decoder_dense_outputs = DecoderDenseLayer(decoder_lstm_outputs)\n",
        "\n",
        "EncDecModel = Model([EncoderInput, DecoderInput], decoder_dense_outputs)"
      ],
      "metadata": {
        "id": "rexQhskhf2Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LIki08OXgBJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel = tf.keras.Model(EncoderInput, encoder_states)\n",
        "\n",
        "DecoderStateInputH = layers.Input(shape=(lstm_units,))\n",
        "DecoderStateInputC = layers.Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [DecoderStateInputH, DecoderStateInputC]\n",
        "\n",
        "decoder_vector_test = Vectorizer(DecoderInput)\n",
        "dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "\n",
        "decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=decoder_states_inputs)\n",
        "decoder_states_test = [state_h_test, state_c_test]\n",
        "decoder_dense_outputs_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "\n",
        "DecModel = Model(\n",
        "    inputs = [DecoderInput, decoder_states_inputs],\n",
        "    outputs = [decoder_dense_outputs_test] + decoder_states_test)"
      ],
      "metadata": {
        "id": "Xz8NQ-0dgGv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model weights\n",
        "EncDecModel.load_weights('/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmComedy50epochs/EncDecModel1Weights')"
      ],
      "metadata": {
        "id": "O4ZUQ-dRq2qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history = EncDecModel.fit(train_dataset,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "DIMe9n8YgjRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "ibwjvuxJOz0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test = Vectorizer([input_word])\n",
        "    dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "    decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=states[0])\n",
        "    decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "    decoder_output = decoder_dense_output_test[0, 0, :].numpy()\n",
        "    word_idx = tf.argmax(decoder_output).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_test), tf.constant(state_c_test)]\n",
        "    return decoder_output, next_word, states"
      ],
      "metadata": {
        "id": "CQ6DhucSmqdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human = 'hello'\n",
        "states = [EncModel.predict([human])]\n",
        "next_word = 'START_'\n",
        "stop_condition = True\n",
        "bot_response = \"\"\n",
        "states[0] = [tf.constant(states[0][0]), tf.constant(states[0][1])]\n",
        "while stop_condition:\n",
        "    next_word, states = decoder_model_test(next_word, states, vocab_list)\n",
        "    bot_response += next_word + ' '\n",
        "    if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "        stop_condition = False\n",
        "print(bot_response)"
      ],
      "metadata": {
        "id": "aSk2VNzBmoEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = False\n",
        "while not end_convo:\n",
        "  # Getting the input from user\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = True\n",
        "  # Encoding the input\n",
        "  stat = EncModel.predict([human])\n",
        "  next_word = 'START_'\n",
        "  stop_condition = True\n",
        "  bot_response = \"\"\n",
        "  stat = [tf.constant(stat[0]), tf.constant(stat[1])]\n",
        "  while stop_condition:\n",
        "      # Decoder model operations starts here\n",
        "      decoder_vector_test = Vectorizer([next_word])\n",
        "      dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "      decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=stat)\n",
        "      decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "      # Decoder model operations end here\n",
        "      word_idx = tf.argmax(decoder_dense_output_test[0, 0, :]).numpy()\n",
        "      next_word = vocab_list[word_idx]\n",
        "      bot_response += next_word + ' '\n",
        "      if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "          stop_condition = False\n",
        "      stat = [state_h_test, state_c_test]\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "YoZJ3YgMgu_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model_test, encoder_model=EncModel, input_text=human, clean_text=clean_text, multi_layer=False)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "KvHtMEK7O_lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmComedy50epochs/EncDecModel1Weights')"
      ],
      "metadata": {
        "id": "Y3uUu9ZBs8go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.load_weights('/content/drive/MyDrive/Chatbot/model_weights_GPU/EncDecModelWeights')"
      ],
      "metadata": {
        "id": "aFPrgGNQtjdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "9JB_TWEPik-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "id": "vM2KcJ20in7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 with stacked lstm\n",
        "* Input for this model is string (question, answer first word(START_))\n",
        "* Output will be the probability of the next word\n",
        "* This model has double layer of lstm units"
      ],
      "metadata": {
        "id": "uAC2wqGxSrol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creatimg embedding objects for encoder and decoder models\n",
        "EncoderEmbeddingLayerM2 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='encoder_embedding_layer_model2')\n",
        "\n",
        "DecoderEmbeddingLayerM2 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='decoder_embedding_layer_model2')"
      ],
      "metadata": {
        "id": "IsJX5C2PSzgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInputM2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vectorM2 = Vectorizer(EncoderInputM2)\n",
        "\n",
        "DecoderInputM2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vectorM2 = Vectorizer(DecoderInputM2)"
      ],
      "metadata": {
        "id": "rXfLOL80T3Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder\n",
        "encoder_embeddings_M2 = EncoderEmbeddingLayerM2(encoder_vectorM2)\n",
        "\n",
        "EncoderLstmLayer1M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, return_sequences=True, name='Encoder_LSTM_layer1_model2')\n",
        "encoder_outputs_layer1_M2, state_h_layer1_M2, state_c_layer1_M2 = EncoderLstmLayer1M2(encoder_embeddings_M2)\n",
        "encoder_states_layer1_M2 = [state_h_layer1_M2, state_c_layer1_M2]\n",
        "\n",
        "EncoderLstmLayer2M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, name='Encoder_LSTM2_layer2_model2')\n",
        "encoder_outputs_layer2_M2, state_h_layer2_M2, state_c_layer2_M2 = EncoderLstmLayer2M2(encoder_outputs_layer1_M2)\n",
        "encoder_states_layer2_M2 = [state_h_layer2_M2, state_c_layer2_M2]"
      ],
      "metadata": {
        "id": "epn5qktMUmFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create decoder\n",
        "decoder_embeddings_M2 = DecoderEmbeddingLayerM2(decoder_vectorM2)\n",
        "\n",
        "DecoderLstmLayer1M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer1_model2')\n",
        "decoder_outputs_layer1_M2, _, _ = DecoderLstmLayer1M2(decoder_embeddings_M2, initial_state=encoder_states_layer1_M2)\n",
        "\n",
        "DecoderLstmLayer2M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer2_model2')\n",
        "decoder_outputs_layer2_M2, _, _ = DecoderLstmLayer2M2(decoder_outputs_layer1_M2, initial_state=encoder_states_layer2_M2)\n",
        "\n",
        "DecoderDenseLayerM2 = tf.keras.layers.Dense(max_vocab_length, activation='softmax', name='Decoder_Dense_layer_model2')\n",
        "decoder_dense_outputs_M2 = DecoderDenseLayerM2(decoder_outputs_layer2_M2)\n",
        "\n",
        "EncDecModel2 = tf.keras.Model([EncoderInputM2, DecoderInputM2], decoder_dense_outputs_M2)"
      ],
      "metadata": {
        "id": "r0UG_qPsWBUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel2.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UaAYzruZazeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel2 = tf.keras.Model(EncoderInputM2, [encoder_states_layer1_M2, encoder_states_layer2_M2])\n",
        "\n",
        "DecoderStateInputHLayer1M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer1M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer1_M2 = [DecoderStateInputHLayer1M2, DecoderStateInputCLayer1M2]\n",
        "\n",
        "DecoderStateInputHLayer2M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer2M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer2_M2 = [DecoderStateInputHLayer2M2, DecoderStateInputCLayer2M2]\n",
        "\n",
        "decoder_vector_test_M2 = Vectorizer(DecoderInputM2)\n",
        "dec_embedding_test_M2 = DecoderEmbeddingLayerM2(decoder_vector_test_M2)\n",
        "\n",
        "decoder_lstm_outputs_test_layer1_M2, state_h_test_layer1_M1, state_c_test_layer1_M2 = DecoderLstmLayer1M2(dec_embedding_test_M2, initial_state=decoder_states_inputs_layer1_M2)\n",
        "decoder_states_test_layer1_M2 = [state_h_test_layer1_M1, state_c_test_layer1_M2]\n",
        "\n",
        "decoder_lstm_outputs_test_layer2_M2, state_h2_test_layer2, state_c2_test_layer2 = DecoderLstmLayer2M2(decoder_lstm_outputs_test_layer1_M2, initial_state=decoder_states_inputs_layer2_M2)\n",
        "decoder_states_test_layer2_M2 = [state_h2_test_layer2, state_c2_test_layer2]\n",
        "\n",
        "decoder_dense_outputs_test_M2 = DecoderDenseLayerM2(decoder_lstm_outputs_test_layer2_M2)\n",
        "\n",
        "DecModel2 = tf.keras.Model(\n",
        "    inputs = [DecoderInputM2, [decoder_states_inputs_layer1_M2, decoder_states_inputs_layer2_M2]],\n",
        "    outputs = [decoder_dense_outputs_test_M2] + [decoder_states_test_layer1_M2, decoder_states_test_layer2_M2])"
      ],
      "metadata": {
        "id": "5wKwxQKFdj6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel2.load_weights('/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmComedy50epochs/EncDecModel2Weights')"
      ],
      "metadata": {
        "id": "LAkbQnCcQqrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history2 = EncDecModel2.fit(train_dataset,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "242D0ea6gWBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel.evaluate(test_dataset) # 28.23%"
      ],
      "metadata": {
        "id": "ogJd2rhDQ0il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model2_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test_M2 = Vectorizer([input_word])\n",
        "    dec_embedding_test_M2 = DecoderEmbeddingLayerM2(decoder_vector_test_M2)\n",
        "    decoder_lstm_outputs_test_layer1_M2, state_h_l1_M2, state_c_l1_M2 = DecoderLstmLayer1M2(dec_embedding_test_M2, initial_state=states[0])\n",
        "    decoder_lstm_outputs_test_layer2_M2, state_h_l2_M2, state_c_l2_M2 = DecoderLstmLayer2M2(decoder_lstm_outputs_test_layer1_M2, initial_state=states[1])\n",
        "    decoder_dense_outputs_test_M2 = DecoderDenseLayerM2(decoder_lstm_outputs_test_layer2_M2)\n",
        "    decoder_output = decoder_dense_outputs_test_M2[0, 0, :].numpy()\n",
        "    word_idx = tf.argmax(decoder_output).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_l1_M2), tf.constant(state_c_l1_M2)]\n",
        "    states[1] = [tf.constant(state_h_l2_M2), tf.constant(state_c_l2_M2)]\n",
        "    return decoder_output, next_word, states"
      ],
      "metadata": {
        "id": "q_P5lSH4ti80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  states = EncModel2.predict([human])\n",
        "  next_word = 'START_'\n",
        "  stop_condition = True\n",
        "  bot_response = \"\"\n",
        "  states[0] = [tf.constant(states[0][0]), tf.constant(states[0][1])]\n",
        "  states[1] = [tf.constant(states[1][0]), tf.constant(states[1][1])]\n",
        "  while stop_condition:\n",
        "      next_word, states = decoder_model2_test(next_word, states, vocab_list)\n",
        "      bot_response += next_word + ' '\n",
        "      if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "          stop_condition = False\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "sULwRMubj1fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model2_test, encoder_model=EncModel2, input_text=human, clean_text=clean_text, multi_layer=True)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "gghUYsOURCJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel2.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmComedy50epochs/EncDecModel2Weights')"
      ],
      "metadata": {
        "id": "tJ-ykPDjki6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "X-E9ebch5Dqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "id": "BVpaNwdg5FEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3 with single layer lstm (all data)\n",
        "\n",
        "* This model is same as Model 1 but trained with all available data\n",
        "* This model has single layer onf LSTM untis"
      ],
      "metadata": {
        "id": "kNN46XpZ4iEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating embedding object for encoder and decoder models\n",
        "EncoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='encoder_embedding_layer')\n",
        "\n",
        "DecoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='decoder_embedding_layer')"
      ],
      "metadata": {
        "id": "R2uoU2Yg40ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vector = Vectorizer(EncoderInput)\n",
        "\n",
        "DecoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vector = Vectorizer(DecoderInput)"
      ],
      "metadata": {
        "id": "2vw9peOC7QNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoder\n",
        "encoder_embeddings = EncoderEmbeddingLayer(encoder_vector)\n",
        "EncoderLstmLayer = layers.LSTM(lstm_units, return_state=True, name='Encoder_LSTM')\n",
        "encoder_lstm_outputs, state_h, state_c = EncoderLstmLayer(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "3_VgW0197WQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Decoder\n",
        "decoder_embeddings = DecoderEmbeddingLayer(decoder_vector)\n",
        "DecoderLstmLayer = layers.LSTM(lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
        "decoder_lstm_outputs, _, _ = DecoderLstmLayer(decoder_embeddings, initial_state=encoder_states)\n",
        "DecoderDenseLayer = layers.Dense(max_vocab_length, activation='softmax', name='Decoder_dense')\n",
        "decoder_dense_outputs = DecoderDenseLayer(decoder_lstm_outputs)\n",
        "\n",
        "EncDecModel = Model([EncoderInput, DecoderInput], decoder_dense_outputs)"
      ],
      "metadata": {
        "id": "r3bXeaKW7XJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "o09F1WPp7a82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel = tf.keras.Model(EncoderInput, encoder_states)\n",
        "\n",
        "DecoderStateInputH = layers.Input(shape=(lstm_units,))\n",
        "DecoderStateInputC = layers.Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [DecoderStateInputH, DecoderStateInputC]\n",
        "\n",
        "decoder_vector_test = Vectorizer(DecoderInput)\n",
        "dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "\n",
        "decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=decoder_states_inputs)\n",
        "decoder_states_test = [state_h_test, state_c_test]\n",
        "decoder_dense_outputs_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "\n",
        "DecModel = Model(\n",
        "    inputs = [DecoderInput, decoder_states_inputs],\n",
        "    outputs = [decoder_dense_outputs_test] + decoder_states_test)"
      ],
      "metadata": {
        "id": "CAnNcQwY7fM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel.load_weights('/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmAllData50epochs/EncDecModel3Weights')"
      ],
      "metadata": {
        "id": "WS9BsycNRYbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history = EncDecModel.fit(train_dataset_all,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset_all)"
      ],
      "metadata": {
        "id": "b96FRK-U7m8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel.evaluate(test_dataset_all) # 28.27%"
      ],
      "metadata": {
        "id": "aFKm_FzzRg9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test = Vectorizer([input_word])\n",
        "    dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "    decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=states[0])\n",
        "    decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "    decoder_output = decoder_dense_output_test[0, 0, :].numpy()\n",
        "    word_idx = tf.argmax(decoder_output).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_test), tf.constant(state_c_test)]\n",
        "    return decoder_output, next_word, states"
      ],
      "metadata": {
        "id": "-1YyTTEc7otf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human = 'hello'\n",
        "states = [EncModel.predict([human])]\n",
        "next_word = 'START_'\n",
        "stop_condition = True\n",
        "bot_response = \"\"\n",
        "states[0] = [tf.constant(states[0][0]), tf.constant(states[0][1])]\n",
        "while stop_condition:\n",
        "    next_word, states = decoder_model_test(next_word, states, vocab_list)\n",
        "    bot_response += next_word + ' '\n",
        "    if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "        stop_condition = False\n",
        "print(bot_response)"
      ],
      "metadata": {
        "id": "oObG4H7L707F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = False\n",
        "while not end_convo:\n",
        "  # Getting the input from user\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = True\n",
        "  # Encoding the input\n",
        "  stat = EncModel.predict([human])\n",
        "  next_word = 'START_'\n",
        "  stop_condition = True\n",
        "  bot_response = \"\"\n",
        "  while stop_condition:\n",
        "      stat = [tf.constant(stat[0]), tf.constant(stat[1])]\n",
        "      # Decoder model operations starts here\n",
        "      decoder_vector_test = Vectorizer([next_word])\n",
        "      dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "      decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=stat)\n",
        "      decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "      # Decoder model operations end here\n",
        "      word_idx = tf.argmax(decoder_dense_output_test[0, 0, :]).numpy()\n",
        "      next_word = vocab_list[word_idx]\n",
        "      bot_response += next_word + ' '\n",
        "      if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "          stop_condition = False\n",
        "      stat = [state_h_test, state_c_test]\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "LvFWaORm74U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model_test, encoder_model=EncModel, input_text=human, clean_text=clean_text, multi_layer=False)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "zxeylIugSCj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmAllData50epochs/EncDecModel3Weights')"
      ],
      "metadata": {
        "id": "1Xua1lbT755M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "kxAh4W5H-vbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "id": "0a6KP5Q9-xDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4 with dual layer lstm (all data)\n",
        "\n",
        "* This model is same as Model 2 but trained with all available data\n",
        "* This model has dual layer of LSTM untis"
      ],
      "metadata": {
        "id": "q1gJsScj-kAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creatimg embedding objects for encoder and decoder models\n",
        "EncoderEmbeddingLayerM4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='encoder_embedding_layer_model4')\n",
        "\n",
        "DecoderEmbeddingLayerM4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='decoder_embedding_layer_model4')"
      ],
      "metadata": {
        "id": "Fc4L4l2Q-qVG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInputM4 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vectorM4 = Vectorizer(EncoderInputM4)\n",
        "\n",
        "DecoderInputM4 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vectorM4 = Vectorizer(DecoderInputM4)"
      ],
      "metadata": {
        "id": "Aa49TDFE-9LA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder\n",
        "encoder_embeddings_M4 = EncoderEmbeddingLayerM4(encoder_vectorM4)\n",
        "\n",
        "EncoderLstmLayer1M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, return_sequences=True, name='Encoder_LSTM_layer1_model4')\n",
        "encoder_outputs_layer1_M4, state_h_layer1_M4, state_c_layer1_M4 = EncoderLstmLayer1M4(encoder_embeddings_M4)\n",
        "encoder_states_layer1_M4 = [state_h_layer1_M4, state_c_layer1_M4]\n",
        "\n",
        "EncoderLstmLayer2M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, name='Encoder_LSTM2_layer2_model4')\n",
        "encoder_outputs_layer2_M4, state_h_layer2_M4, state_c_layer2_M4 = EncoderLstmLayer2M4(encoder_outputs_layer1_M4)\n",
        "encoder_states_layer2_M4 = [state_h_layer2_M4, state_c_layer2_M4]"
      ],
      "metadata": {
        "id": "E-BoW4zU-99K"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create decoder\n",
        "decoder_embeddings_M4 = DecoderEmbeddingLayerM4(decoder_vectorM4)\n",
        "\n",
        "DecoderLstmLayer1M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer1_model4')\n",
        "decoder_outputs_layer1_M4, _, _ = DecoderLstmLayer1M4(decoder_embeddings_M4, initial_state=encoder_states_layer1_M4)\n",
        "\n",
        "DecoderLstmLayer2M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer2_model4')\n",
        "decoder_outputs_layer2_M4, _, _ = DecoderLstmLayer2M4(decoder_outputs_layer1_M4, initial_state=encoder_states_layer2_M4)\n",
        "\n",
        "DecoderDenseLayerM4 = tf.keras.layers.Dense(max_vocab_length, activation='softmax', name='Decoder_Dense_layer_model4')\n",
        "decoder_dense_outputs_M4 = DecoderDenseLayerM4(decoder_outputs_layer2_M4)\n",
        "\n",
        "EncDecModel4 = tf.keras.Model([EncoderInputM4, DecoderInputM4], decoder_dense_outputs_M4)"
      ],
      "metadata": {
        "id": "clLW2uIE_BCV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel4.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2jyCHvKi_HlV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel4 = tf.keras.Model(EncoderInputM4, [encoder_states_layer1_M4, encoder_states_layer2_M4])\n",
        "\n",
        "DecoderStateInputHLayer1M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer1M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer1_M4 = [DecoderStateInputHLayer1M4, DecoderStateInputCLayer1M4]\n",
        "\n",
        "DecoderStateInputHLayer2M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer2M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer2_M4 = [DecoderStateInputHLayer2M4, DecoderStateInputCLayer2M4]\n",
        "\n",
        "decoder_vector_test_M4 = Vectorizer(DecoderInputM4)\n",
        "dec_embedding_test_M4 = DecoderEmbeddingLayerM4(decoder_vector_test_M4)\n",
        "\n",
        "decoder_lstm_outputs_test_layer1_M4, state_h_test_layer1_M4, state_c_test_layer1_M4 = DecoderLstmLayer1M4(dec_embedding_test_M4, initial_state=decoder_states_inputs_layer1_M4)\n",
        "decoder_states_test_layer1_M4 = [state_h_test_layer1_M4, state_c_test_layer1_M4]\n",
        "\n",
        "decoder_lstm_outputs_test_layer2_M4, state_h2_test_layer2, state_c2_test_layer2 = DecoderLstmLayer2M4(decoder_lstm_outputs_test_layer1_M4, initial_state=decoder_states_inputs_layer2_M4)\n",
        "decoder_states_test_layer2_M4 = [state_h2_test_layer2, state_c2_test_layer2]\n",
        "\n",
        "decoder_dense_outputs_test_M4 = DecoderDenseLayerM4(decoder_lstm_outputs_test_layer2_M4)\n",
        "\n",
        "DecModel4 = tf.keras.Model(\n",
        "    inputs = [DecoderInputM4, [decoder_states_inputs_layer1_M4, decoder_states_inputs_layer2_M4]],\n",
        "    outputs = [decoder_dense_outputs_test_M4] + [decoder_states_test_layer1_M4, decoder_states_test_layer2_M4])"
      ],
      "metadata": {
        "id": "v1oig1X2_M9q"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel4.load_weights('/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmAllData50epochs/EncDecModel4Weights')"
      ],
      "metadata": {
        "id": "DE8K7vDfSUTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2a14a8-bb17-48fb-e415-6c76677c97cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2e1fcbf350>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history4 = EncDecModel4.fit(train_dataset_all,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset_all)"
      ],
      "metadata": {
        "id": "FYuOcGKL_ROP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel4.evaluate(test_dataset_all) # 29.13%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBmlPaInSr42",
        "outputId": "7cb03829-1666-46bd-9ffd-182caef41485"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232/232 [==============================] - 20s 38ms/step - loss: 2.4871 - accuracy: 0.2913\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4870564937591553, 0.29128652811050415]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model4_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test_M4 = Vectorizer([input_word])\n",
        "    dec_embedding_test_M4 = DecoderEmbeddingLayerM4(decoder_vector_test_M4)\n",
        "    decoder_lstm_outputs_test_layer1_M4, state_h_l1_M4, state_c_l1_M4 = DecoderLstmLayer1M4(dec_embedding_test_M4, initial_state=states[0])\n",
        "    decoder_lstm_outputs_test_layer2_M4, state_h_l2_M4, state_c_l2_M4 = DecoderLstmLayer2M4(decoder_lstm_outputs_test_layer1_M4, initial_state=states[1])\n",
        "    decoder_dense_outputs_test_M4 = DecoderDenseLayerM4(decoder_lstm_outputs_test_layer2_M4)\n",
        "    decoder_output = decoder_dense_outputs_test_M4[0, 0, :].numpy()\n",
        "    word_idx = tf.argmax(decoder_output).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_l1_M4), tf.constant(state_c_l1_M4)]\n",
        "    states[1] = [tf.constant(state_h_l2_M4), tf.constant(state_c_l2_M4)]\n",
        "    return decoder_output, next_word, states"
      ],
      "metadata": {
        "id": "6D-H_VOA_XJO"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model4_test, \n",
        "                                 encoder_model=EncModel4, input_text=human, clean_text=clean_text, multi_layer=True,\n",
        "                                 go_beam=True)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4lO-G7jL4zT",
        "outputId": "443bcc48-3416-46e3-dacb-2254eeefbfbd"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: hello what do you want \n",
            "Human: how are you\n",
            "KATTA: okay \n",
            "Human: great\n",
            "KATTA: well i could have told you \n",
            "Human: END CONVO\n",
            "KATTA: threatening me \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Human: hi\n",
        "KATTA: hello _END \n",
        "Human: what\n",
        "KATTA: do not let me go _END \n",
        "Human: aho are you\n",
        "KATTA: not you _END \n",
        "Human: who are you\n",
        "KATTA: gondorff asked me to meet you _END \n",
        "Human: are you a robot\n",
        "KATTA: no _END \n",
        "Human: are you a human\n",
        "KATTA: yes _END \n",
        "Human: why are you here\n",
        "KATTA: i do not know i am sorry _END \n",
        "Human: i like you\n",
        "KATTA: sure you have got to do is not that bad _END \n",
        "Human: do you like me\n",
        "KATTA: it was not me _END \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6ciEkDBBMwmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel4.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmAllData50epochs/EncDecModel4Weights')"
      ],
      "metadata": {
        "id": "CJPov975SSbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "p1TsQWdThRaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aFTBHi5itSYv"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human = 'how are you'\n",
        "make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model4_test, \n",
        "                                   encoder_model=EncModel4, input_text=human, clean_text=clean_text, \n",
        "                                   multi_layer=True, go_beam=True, top_percentage=75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QKpe51m_tRtf",
        "outputId": "a7633291-16d9-434f-94df-a32a3e0912e4"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'all right '"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gNVBvksS35_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}