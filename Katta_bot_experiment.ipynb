{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Katta_bot_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dzjvhk7pLxqj",
        "PHbTXC31aZHm",
        "ew85nvMjZ2u3",
        "7HzkZ6bJbnAb",
        "-HdhtwIkc036",
        "ryH64-D95uv9",
        "9JB_TWEPik-O",
        "X-E9ebch5Dqd",
        "kxAh4W5H-vbp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KATTA bot local\n",
        "\n",
        "This bot code is a replica of the local code"
      ],
      "metadata": {
        "id": "_0tlGbQjLisE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENV setup\n",
        "\n",
        "Setup libraries and dependent data files "
      ],
      "metadata": {
        "id": "Dzjvhk7pLxqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pArwJ2WhD-o",
        "outputId": "be6a468b-ebdd-494a-b0cd-715d6551bf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z7HWgNBLIbr"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries for data preparation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up data utilities\n",
        "\n",
        "Creating the required functions for preparing the data"
      ],
      "metadata": {
        "id": "Q81GXw8yT43k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_name: str, data_location: str = './data/'):\n",
        "    \"\"\"\n",
        "    This function will be loading the data from the filenames which are given as input and return the list of lines from the data file\n",
        "    input: file_name -> str, data_location -> str = ./data/ by default\n",
        "    output: lines -> list data lines list from the input file\n",
        "    \"\"\"\n",
        "    def fix_dir(dir_name: str):\n",
        "        if dir_name[-1] == '/':\n",
        "            return dir_name\n",
        "        return dir_name + '/'\n",
        "    \n",
        "    data_file = fix_dir(data_location) + file_name\n",
        "    with open(data_file, 'r', encoding='utf-8', errors='ignore') as dfile:\n",
        "        lines = dfile.read().split('\\n')\n",
        "    \n",
        "    print(f'Data read from {data_file} and converted into {len(lines)} lines')\n",
        "\n",
        "    return lines"
      ],
      "metadata": {
        "id": "I6bF_nJ9PviO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(movie_titles: list, movie_conversations: list, movie_lines: list):\n",
        "    \"\"\"\n",
        "    This function prepares data dictionary for each files it outputs list of dictionaries for all the major datasets \n",
        "    inputs: movie_titles -> list, movie_conversations -> list, movie_lines -> list\n",
        "    outputs: movie_title_list -> list(dict), movie_conversation_list -> list(dict), movie_lines_list -> list(dict)\n",
        "    \"\"\"\n",
        "    # Prepare dictionary for movie meta data\n",
        "    movie_title_list = []\n",
        "    for line in movie_titles:\n",
        "        if not line:\n",
        "            continue # for identifying and ignoring empty lines\n",
        "        movie_title_info = {}\n",
        "        movie_info = line.split(' +++$+++ ')\n",
        "        movie_title_info['movie_id'] = movie_info[0].strip()\n",
        "        movie_title_info['name'] = movie_info[1].strip()\n",
        "        movie_title_info['year'] = movie_info[2].strip()\n",
        "        movie_title_info['rating'] = movie_info[3].strip()\n",
        "        movie_title_info['genre'] = movie_info[-1][2:-2].strip().split(\"', '\") # this is for splitting the genres from ['comedy', 'romance'] to a list\n",
        "        movie_title_list.append(movie_title_info)\n",
        "\n",
        "    # Prepare dictionary for movie convo meta data\n",
        "    movie_conversation_list = []\n",
        "    for line in movie_conversations:\n",
        "        if not line:\n",
        "            continue # for identifying and ignoring empty lines\n",
        "        movie_conversation_info = {}\n",
        "        conversation_info = line.split(' +++$+++ ')\n",
        "        movie_conversation_info['speaker1'] = conversation_info[0].strip()\n",
        "        movie_conversation_info['speaker2'] = conversation_info[1].strip()\n",
        "        movie_conversation_info['movie_id'] = conversation_info[2].strip()\n",
        "        movie_conversation_info['line_ids'] = conversation_info[-1][2:-2].strip().split(\"', '\")# this is for splitting the conversation info from ['L198', 'L199'] to a list\n",
        "        movie_conversation_list.append(movie_conversation_info)\n",
        "\n",
        "    # Prepare dictionary for movie dialogues\n",
        "    movie_lines_list = []\n",
        "    for line in movie_lines:\n",
        "        if not line:\n",
        "            continue # for identifying and ignoring empty lines\n",
        "        movie_line_info = {}\n",
        "        line_info = line.split(' +++$+++ ')\n",
        "        movie_line_info['line_id'] = line_info[0].strip()\n",
        "        movie_line_info['speaker'] = line_info[1].strip()\n",
        "        movie_line_info['movie_id'] = line_info[2].strip()\n",
        "        movie_line_info['character'] = line_info[3].strip()\n",
        "        movie_line_info['dialogue'] = line_info[-1].strip()\n",
        "        movie_lines_list.append(movie_line_info)\n",
        "\n",
        "    return movie_title_list, movie_conversation_list, movie_lines_list"
      ],
      "metadata": {
        "id": "6NZcLq9cUM4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_from_dict(data_dict_list: list):\n",
        "    \"\"\"\n",
        "    This function converts the list of dictionaries into pandas dataframe\n",
        "    input: data_dict_list -> list(dict)\n",
        "    output: pandas dataframe prepared from the list\n",
        "    \"\"\"\n",
        "    return pd.DataFrame.from_dict(data_dict_list)"
      ],
      "metadata": {
        "id": "NU_HFQpJUPNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_genre_dict(movie_title_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    This line takes the input as movie titles pandas dataframe and prepares the genre dict\n",
        "    input: movie_title_df -> pandas.DataFrame\n",
        "    output: genre_dict -> dict the output will have the dictionary with keys as genre and values as list of movies from that genre\n",
        "    \"\"\"\n",
        "    # Get the list of available genres from the whole dataset \n",
        "    genres = movie_title_df['genre'].to_numpy()\n",
        "    genre_set = set()\n",
        "    for genre_list in genres:\n",
        "        for genre in genre_list:\n",
        "            if genre:\n",
        "                genre_set.add(genre)\n",
        "    \n",
        "    # Checking the count of movies in each genres and storing the movies with respect to their genres in the dictionary\n",
        "    genre_dict = {}\n",
        "    for genre_name in genre_set:\n",
        "        genre_dict[genre_name] = []\n",
        "    for movie, genre_list in movie_title_df[['movie_id', 'genre']].to_numpy():\n",
        "        for genre in genre_list:\n",
        "            if genre:\n",
        "              genre_dict[genre].append(movie)\n",
        "    \n",
        "    print('Genre dictionary prepared')\n",
        "\n",
        "    return genre_dict"
      ],
      "metadata": {
        "id": "Mg8QPq23URgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_conversations(movie_lines_df: pd.DataFrame, movie_conversation_df: pd.DataFrame, only_start: bool = False):\n",
        "    \"\"\"\n",
        "    This line takes the input as movie lines pandas dataframe and prepares the genre dict\n",
        "    input: movie_lines_df -> pandas.DataFrame, movie_conversation_df -> pandas.DataFrame\n",
        "    output: dialogue_dict -> dict dictionary with line_id as key and respective line as value, conversation_data_df -> pandas.DataFrame will have question and answers dataframe\n",
        "    \"\"\"\n",
        "    # Make conversation line dictionary for preparing the final dataset\n",
        "    dialogue_ids = movie_lines_df['line_id'].to_numpy()\n",
        "    dialogue_lines = movie_lines_df['dialogue'].to_numpy()\n",
        "    dialogue_dict = {}\n",
        "    for dialogue_id, dialogue_line in zip(dialogue_ids, dialogue_lines):\n",
        "        dialogue_dict[dialogue_id] = dialogue_line\n",
        "\n",
        "    # prepare final/actual dictionary for creating the chat bot\n",
        "    # This dictionary will have the conversation wise data.\n",
        "    conversation_data_dict = {}\n",
        "    conversation_data_dict['movie_id'] = []\n",
        "    conversation_data_dict['input'] = []\n",
        "    conversation_data_dict['target'] = []\n",
        "    for movie_id, convo_list in movie_conversation_df[['movie_id', 'line_ids']].to_numpy():\n",
        "        for convos in range(len(convo_list)-1):\n",
        "            conversation_data_dict['movie_id'].append(movie_id)\n",
        "            conversation_data_dict['input'].append(dialogue_dict[convo_list[convos]])\n",
        "            conversation_data_dict['target'].append(dialogue_dict[convo_list[convos+1]])\n",
        "            if only_start:\n",
        "              break\n",
        "\n",
        "    # Prepare dataframe from the dictionary for better access\n",
        "    conversation_data_df = pd.DataFrame.from_dict(conversation_data_dict)\n",
        "    print('Conversations prepared')\n",
        "    \n",
        "    return dialogue_dict, conversation_data_df"
      ],
      "metadata": {
        "id": "xJWwWG28UTx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function for data cleaning\n",
        "def clean_text(input_text: str, add_tags: bool = False, start_tag: str = 'START_ ', end_tag: str = ' _END', \n",
        "                remove_punc: bool = True, remove_symbols: str = '[^0-9a-z #+_]', ignore_words: list = [], \n",
        "                remove_numbers: bool = True, replace_word_from: list = [], replace_word_to: list = []):\n",
        "    \"\"\"\n",
        "    Input: input_text (string), add_tags (optional - bool), start_tag (optional - string), end_tag (optional - string), \n",
        "            remove_punc (optional - bool), remove_symbols (optional - string), ignore_words (optional - list), remove_numbers (optional - bool),\n",
        "            replace_word_from (optional - bool), replace_word_to (optional - bool)\n",
        "    Output: cleaned text (string)\n",
        "    description:\n",
        "        This function will clean the input text given by removong the bad symbols, numbers, punctuations, extra spaces... and return back the cleaned text\n",
        "        if the add_tags value is True (it's False by default) it will add the start tag and end tags at the start and end of the text\n",
        "        we can also define the start_tag and end_tag values\n",
        "    \"\"\"\n",
        "    def replace_common_words(text: str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(\"i'm\", \"i am\", text)\n",
        "        text = re.sub(\"he's\", \"he is\", text)\n",
        "        text = re.sub(\"she's\", \"she is\", text)\n",
        "        text = re.sub(\"that's\", \"that is\", text)\n",
        "        text = re.sub(\"what's\", \"what is\", text)\n",
        "        text = re.sub(\"where's\", \"where is\", text)\n",
        "        text = re.sub(\"'ll\", \" will\", text)\n",
        "        text = re.sub(\"'ve\", \" have\", text)\n",
        "        text = re.sub(\"'re\", \" are\", text)\n",
        "        text = re.sub(\"'d\", \" would\", text)\n",
        "        text = re.sub(\"n't\", \" not\", text)\n",
        "        return text\n",
        "\n",
        "    def remove_punctuation(text: str):\n",
        "        punctuation_list = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in punctuation_list)\n",
        "\n",
        "    def remove_bad_symbols(text: str, symbols: str):\n",
        "        bad_symbols = re.compile(symbols)\n",
        "        return bad_symbols.sub(' ', text)\n",
        "\n",
        "    def remove_extra_space(text: str):\n",
        "        extra_space = re.compile(' +')\n",
        "        return extra_space.sub(' ', text)\n",
        "\n",
        "    def remove_ignore_words(text: str, ignore_words_list: list):\n",
        "        for word in ignore_words_list:\n",
        "            text = text.replace(word, \" \")\n",
        "        return text\n",
        "    \n",
        "    def remove_digits(text:str):\n",
        "        remove_digit = str.maketrans('', '', string.digits)\n",
        "        return text.translate(remove_digit)\n",
        "\n",
        "    def replace_words(text: str, replace_word_list_from: list, replace_word_list_to: list):\n",
        "        for from_word, to_word in zip(replace_word_list_from, replace_word_list_to):\n",
        "            text = text.replace(str(from_word).lower(), str(to_word).lower())\n",
        "        return text\n",
        "\n",
        "    def add_start_end_tags(text: str):\n",
        "        return start_tag + text + end_tag\n",
        "\n",
        "    input_text = input_text.lower()\n",
        "    input_text = replace_common_words(input_text)\n",
        "    input_text = replace_words(input_text, replace_word_from, replace_word_to) if replace_word_from and (len(replace_word_from) == len(replace_word_to)) else input_text\n",
        "    input_text = remove_ignore_words(input_text, ignore_words) if ignore_words else input_text\n",
        "    input_text = remove_digits(input_text) if remove_numbers else input_text\n",
        "    input_text = remove_punctuation(input_text) if remove_punc else input_text\n",
        "    input_text = remove_bad_symbols(input_text, remove_symbols) if remove_symbols else input_text\n",
        "    input_text = add_start_end_tags(input_text) if add_tags else input_text\n",
        "    input_text = remove_extra_space(input_text)\n",
        "    #print('Data cleaning done')\n",
        "    \n",
        "    return input_text.strip()"
      ],
      "metadata": {
        "id": "t5BaV0yJUWBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_short_long(conversation_data_df: pd.DataFrame, min_q_length: int = 2, max_q_length: int = 25, min_a_length: int = 2, max_a_length: int = 25):\n",
        "    \"\"\"\n",
        "    This function takes list of input dialogues and list of target dialogues and returns only the dialogues with given length\n",
        "    input: conversation_data_df -> pandas.DataFrame\n",
        "    output: filtered_conversation_df -> pandas.DataFrame\n",
        "    \"\"\"\n",
        "    movie_id_seq, qseq, aseq = conversation_data_df['movie_id'].to_numpy(), conversation_data_df['input'].to_numpy(), conversation_data_df['target'].to_numpy()\n",
        "    conversation_data_dict = {}\n",
        "    conversation_data_dict['movie_id'], conversation_data_dict['input'], conversation_data_dict['target'] = [], [], []\n",
        "    raw_data_len = len(movie_id_seq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= min_q_length and qlen <= max_q_length:\n",
        "            if alen >= min_a_length and alen <= max_a_length:\n",
        "                conversation_data_dict['movie_id'].append(movie_id_seq[i])\n",
        "                conversation_data_dict['input'].append(qseq[i])\n",
        "                conversation_data_dict['target'].append(aseq[i])\n",
        "    \n",
        "    filt_data_len = len(conversation_data_dict['movie_id'])\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(f'{filtered}% filtered from original data')\n",
        "\n",
        "    return pd.DataFrame.from_dict(conversation_data_dict)"
      ],
      "metadata": {
        "id": "xsMgpAMAYeW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_vectorize_filter_unk(conversation_data_df: pd.DataFrame, Vectorizer: TextVectorization, unk: str = '[UNK]', test_split: float = 0.2, seed: int = 42):\n",
        "    \"\"\"\n",
        "    This function takes list of input dialogues and list of target dialogues and returns only the dialogues with less unknown tokens\n",
        "    input: conversation_data_df -> pandas.DataFrame, vectorizer object\n",
        "    output: training_data -> dict data needed for training, testing_data -> data needed for testing\n",
        "    \"\"\"\n",
        "    def remove_start_tag(input_with_start_tag: str):\n",
        "        return ' '.join(input_with_start_tag.split()[1:])\n",
        "\n",
        "    movie_id_seq, qseq, aseq = conversation_data_df['movie_id'].to_numpy(), conversation_data_df['input'].to_numpy(), conversation_data_df['target'].to_numpy()\n",
        "    training_data = {}\n",
        "    testing_data = {}\n",
        "    training_data['input'], training_data['target'], training_data['input_vectors'], training_data['target_vectors'] = [], [], [], []\n",
        "    testing_data['input'], testing_data['target'], testing_data['input_vectors'], testing_data['target_vectors'] = [], [], [], []\n",
        "\n",
        "    raw_data_len = len(movie_id_seq)\n",
        "    vocab_list = Vectorizer.get_vocabulary()\n",
        "    unk_index = vocab_list.index(unk)\n",
        "\n",
        "    train_inputs, test_inputs, train_targets, test_targets = train_test_split(qseq, aseq, test_size=test_split, random_state=seed)\n",
        "    \n",
        "    start_tag_removed_train_targets = [remove_start_tag(target) for target in train_targets]\n",
        "    start_tag_removed_test_targets = [remove_start_tag(target) for target in test_targets]\n",
        "\n",
        "    train_vectorized_inputs, train_vectorized_targets = Vectorizer(train_inputs), Vectorizer(start_tag_removed_train_targets)\n",
        "    test_vectorized_inputs, test_vectorized_targets = Vectorizer(test_inputs), Vectorizer(start_tag_removed_test_targets)\n",
        "\n",
        "    for idx, (input_tensor, target_tensor) in enumerate(zip(train_vectorized_inputs, train_vectorized_targets)):\n",
        "        input_list = list(input_tensor.numpy())\n",
        "        target_list = list(target_tensor.numpy())\n",
        "        unknown_count_q = input_list.count(unk_index)\n",
        "        unknown_count_a = target_list.count(unk_index)\n",
        "        if unknown_count_a <=1 :\n",
        "            if unknown_count_q > 0:\n",
        "                temp_list = list(filter(lambda num: num != 0, input_list)) # This list will have the inputs without zeros padded\n",
        "                if unknown_count_q/len(temp_list) > 0.2:\n",
        "                    continue\n",
        "            training_data['input'].append(train_inputs[idx])\n",
        "            training_data['target'].append(train_targets[idx])\n",
        "            training_data['input_vectors'].append(input_tensor)\n",
        "            training_data['target_vectors'].append(target_tensor)\n",
        "        \n",
        "    testing_data['input'], testing_data['target'] = test_inputs, test_targets \n",
        "    testing_data['input_vectors'], testing_data['target_vectors'] = test_vectorized_inputs, test_vectorized_targets\n",
        "\n",
        "    print(f'Training data points: {len(train_inputs)}')\n",
        "    print(f'Test data points: {len(test_inputs)}')\n",
        "    filt_data_len = len(training_data['input'])\n",
        "    filtered = int((len(train_inputs) - filt_data_len)*100/len(train_inputs))\n",
        "    print(f'{filtered}% filtered from training data points')\n",
        "    print(f'After unknown token filters training data points: {filt_data_len}')\n",
        "\n",
        "    return training_data, testing_data"
      ],
      "metadata": {
        "id": "uBkM_XSTYp2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(vocab_list, decoder_model_function, encoder_model: Model, input_text: str = 'hi', next_word: str = 'START_', clean_text = clean_text, max_length: int = 19, multi_layer: bool = True):\n",
        "    \"\"\"\n",
        "    This function takes inputs as follows and returns the model response.\n",
        "    input: vocab_list -> this is the list of voicabulary used in the model,\n",
        "            model_function -> this is a reference functions in wich the decoder model is defined, \n",
        "            encoder_model -> this is the encoder model which need to be used for input text encoding, \n",
        "            input_text -> this is the input phrase for which the model create the response the default value if 'hi', \n",
        "            next_word -> this is the trigger or start word for the decoder model, the default value is 'START_',\n",
        "            clean_text -> this is a referance of the function which need to be used for cleaning the text the default is 'clean_text' function written or imported in this python file,\n",
        "            max_length -> max length of the bot response defaults to 19\n",
        "            multi_layer -> if the model single layer then this has to be False by default it is True\n",
        "    output: bot_response -> this is the predicted response of the bot\n",
        "    \"\"\"\n",
        "    states_list = []\n",
        "    input_text = clean_text(input_text)\n",
        "    if multi_layer:\n",
        "        encoder_output = encoder_model.predict([input_text])\n",
        "    else:\n",
        "        encoder_output = [encoder_model.predict([input_text])]\n",
        "    for states in encoder_output:\n",
        "        states_list.append([tf.constant(states[0]), tf.constant(states[1])])\n",
        "    stop_condition = True\n",
        "    bot_response = \"\"\n",
        "    states = states_list\n",
        "    while stop_condition:\n",
        "        next_word, states = decoder_model_function(next_word, states, vocab_list)\n",
        "        if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "            break\n",
        "        bot_response += next_word + ' '\n",
        "    return bot_response"
      ],
      "metadata": {
        "id": "Xmur94DQPP2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable setup\n",
        "\n",
        "Setting up the variable values for the entire program"
      ],
      "metadata": {
        "id": "PHbTXC31aZHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the variable for preparing the model\n",
        "only_start = False\n",
        "max_vocab_length = 15000\n",
        "max_length = 20\n",
        "test_split = 0.2\n",
        "random_seed = 42\n",
        "data_subset = -1\n",
        "subset = 'comedy'\n",
        "embedding_output_dimension = 128\n",
        "lstm_units = 400\n",
        "stacked_lstm_units = 256\n",
        "dropout_rate = 0.2\n",
        "epoch = 50\n",
        "sparse_loss_fun = 'sparse_categorical_crossentropy'\n",
        "one_hot_loss_fuc = 'categorical_crossentropy'"
      ],
      "metadata": {
        "id": "BixzsI92afMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "preparing the datasets for model creation"
      ],
      "metadata": {
        "id": "ew85nvMjZ2u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data \n",
        "# Load the movie details meta data\n",
        "movie_titles = load_data(file_name='movie_titles_metadata.txt', data_location='/content/drive/MyDrive/Chatbot/data/')\n",
        "\n",
        "# Load the conversation meta data\n",
        "movie_conversations = load_data(file_name='movie_conversations.txt', data_location='/content/drive/MyDrive/Chatbot/data/')\n",
        "\n",
        "# Load the conversation lines\n",
        "movie_lines = load_data(file_name='movie_lines.txt', data_location='/content/drive/MyDrive/Chatbot/data/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJn9j9sKYwlF",
        "outputId": "4fd69ab6-48f1-4622-a568-c4e4c8fd9498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data read from /content/drive/MyDrive/Chatbot/data/movie_titles_metadata.txt and converted into 618 lines\n",
            "Data read from /content/drive/MyDrive/Chatbot/data/movie_conversations.txt and converted into 83098 lines\n",
            "Data read from /content/drive/MyDrive/Chatbot/data/movie_lines.txt and converted into 304714 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dictionary for all data\n",
        "movie_title_list, movie_conversation_list, movie_lines_list = prepare_data(movie_titles=movie_titles, movie_conversations=movie_conversations, movie_lines=movie_lines)"
      ],
      "metadata": {
        "id": "nTHy4B45aGbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataframe from  the dictionary\n",
        "movie_title_df = dataframe_from_dict(data_dict_list=movie_title_list)\n",
        "movie_conversation_df = dataframe_from_dict(data_dict_list=movie_conversation_list)\n",
        "movie_lines_df = dataframe_from_dict(data_dict_list=movie_lines_list)"
      ],
      "metadata": {
        "id": "GEGdcrAFam0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare genre dictionary\n",
        "genre_dict = get_genre_dict(movie_title_df=movie_title_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8e19NAzasQd",
        "outputId": "ff1eb08b-cb58-4df1-b488-78f03981f273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre dictionary prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dialogue dict for final dataset\n",
        "dialogue_dict, conversation_data_df = prepare_conversations(movie_lines_df=movie_lines_df, movie_conversation_df=movie_conversation_df, only_start=only_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q9lPdXia03A",
        "outputId": "0f18a21d-6db1-41c8-f290-e6bf779a5457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do cleaning of the text data\n",
        "conversation_data_df['input'] = conversation_data_df['input'].apply(clean_text)\n",
        "conversation_data_df['target'] = conversation_data_df['target'].apply(clean_text, add_tags=True)"
      ],
      "metadata": {
        "id": "-SP5oxjya5bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering data which are not in appropriate length\n",
        "filtered_conversation_df = filter_short_long(conversation_data_df=conversation_data_df, \n",
        "                                                        min_q_length=2, max_q_length=20, \n",
        "                                                        min_a_length=2, max_a_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAvHdAKEbKnI",
        "outputId": "234c2ce8-c790-49c9-b1e9-70d7f8caa7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33% filtered from original data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Vectorizer"
      ],
      "metadata": {
        "id": "7HzkZ6bJbnAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text vectorizer object\n",
        "Vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                output_mode=\"int\",\n",
        "                                output_sequence_length=max_length,\n",
        "                                standardize=None)"
      ],
      "metadata": {
        "id": "imiSJBHAbXKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the text vectorizer for the dataset\n",
        "Vectorizer.adapt(filtered_conversation_df['target'].to_numpy())"
      ],
      "metadata": {
        "id": "BlD4ITMTbtSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = Vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "exs1F0fPbyLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4_ocymL14vS",
        "outputId": "4ff34893-8cf9-4d74-85e5-17263663a131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare traing and test datasets from subset of data"
      ],
      "metadata": {
        "id": "-HdhtwIkc036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only the comedy movies data\n",
        "subset_movies_list = genre_dict[subset]\n",
        "\n",
        "subset_movie_line_df = filtered_conversation_df[filtered_conversation_df['movie_id'].isin(subset_movies_list)][:data_subset]"
      ],
      "metadata": {
        "id": "pIV_XNJJcT23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for datasets also remove the sentences with most unknown tokens\n",
        "training_data, testing_data = split_vectorize_filter_unk(conversation_data_df=subset_movie_line_df, Vectorizer=Vectorizer, test_split=test_split, seed=random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K4o4bvOd0ok",
        "outputId": "ea7af26e-e12c-4d9e-b53d-2768651cfc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data points: 36973\n",
            "Test data points: 9244\n",
            "5% filtered from training data points\n",
            "After unknown token filters training data points: 35071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing datasets\n",
        "train_inputs = np.array(training_data['input'])\n",
        "train_targets = np.array(training_data['target'])\n",
        "#train_vector_targets = tf.keras.utils.to_categorical(np.array(training_data['target_vectors']), max_vocab_length)\n",
        "train_vector_targets = tf.expand_dims(tf.constant(np.array(training_data['target_vectors'])), axis=-1)\n",
        "\n",
        "test_inputs = np.array(testing_data['input'])\n",
        "test_targets = np.array(testing_data['target'])\n",
        "#test_vector_targets = tf.keras.utils.to_categorical(np.array(testing_data['target_vectors']), max_vocab_length)\n",
        "test_vector_targets = tf.expand_dims(tf.constant(np.array(testing_data['target_vectors'])), axis=-1)"
      ],
      "metadata": {
        "id": "KdkMNBGleYXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataset for training and validation\n",
        "train_data_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
        "train_lables_dataset = tf.data.Dataset.from_tensor_slices(train_vector_targets)\n",
        "train_dataset = tf.data.Dataset.zip((train_data_dataset, train_lables_dataset))\n",
        "train_dataset = train_dataset.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_data_dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_targets))\n",
        "test_lables_dataset = tf.data.Dataset.from_tensor_slices(test_vector_targets)\n",
        "test_dataset = tf.data.Dataset.zip((test_data_dataset, test_lables_dataset))\n",
        "test_dataset = test_dataset.batch(128).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "3GMCmqe1ek3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare traing and test datasets from all available data"
      ],
      "metadata": {
        "id": "ryH64-D95uv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for datasets also remove the sentences with most unknown tokens\n",
        "training_data_all, testing_data_all = split_vectorize_filter_unk(conversation_data_df=filtered_conversation_df, Vectorizer=Vectorizer, test_split=test_split, seed=random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3g0Lwxs6Len",
        "outputId": "83f02901-b303-47aa-8572-88e0d0d9f3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data points: 118565\n",
            "Test data points: 29642\n",
            "4% filtered from training data points\n",
            "After unknown token filters training data points: 112655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing datasets\n",
        "train_inputs_all = np.array(training_data_all['input'])\n",
        "train_targets_all = np.array(training_data_all['target'])\n",
        "#train_vector_targets_all = tf.keras.utils.to_categorical(np.array(training_data_all['target_vectors']), max_vocab_length)\n",
        "train_vector_targets_all = tf.expand_dims(tf.constant(np.array(training_data_all['target_vectors'])), axis=-1)\n",
        "\n",
        "test_inputs_all = np.array(testing_data_all['input'])\n",
        "test_targets_all = np.array(testing_data_all['target'])\n",
        "#test_vector_targets_all = tf.keras.utils.to_categorical(np.array(testing_data_all['target_vectors']), max_vocab_length)\n",
        "test_vector_targets_all = tf.expand_dims(tf.constant(np.array(testing_data_all['target_vectors'])), axis=-1)"
      ],
      "metadata": {
        "id": "v7oQ9gz16Sjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataset for training and validation\n",
        "train_data_dataset_all = tf.data.Dataset.from_tensor_slices((train_inputs_all, train_targets_all))\n",
        "train_lables_dataset_all = tf.data.Dataset.from_tensor_slices(train_vector_targets_all)\n",
        "train_dataset_all = tf.data.Dataset.zip((train_data_dataset_all, train_lables_dataset_all))\n",
        "train_dataset_all = train_dataset_all.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_data_dataset_all = tf.data.Dataset.from_tensor_slices((test_inputs_all, test_targets_all))\n",
        "test_lables_dataset_all = tf.data.Dataset.from_tensor_slices(test_vector_targets_all)\n",
        "test_dataset_all = tf.data.Dataset.zip((test_data_dataset_all, test_lables_dataset_all))\n",
        "test_dataset_all = test_dataset_all.batch(128).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "D2qcdqzt6jzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "YeKD2Bp_YduR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "KTfvtk92YcVJ",
        "outputId": "1c598a07-0e2b-4b8b-e972-d391176349e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-85ea980a1e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfail_here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'fail_here' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation"
      ],
      "metadata": {
        "id": "NNT3ppX8ezDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 with single layer LSTM\n",
        "\n",
        "* Input for this model is string (question, answer first word(START_))\n",
        "* Output will be the probability of the next word\n",
        "* This model has single layer of LSTM Units"
      ],
      "metadata": {
        "id": "Xq8zCUfve1wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating embedding object for encoder and decoder models\n",
        "EncoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='encoder_embedding_layer')\n",
        "\n",
        "DecoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='decoder_embedding_layer')"
      ],
      "metadata": {
        "id": "A44HSmIfflsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vector = Vectorizer(EncoderInput)\n",
        "\n",
        "DecoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vector = Vectorizer(DecoderInput)"
      ],
      "metadata": {
        "id": "smxZFfyJevw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoder\n",
        "encoder_embeddings = EncoderEmbeddingLayer(encoder_vector)\n",
        "EncoderLstmLayer = layers.LSTM(lstm_units, return_state=True, name='Encoder_LSTM')\n",
        "encoder_lstm_outputs, state_h, state_c = EncoderLstmLayer(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "GPRBWfWAfmir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Decoder\n",
        "decoder_embeddings = DecoderEmbeddingLayer(decoder_vector)\n",
        "DecoderLstmLayer = layers.LSTM(lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
        "decoder_lstm_outputs, _, _ = DecoderLstmLayer(decoder_embeddings, initial_state=encoder_states)\n",
        "DecoderDenseLayer = layers.Dense(max_vocab_length, activation='softmax', name='Decoder_dense')\n",
        "decoder_dense_outputs = DecoderDenseLayer(decoder_lstm_outputs)\n",
        "\n",
        "EncDecModel = Model([EncoderInput, DecoderInput], decoder_dense_outputs)"
      ],
      "metadata": {
        "id": "rexQhskhf2Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LIki08OXgBJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel = tf.keras.Model(EncoderInput, encoder_states)\n",
        "\n",
        "DecoderStateInputH = layers.Input(shape=(lstm_units,))\n",
        "DecoderStateInputC = layers.Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [DecoderStateInputH, DecoderStateInputC]\n",
        "\n",
        "decoder_vector_test = Vectorizer(DecoderInput)\n",
        "dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "\n",
        "decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=decoder_states_inputs)\n",
        "decoder_states_test = [state_h_test, state_c_test]\n",
        "decoder_dense_outputs_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "\n",
        "DecModel = Model(\n",
        "    inputs = [DecoderInput, decoder_states_inputs],\n",
        "    outputs = [decoder_dense_outputs_test] + decoder_states_test)"
      ],
      "metadata": {
        "id": "Xz8NQ-0dgGv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model weights\n",
        "EncDecModel.load_weights('/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmComedy50epochs/EncDecModel1Weights')"
      ],
      "metadata": {
        "id": "O4ZUQ-dRq2qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fd5bb9-12c4-4c0d-8ec1-ffd62bf69035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f654b2576d0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history = EncDecModel.fit(train_dataset,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "DIMe9n8YgjRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibwjvuxJOz0v",
        "outputId": "c09b4566-7dd3-4c8c-cdd7-e4c231493594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 7s 39ms/step - loss: 2.5522 - accuracy: 0.2823\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.5522470474243164, 0.28226396441459656]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test = Vectorizer([input_word])\n",
        "    dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "    decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=states[0])\n",
        "    decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "    word_idx = tf.argmax(decoder_dense_output_test[0, 0, :]).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_test), tf.constant(state_c_test)]\n",
        "    return next_word, states"
      ],
      "metadata": {
        "id": "CQ6DhucSmqdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human = 'hello'\n",
        "states = [EncModel.predict([human])]\n",
        "next_word = 'START_'\n",
        "stop_condition = True\n",
        "bot_response = \"\"\n",
        "states[0] = [tf.constant(states[0][0]), tf.constant(states[0][1])]\n",
        "while stop_condition:\n",
        "    next_word, states = decoder_model_test(next_word, states, vocab_list)\n",
        "    bot_response += next_word + ' '\n",
        "    if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "        stop_condition = False\n",
        "print(bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSk2VNzBmoEw",
        "outputId": "c012a30c-f4ab-437a-ffb6-a49c2b8feb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello why are you _END \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = False\n",
        "while not end_convo:\n",
        "  # Getting the input from user\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = True\n",
        "  # Encoding the input\n",
        "  stat = EncModel.predict([human])\n",
        "  next_word = 'START_'\n",
        "  stop_condition = True\n",
        "  bot_response = \"\"\n",
        "  stat = [tf.constant(stat[0]), tf.constant(stat[1])]\n",
        "  while stop_condition:\n",
        "      # Decoder model operations starts here\n",
        "      decoder_vector_test = Vectorizer([next_word])\n",
        "      dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "      decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=stat)\n",
        "      decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "      # Decoder model operations end here\n",
        "      word_idx = tf.argmax(decoder_dense_output_test[0, 0, :]).numpy()\n",
        "      next_word = vocab_list[word_idx]\n",
        "      bot_response += next_word + ' '\n",
        "      if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "          stop_condition = False\n",
        "      stat = [state_h_test, state_c_test]\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoZJ3YgMgu_w",
        "outputId": "9638ee2f-7464-47de-b0d0-f706e2a4d8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: hi _END \n",
            "Human: what\n",
            "KATTA: one while you are _END \n",
            "Human: who are you\n",
            "KATTA: i am not lefthanded _END \n",
            "Human: are you a robot\n",
            "KATTA: oh no no no no no no no no no no no just watching the historical records yes _END \n",
            "Human: are you a human\n",
            "KATTA: if you are not sure i would rather not _END \n",
            "Human: why are you here\n",
            "KATTA: i do not know _END \n",
            "Human: i do not know i am sorry\n",
            "KATTA: do not puss out _END \n",
            "Human: i like you\n",
            "KATTA: i am not _END \n",
            "Human: END CONVO\n",
            "KATTA: [UNK] you are a great gal you would do it you would not understand that _END \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model_test, encoder_model=EncModel, input_text=human, clean_text=clean_text, multi_layer=False)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvHtMEK7O_lQ",
        "outputId": "0cb67bc2-aabe-4405-c3dc-ffb1321e8f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: hi \n",
            "Human: how are you buddy\n",
            "KATTA: how would you know \n",
            "Human: are you a robot\n",
            "KATTA: oh no no no no no no no no no no no just watching the historical records yes \n",
            "Human: END CONVO\n",
            "KATTA: that is the [UNK] that is right \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmComedy50epochs/EncDecModel1Weights')"
      ],
      "metadata": {
        "id": "Y3uUu9ZBs8go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.load_weights('/content/drive/MyDrive/Chatbot/model_weights_GPU/EncDecModelWeights')"
      ],
      "metadata": {
        "id": "aFPrgGNQtjdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "9JB_TWEPik-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "id": "vM2KcJ20in7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 with stacked lstm\n",
        "* Input for this model is string (question, answer first word(START_))\n",
        "* Output will be the probability of the next word\n",
        "* This model has double layer of lstm units"
      ],
      "metadata": {
        "id": "uAC2wqGxSrol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creatimg embedding objects for encoder and decoder models\n",
        "EncoderEmbeddingLayerM2 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='encoder_embedding_layer_model2')\n",
        "\n",
        "DecoderEmbeddingLayerM2 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='decoder_embedding_layer_model2')"
      ],
      "metadata": {
        "id": "IsJX5C2PSzgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInputM2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vectorM2 = Vectorizer(EncoderInputM2)\n",
        "\n",
        "DecoderInputM2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vectorM2 = Vectorizer(DecoderInputM2)"
      ],
      "metadata": {
        "id": "rXfLOL80T3Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder\n",
        "encoder_embeddings_M2 = EncoderEmbeddingLayerM2(encoder_vectorM2)\n",
        "\n",
        "EncoderLstmLayer1M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, return_sequences=True, name='Encoder_LSTM_layer1_model2')\n",
        "encoder_outputs_layer1_M2, state_h_layer1_M2, state_c_layer1_M2 = EncoderLstmLayer1M2(encoder_embeddings_M2)\n",
        "encoder_states_layer1_M2 = [state_h_layer1_M2, state_c_layer1_M2]\n",
        "\n",
        "EncoderLstmLayer2M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, name='Encoder_LSTM2_layer2_model2')\n",
        "encoder_outputs_layer2_M2, state_h_layer2_M2, state_c_layer2_M2 = EncoderLstmLayer2M2(encoder_outputs_layer1_M2)\n",
        "encoder_states_layer2_M2 = [state_h_layer2_M2, state_c_layer2_M2]"
      ],
      "metadata": {
        "id": "epn5qktMUmFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create decoder\n",
        "decoder_embeddings_M2 = DecoderEmbeddingLayerM2(decoder_vectorM2)\n",
        "\n",
        "DecoderLstmLayer1M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer1_model2')\n",
        "decoder_outputs_layer1_M2, _, _ = DecoderLstmLayer1M2(decoder_embeddings_M2, initial_state=encoder_states_layer1_M2)\n",
        "\n",
        "DecoderLstmLayer2M2 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer2_model2')\n",
        "decoder_outputs_layer2_M2, _, _ = DecoderLstmLayer2M2(decoder_outputs_layer1_M2, initial_state=encoder_states_layer2_M2)\n",
        "\n",
        "DecoderDenseLayerM2 = tf.keras.layers.Dense(max_vocab_length, activation='softmax', name='Decoder_Dense_layer_model2')\n",
        "decoder_dense_outputs_M2 = DecoderDenseLayerM2(decoder_outputs_layer2_M2)\n",
        "\n",
        "EncDecModel2 = tf.keras.Model([EncoderInputM2, DecoderInputM2], decoder_dense_outputs_M2)"
      ],
      "metadata": {
        "id": "r0UG_qPsWBUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel2.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UaAYzruZazeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel2 = tf.keras.Model(EncoderInputM2, [encoder_states_layer1_M2, encoder_states_layer2_M2])\n",
        "\n",
        "DecoderStateInputHLayer1M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer1M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer1_M2 = [DecoderStateInputHLayer1M2, DecoderStateInputCLayer1M2]\n",
        "\n",
        "DecoderStateInputHLayer2M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer2M2 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer2_M2 = [DecoderStateInputHLayer2M2, DecoderStateInputCLayer2M2]\n",
        "\n",
        "decoder_vector_test_M2 = Vectorizer(DecoderInputM2)\n",
        "dec_embedding_test_M2 = DecoderEmbeddingLayerM2(decoder_vector_test_M2)\n",
        "\n",
        "decoder_lstm_outputs_test_layer1_M2, state_h_test_layer1_M1, state_c_test_layer1_M2 = DecoderLstmLayer1M2(dec_embedding_test_M2, initial_state=decoder_states_inputs_layer1_M2)\n",
        "decoder_states_test_layer1_M2 = [state_h_test_layer1_M1, state_c_test_layer1_M2]\n",
        "\n",
        "decoder_lstm_outputs_test_layer2_M2, state_h2_test_layer2, state_c2_test_layer2 = DecoderLstmLayer2M2(decoder_lstm_outputs_test_layer1_M2, initial_state=decoder_states_inputs_layer2_M2)\n",
        "decoder_states_test_layer2_M2 = [state_h2_test_layer2, state_c2_test_layer2]\n",
        "\n",
        "decoder_dense_outputs_test_M2 = DecoderDenseLayerM2(decoder_lstm_outputs_test_layer2_M2)\n",
        "\n",
        "DecModel2 = tf.keras.Model(\n",
        "    inputs = [DecoderInputM2, [decoder_states_inputs_layer1_M2, decoder_states_inputs_layer2_M2]],\n",
        "    outputs = [decoder_dense_outputs_test_M2] + [decoder_states_test_layer1_M2, decoder_states_test_layer2_M2])"
      ],
      "metadata": {
        "id": "5wKwxQKFdj6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel2.load_weights('/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmComedy50epochs/EncDecModel2Weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAkbQnCcQqrU",
        "outputId": "3f6ba983-0044-4d43-e325-9375e4e73ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6522156ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history2 = EncDecModel2.fit(train_dataset,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "242D0ea6gWBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel.evaluate(test_dataset) # 28.23%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJd2rhDQ0il",
        "outputId": "0271edc3-8ae1-48a0-97c3-ca36482b613d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 3s 39ms/step - loss: 2.5522 - accuracy: 0.2823\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.5522470474243164, 0.28226396441459656]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model2_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test_M2 = Vectorizer([input_word])\n",
        "    dec_embedding_test_M2 = DecoderEmbeddingLayerM2(decoder_vector_test_M2)\n",
        "    decoder_lstm_outputs_test_layer1_M2, state_h_l1_M2, state_c_l1_M2 = DecoderLstmLayer1M2(dec_embedding_test_M2, initial_state=states[0])\n",
        "    decoder_lstm_outputs_test_layer2_M2, state_h_l2_M2, state_c_l2_M2 = DecoderLstmLayer2M2(decoder_lstm_outputs_test_layer1_M2, initial_state=states[1])\n",
        "    decoder_dense_outputs_test_M2 = DecoderDenseLayerM2(decoder_lstm_outputs_test_layer2_M2)\n",
        "    word_idx = tf.argmax(decoder_dense_outputs_test_M2[0, 0, :]).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_l1_M2), tf.constant(state_c_l1_M2)]\n",
        "    states[1] = [tf.constant(state_h_l2_M2), tf.constant(state_c_l2_M2)]\n",
        "    return next_word, states"
      ],
      "metadata": {
        "id": "q_P5lSH4ti80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  states = EncModel2.predict([human])\n",
        "  next_word = 'START_'\n",
        "  stop_condition = True\n",
        "  bot_response = \"\"\n",
        "  states[0] = [tf.constant(states[0][0]), tf.constant(states[0][1])]\n",
        "  states[1] = [tf.constant(states[1][0]), tf.constant(states[1][1])]\n",
        "  while stop_condition:\n",
        "      next_word, states = decoder_model2_test(next_word, states, vocab_list)\n",
        "      bot_response += next_word + ' '\n",
        "      if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "          stop_condition = False\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "id": "sULwRMubj1fN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f279fd5-8bc8-4ee5-ade9-c1521c68ad62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: hi _END \n",
            "Human: how are you\n",
            "KATTA: insane i am going to be a jets fan _END \n",
            "Human: END CONVO\n",
            "KATTA: no no no no i am not _END \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model2_test, encoder_model=EncModel2, input_text=human, clean_text=clean_text, multi_layer=True)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gghUYsOURCJ7",
        "outputId": "d6b78001-bb5a-4af6-ee02-7fd91ea2a089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: hi \n",
            "Human: how are you\n",
            "KATTA: insane i am going to be a jets fan \n",
            "Human: END CONVO\n",
            "KATTA: you know i have got a great age of enlightenment \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel2.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmComedy50epochs/EncDecModel2Weights')"
      ],
      "metadata": {
        "id": "tJ-ykPDjki6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "X-E9ebch5Dqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "id": "BVpaNwdg5FEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3 with single layer lstm (all data)\n",
        "\n",
        "* This model is same as Model 1 but trained with all available data\n",
        "* This model has single layer onf LSTM untis"
      ],
      "metadata": {
        "id": "kNN46XpZ4iEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating embedding object for encoder and decoder models\n",
        "EncoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='encoder_embedding_layer')\n",
        "\n",
        "DecoderEmbeddingLayer = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                output_dim=embedding_output_dimension, # 128\n",
        "                                input_length=max_length,\n",
        "                                mask_zero=True,\n",
        "                                name='decoder_embedding_layer')"
      ],
      "metadata": {
        "id": "R2uoU2Yg40ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vector = Vectorizer(EncoderInput)\n",
        "\n",
        "DecoderInput = layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vector = Vectorizer(DecoderInput)"
      ],
      "metadata": {
        "id": "2vw9peOC7QNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoder\n",
        "encoder_embeddings = EncoderEmbeddingLayer(encoder_vector)\n",
        "EncoderLstmLayer = layers.LSTM(lstm_units, return_state=True, name='Encoder_LSTM')\n",
        "encoder_lstm_outputs, state_h, state_c = EncoderLstmLayer(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "3_VgW0197WQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Decoder\n",
        "decoder_embeddings = DecoderEmbeddingLayer(decoder_vector)\n",
        "DecoderLstmLayer = layers.LSTM(lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
        "decoder_lstm_outputs, _, _ = DecoderLstmLayer(decoder_embeddings, initial_state=encoder_states)\n",
        "DecoderDenseLayer = layers.Dense(max_vocab_length, activation='softmax', name='Decoder_dense')\n",
        "decoder_dense_outputs = DecoderDenseLayer(decoder_lstm_outputs)\n",
        "\n",
        "EncDecModel = Model([EncoderInput, DecoderInput], decoder_dense_outputs)"
      ],
      "metadata": {
        "id": "r3bXeaKW7XJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "o09F1WPp7a82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel = tf.keras.Model(EncoderInput, encoder_states)\n",
        "\n",
        "DecoderStateInputH = layers.Input(shape=(lstm_units,))\n",
        "DecoderStateInputC = layers.Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [DecoderStateInputH, DecoderStateInputC]\n",
        "\n",
        "decoder_vector_test = Vectorizer(DecoderInput)\n",
        "dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "\n",
        "decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=decoder_states_inputs)\n",
        "decoder_states_test = [state_h_test, state_c_test]\n",
        "decoder_dense_outputs_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "\n",
        "DecModel = Model(\n",
        "    inputs = [DecoderInput, decoder_states_inputs],\n",
        "    outputs = [decoder_dense_outputs_test] + decoder_states_test)"
      ],
      "metadata": {
        "id": "CAnNcQwY7fM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel.load_weights('/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmAllData50epochs/EncDecModel3Weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS9BsycNRYbg",
        "outputId": "d9128e65-f269-4e46-dcfc-f4c6a245e803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f64e0372290>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history = EncDecModel.fit(train_dataset_all,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset_all)"
      ],
      "metadata": {
        "id": "b96FRK-U7m8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel.evaluate(test_dataset_all) # 28.27%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFKm_FzzRg9T",
        "outputId": "b7f697a7-732b-463a-ef01-46f61b92ceb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232/232 [==============================] - 13s 39ms/step - loss: 2.6889 - accuracy: 0.2827\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.688870429992676, 0.2827378511428833]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test = Vectorizer([input_word])\n",
        "    dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "    decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=states[0])\n",
        "    decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "    word_idx = tf.argmax(decoder_dense_output_test[0, 0, :]).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_test), tf.constant(state_c_test)]\n",
        "    return next_word, states"
      ],
      "metadata": {
        "id": "-1YyTTEc7otf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human = 'hello'\n",
        "states = [EncModel.predict([human])]\n",
        "next_word = 'START_'\n",
        "stop_condition = True\n",
        "bot_response = \"\"\n",
        "states[0] = [tf.constant(states[0][0]), tf.constant(states[0][1])]\n",
        "while stop_condition:\n",
        "    next_word, states = decoder_model_test(next_word, states, vocab_list)\n",
        "    bot_response += next_word + ' '\n",
        "    if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "        stop_condition = False\n",
        "print(bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oObG4H7L707F",
        "outputId": "721dd854-e928-44eb-d14b-d6cc8800b2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am sorry _END \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = False\n",
        "while not end_convo:\n",
        "  # Getting the input from user\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = True\n",
        "  # Encoding the input\n",
        "  stat = EncModel.predict([human])\n",
        "  next_word = 'START_'\n",
        "  stop_condition = True\n",
        "  bot_response = \"\"\n",
        "  while stop_condition:\n",
        "      stat = [tf.constant(stat[0]), tf.constant(stat[1])]\n",
        "      # Decoder model operations starts here\n",
        "      decoder_vector_test = Vectorizer([next_word])\n",
        "      dec_embedding_test = DecoderEmbeddingLayer(decoder_vector_test)\n",
        "      decoder_lstm_outputs_test, state_h_test, state_c_test = DecoderLstmLayer(dec_embedding_test, initial_state=stat)\n",
        "      decoder_dense_output_test = DecoderDenseLayer(decoder_lstm_outputs_test)\n",
        "      # Decoder model operations end here\n",
        "      word_idx = tf.argmax(decoder_dense_output_test[0, 0, :]).numpy()\n",
        "      next_word = vocab_list[word_idx]\n",
        "      bot_response += next_word + ' '\n",
        "      if next_word == '_END' or len(bot_response.split()) > max_length:\n",
        "          stop_condition = False\n",
        "      stat = [state_h_test, state_c_test]\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvFWaORm74U5",
        "outputId": "7571735c-7f16-4fd3-b432-6cceb61a1777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: i do not know if you are hopeless i will see you at all _END \n",
            "Human: how are you\n",
            "KATTA: [UNK] _END \n",
            "Human: why are you behaving so stupid\n",
            "KATTA: he wants to see you _END \n",
            "Human: okay bye\n",
            "KATTA: bye dad _END \n",
            "Human: good bye\n",
            "KATTA: bye _END \n",
            "Human: hello\n",
            "KATTA: i am sorry _END \n",
            "Human: END CONVO\n",
            "KATTA: have you seen him in the canary islands a message on the road _END \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model_test, encoder_model=EncModel, input_text=human, clean_text=clean_text, multi_layer=False)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxeylIugSCj4",
        "outputId": "3e723ee4-6a05-4b61-fba9-a4bf4a78c310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: i do not know if you are hopeless i will see you at all \n",
            "Human: hello\n",
            "KATTA: i am sorry \n",
            "Human: END CONVO\n",
            "KATTA: it you bastard \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/1LayerLstmAllData50epochs/EncDecModel3Weights')"
      ],
      "metadata": {
        "id": "1Xua1lbT755M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop execution"
      ],
      "metadata": {
        "id": "kxAh4W5H-vbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fail_here"
      ],
      "metadata": {
        "id": "0a6KP5Q9-xDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4 with dual layer lstm (all data)\n",
        "\n",
        "* This model is same as Model 2 but trained with all available data\n",
        "* This model has dual layer of LSTM untis"
      ],
      "metadata": {
        "id": "q1gJsScj-kAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creatimg embedding objects for encoder and decoder models\n",
        "EncoderEmbeddingLayerM4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='encoder_embedding_layer_model4')\n",
        "\n",
        "DecoderEmbeddingLayerM4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=embedding_output_dimension,\n",
        "                                     input_length=max_length,\n",
        "                                     mask_zero=True,\n",
        "                                     name='decoder_embedding_layer_model4')"
      ],
      "metadata": {
        "id": "Fc4L4l2Q-qVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder & decoder initial layers\n",
        "EncoderInputM4 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "encoder_vectorM4 = Vectorizer(EncoderInputM4)\n",
        "\n",
        "DecoderInputM4 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_vectorM4 = Vectorizer(DecoderInputM4)"
      ],
      "metadata": {
        "id": "Aa49TDFE-9LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder\n",
        "encoder_embeddings_M4 = EncoderEmbeddingLayerM4(encoder_vectorM4)\n",
        "\n",
        "EncoderLstmLayer1M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, return_sequences=True, name='Encoder_LSTM_layer1_model4')\n",
        "encoder_outputs_layer1_M4, state_h_layer1_M4, state_c_layer1_M4 = EncoderLstmLayer1M4(encoder_embeddings_M4)\n",
        "encoder_states_layer1_M4 = [state_h_layer1_M4, state_c_layer1_M4]\n",
        "\n",
        "EncoderLstmLayer2M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, name='Encoder_LSTM2_layer2_model4')\n",
        "encoder_outputs_layer2_M4, state_h_layer2_M4, state_c_layer2_M4 = EncoderLstmLayer2M4(encoder_outputs_layer1_M4)\n",
        "encoder_states_layer2_M4 = [state_h_layer2_M4, state_c_layer2_M4]"
      ],
      "metadata": {
        "id": "E-BoW4zU-99K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create decoder\n",
        "decoder_embeddings_M4 = DecoderEmbeddingLayerM4(decoder_vectorM4)\n",
        "\n",
        "DecoderLstmLayer1M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer1_model4')\n",
        "decoder_outputs_layer1_M4, _, _ = DecoderLstmLayer1M4(decoder_embeddings_M4, initial_state=encoder_states_layer1_M4)\n",
        "\n",
        "DecoderLstmLayer2M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer2_model4')\n",
        "decoder_outputs_layer2_M4, _, _ = DecoderLstmLayer2M4(decoder_outputs_layer1_M4, initial_state=encoder_states_layer2_M4)\n",
        "\n",
        "DecoderDenseLayerM4 = tf.keras.layers.Dense(max_vocab_length, activation='softmax', name='Decoder_Dense_layer_model4')\n",
        "decoder_dense_outputs_M4 = DecoderDenseLayerM4(decoder_outputs_layer2_M4)\n",
        "\n",
        "EncDecModel4 = tf.keras.Model([EncoderInputM4, DecoderInputM4], decoder_dense_outputs_M4)"
      ],
      "metadata": {
        "id": "clLW2uIE_BCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "EncDecModel4.compile(loss=sparse_loss_fun,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2jyCHvKi_HlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder at test time\n",
        "EncModel4 = tf.keras.Model(EncoderInputM4, [encoder_states_layer1_M4, encoder_states_layer2_M4])\n",
        "\n",
        "DecoderStateInputHLayer1M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer1M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer1_M4 = [DecoderStateInputHLayer1M4, DecoderStateInputCLayer1M4]\n",
        "\n",
        "DecoderStateInputHLayer2M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "DecoderStateInputCLayer2M4 = tf.keras.layers.Input(shape=(stacked_lstm_units,))\n",
        "decoder_states_inputs_layer2_M4 = [DecoderStateInputHLayer2M4, DecoderStateInputCLayer2M4]\n",
        "\n",
        "decoder_vector_test_M4 = Vectorizer(DecoderInputM4)\n",
        "dec_embedding_test_M4 = DecoderEmbeddingLayerM4(decoder_vector_test_M4)\n",
        "\n",
        "decoder_lstm_outputs_test_layer1_M4, state_h_test_layer1_M4, state_c_test_layer1_M4 = DecoderLstmLayer1M4(dec_embedding_test_M4, initial_state=decoder_states_inputs_layer1_M4)\n",
        "decoder_states_test_layer1_M4 = [state_h_test_layer1_M4, state_c_test_layer1_M4]\n",
        "\n",
        "decoder_lstm_outputs_test_layer2_M4, state_h2_test_layer2, state_c2_test_layer2 = DecoderLstmLayer2M4(decoder_lstm_outputs_test_layer1_M4, initial_state=decoder_states_inputs_layer2_M4)\n",
        "decoder_states_test_layer2_M4 = [state_h2_test_layer2, state_c2_test_layer2]\n",
        "\n",
        "decoder_dense_outputs_test_M4 = DecoderDenseLayerM4(decoder_lstm_outputs_test_layer2_M4)\n",
        "\n",
        "DecModel4 = tf.keras.Model(\n",
        "    inputs = [DecoderInputM4, [decoder_states_inputs_layer1_M4, decoder_states_inputs_layer2_M4]],\n",
        "    outputs = [decoder_dense_outputs_test_M4] + [decoder_states_test_layer1_M4, decoder_states_test_layer2_M4])"
      ],
      "metadata": {
        "id": "v1oig1X2_M9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel4.load_weights('/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmAllData50epochs/EncDecModel4Weights')"
      ],
      "metadata": {
        "id": "DE8K7vDfSUTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_history4 = EncDecModel4.fit(train_dataset_all,\n",
        "#                epochs=epoch,\n",
        "#                validation_data=test_dataset_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYuOcGKL_ROP",
        "outputId": "9f2eb155-fc10-4a4f-d7d3-ea1929253c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "881/881 [==============================] - 86s 79ms/step - loss: 2.4082 - accuracy: 0.1697 - val_loss: 2.2525 - val_accuracy: 0.2392\n",
            "Epoch 2/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 2.0733 - accuracy: 0.2710 - val_loss: 1.9508 - val_accuracy: 0.3003\n",
            "Epoch 3/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.8877 - accuracy: 0.3093 - val_loss: 1.8828 - val_accuracy: 0.3121\n",
            "Epoch 4/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.8252 - accuracy: 0.3178 - val_loss: 1.8519 - val_accuracy: 0.3175\n",
            "Epoch 5/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.7822 - accuracy: 0.3229 - val_loss: 1.8340 - val_accuracy: 0.3199\n",
            "Epoch 6/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.7465 - accuracy: 0.3275 - val_loss: 1.8234 - val_accuracy: 0.3215\n",
            "Epoch 7/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.7150 - accuracy: 0.3313 - val_loss: 1.8187 - val_accuracy: 0.3227\n",
            "Epoch 8/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.6859 - accuracy: 0.3347 - val_loss: 1.8184 - val_accuracy: 0.3236\n",
            "Epoch 9/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.6579 - accuracy: 0.3381 - val_loss: 1.8207 - val_accuracy: 0.3249\n",
            "Epoch 10/50\n",
            "881/881 [==============================] - 63s 71ms/step - loss: 1.6309 - accuracy: 0.3412 - val_loss: 1.8269 - val_accuracy: 0.3254\n",
            "Epoch 11/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.6043 - accuracy: 0.3444 - val_loss: 1.8357 - val_accuracy: 0.3253\n",
            "Epoch 12/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.5781 - accuracy: 0.3477 - val_loss: 1.8472 - val_accuracy: 0.3251\n",
            "Epoch 13/50\n",
            "881/881 [==============================] - 64s 72ms/step - loss: 1.5528 - accuracy: 0.3512 - val_loss: 1.8607 - val_accuracy: 0.3250\n",
            "Epoch 14/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.5283 - accuracy: 0.3550 - val_loss: 1.8755 - val_accuracy: 0.3252\n",
            "Epoch 15/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.5044 - accuracy: 0.3594 - val_loss: 1.8912 - val_accuracy: 0.3244\n",
            "Epoch 16/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.4815 - accuracy: 0.3638 - val_loss: 1.9082 - val_accuracy: 0.3237\n",
            "Epoch 17/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.4593 - accuracy: 0.3690 - val_loss: 1.9252 - val_accuracy: 0.3232\n",
            "Epoch 18/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.4380 - accuracy: 0.3741 - val_loss: 1.9415 - val_accuracy: 0.3230\n",
            "Epoch 19/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.4174 - accuracy: 0.3793 - val_loss: 1.9588 - val_accuracy: 0.3220\n",
            "Epoch 20/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.3973 - accuracy: 0.3846 - val_loss: 1.9752 - val_accuracy: 0.3210\n",
            "Epoch 21/50\n",
            "881/881 [==============================] - 63s 71ms/step - loss: 1.3782 - accuracy: 0.3899 - val_loss: 1.9926 - val_accuracy: 0.3196\n",
            "Epoch 22/50\n",
            "881/881 [==============================] - 64s 72ms/step - loss: 1.3599 - accuracy: 0.3950 - val_loss: 2.0094 - val_accuracy: 0.3188\n",
            "Epoch 23/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.3419 - accuracy: 0.4002 - val_loss: 2.0286 - val_accuracy: 0.3174\n",
            "Epoch 24/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.3246 - accuracy: 0.4053 - val_loss: 2.0479 - val_accuracy: 0.3164\n",
            "Epoch 25/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.3081 - accuracy: 0.4105 - val_loss: 2.0660 - val_accuracy: 0.3148\n",
            "Epoch 26/50\n",
            "881/881 [==============================] - 66s 74ms/step - loss: 1.2916 - accuracy: 0.4160 - val_loss: 2.0871 - val_accuracy: 0.3131\n",
            "Epoch 27/50\n",
            "881/881 [==============================] - 66s 74ms/step - loss: 1.2753 - accuracy: 0.4211 - val_loss: 2.1056 - val_accuracy: 0.3125\n",
            "Epoch 28/50\n",
            "881/881 [==============================] - 66s 74ms/step - loss: 1.2592 - accuracy: 0.4263 - val_loss: 2.1217 - val_accuracy: 0.3110\n",
            "Epoch 29/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.2432 - accuracy: 0.4315 - val_loss: 2.1354 - val_accuracy: 0.3104\n",
            "Epoch 30/50\n",
            "881/881 [==============================] - 66s 74ms/step - loss: 1.2273 - accuracy: 0.4368 - val_loss: 2.1509 - val_accuracy: 0.3102\n",
            "Epoch 31/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.2119 - accuracy: 0.4420 - val_loss: 2.1664 - val_accuracy: 0.3089\n",
            "Epoch 32/50\n",
            "881/881 [==============================] - 63s 72ms/step - loss: 1.1973 - accuracy: 0.4471 - val_loss: 2.1817 - val_accuracy: 0.3084\n",
            "Epoch 33/50\n",
            "881/881 [==============================] - 64s 72ms/step - loss: 1.1825 - accuracy: 0.4523 - val_loss: 2.1957 - val_accuracy: 0.3080\n",
            "Epoch 34/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.1683 - accuracy: 0.4573 - val_loss: 2.2118 - val_accuracy: 0.3060\n",
            "Epoch 35/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.1545 - accuracy: 0.4621 - val_loss: 2.2256 - val_accuracy: 0.3060\n",
            "Epoch 36/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.1409 - accuracy: 0.4671 - val_loss: 2.2417 - val_accuracy: 0.3048\n",
            "Epoch 37/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.1276 - accuracy: 0.4718 - val_loss: 2.2579 - val_accuracy: 0.3037\n",
            "Epoch 38/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.1140 - accuracy: 0.4768 - val_loss: 2.2727 - val_accuracy: 0.3022\n",
            "Epoch 39/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.1014 - accuracy: 0.4817 - val_loss: 2.2861 - val_accuracy: 0.3018\n",
            "Epoch 40/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.0891 - accuracy: 0.4861 - val_loss: 2.2997 - val_accuracy: 0.3002\n",
            "Epoch 41/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.0774 - accuracy: 0.4908 - val_loss: 2.3126 - val_accuracy: 0.2995\n",
            "Epoch 42/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.0655 - accuracy: 0.4951 - val_loss: 2.3261 - val_accuracy: 0.2991\n",
            "Epoch 43/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.0542 - accuracy: 0.4995 - val_loss: 2.3438 - val_accuracy: 0.2981\n",
            "Epoch 44/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.0437 - accuracy: 0.5034 - val_loss: 2.3619 - val_accuracy: 0.2984\n",
            "Epoch 45/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.0328 - accuracy: 0.5076 - val_loss: 2.3799 - val_accuracy: 0.2969\n",
            "Epoch 46/50\n",
            "881/881 [==============================] - 64s 73ms/step - loss: 1.0220 - accuracy: 0.5122 - val_loss: 2.4030 - val_accuracy: 0.2963\n",
            "Epoch 47/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 1.0113 - accuracy: 0.5166 - val_loss: 2.4240 - val_accuracy: 0.2949\n",
            "Epoch 48/50\n",
            "881/881 [==============================] - 65s 73ms/step - loss: 1.0016 - accuracy: 0.5198 - val_loss: 2.4468 - val_accuracy: 0.2922\n",
            "Epoch 49/50\n",
            "881/881 [==============================] - 66s 75ms/step - loss: 0.9919 - accuracy: 0.5238 - val_loss: 2.4670 - val_accuracy: 0.2911\n",
            "Epoch 50/50\n",
            "881/881 [==============================] - 65s 74ms/step - loss: 0.9827 - accuracy: 0.5273 - val_loss: 2.4871 - val_accuracy: 0.2913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EncDecModel4.evaluate(test_dataset_all) # 29.13%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBmlPaInSr42",
        "outputId": "eb58aa48-c665-49da-8baa-4ff3e8016c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232/232 [==============================] - 9s 39ms/step - loss: 2.4871 - accuracy: 0.2913\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4870564937591553, 0.29128652811050415]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model4_test(input_word: str, states: list, vocab_list: list):\n",
        "    decoder_vector_test_M4 = Vectorizer([input_word])\n",
        "    dec_embedding_test_M4 = DecoderEmbeddingLayerM4(decoder_vector_test_M4)\n",
        "    decoder_lstm_outputs_test_layer1_M4, state_h_l1_M4, state_c_l1_M4 = DecoderLstmLayer1M4(dec_embedding_test_M4, initial_state=states[0])\n",
        "    decoder_lstm_outputs_test_layer2_M4, state_h_l2_M4, state_c_l2_M4 = DecoderLstmLayer2M4(decoder_lstm_outputs_test_layer1_M4, initial_state=states[1])\n",
        "    decoder_dense_outputs_test_M4 = DecoderDenseLayerM4(decoder_lstm_outputs_test_layer2_M4)\n",
        "    word_idx = tf.argmax(decoder_dense_outputs_test_M4[0, 0, :]).numpy()\n",
        "    next_word = vocab_list[word_idx]\n",
        "    states[0] = [tf.constant(state_h_l1_M4), tf.constant(state_c_l1_M4)]\n",
        "    states[1] = [tf.constant(state_h_l2_M4), tf.constant(state_c_l2_M4)]\n",
        "    return next_word, states"
      ],
      "metadata": {
        "id": "6D-H_VOA_XJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_convo = True\n",
        "while end_convo:\n",
        "  human = input(\"Human: \")\n",
        "  if human == 'END CONVO':\n",
        "    end_convo = False\n",
        "  bot_response = make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model4_test, encoder_model=EncModel4, input_text=human, clean_text=clean_text, multi_layer=True)\n",
        "  print(\"KATTA:\", bot_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4lO-G7jL4zT",
        "outputId": "b1782041-098d-4336-d1ea-5f407dd2ad43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi\n",
            "KATTA: hello \n",
            "Human: why are you here\n",
            "KATTA: i do not know i am sorry \n",
            "Human: END CONVO\n",
            "KATTA: threatening me \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Human: hi\n",
        "KATTA: hello _END \n",
        "Human: what\n",
        "KATTA: do not let me go _END \n",
        "Human: aho are you\n",
        "KATTA: not you _END \n",
        "Human: who are you\n",
        "KATTA: gondorff asked me to meet you _END \n",
        "Human: are you a robot\n",
        "KATTA: no _END \n",
        "Human: are you a human\n",
        "KATTA: yes _END \n",
        "Human: why are you here\n",
        "KATTA: i do not know i am sorry _END \n",
        "Human: i like you\n",
        "KATTA: sure you have got to do is not that bad _END \n",
        "Human: do you like me\n",
        "KATTA: it was not me _END \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6ciEkDBBMwmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncDecModel4.save_weights(filepath='/content/drive/MyDrive/Chatbot/model_weights/2LayerLstmAllData50epochs/EncDecModel4Weights')"
      ],
      "metadata": {
        "id": "CJPov975SSbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}