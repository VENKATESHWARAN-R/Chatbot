{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "\n",
    "> Loading the libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the libraries\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print('Loading the libraries')\n",
    "print('...')\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required notebooks\n",
    "#import import_ipynb\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraires\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Libraries loaded \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the variables and objects \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Setting up the variables and objects \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the variable for preparing the model\n",
    "random_seed = 42\n",
    "embedding_output_dimension = 128\n",
    "lstm_units = 400\n",
    "stacked_lstm_units = 256\n",
    "sparse_loss_fun = 'sparse_categorical_crossentropy'\n",
    "one_hot_loss_fuc = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectorizer\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print('Preparing Vectorizer')\n",
    "print('...')\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved vectorizer\n",
    "Vectorizer_dump = pickle.load(open('dependencies/vectorizer.pkl', 'rb'))\n",
    "vectorizer_config = Vectorizer_dump['config']\n",
    "max_vocab_length = vectorizer_config['max_tokens']\n",
    "output_mode = vectorizer_config['output_mode']\n",
    "max_length = vectorizer_config['output_sequence_length']\n",
    "standardizer = vectorizer_config['standardize']\n",
    "#Vectorizer = TextVectorization.from_config(Vectorizer_dump['config'])\n",
    "# Prepare text vectorizer object\n",
    "# Following need to be ran in order to prepare the model. The from_config method won't work while creating model as it's not defined in runtime\n",
    "Vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                output_mode=\"int\",\n",
    "                                output_sequence_length=max_length,\n",
    "                                standardize=None)\n",
    "# Below line need to be included incase of older version of tensorflow\n",
    "Vectorizer.adapt(['xyz'])\n",
    "Vectorizer.set_weights(Vectorizer_dump['weights'])\n",
    "vocab_list = Vectorizer_dump['vocab_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer Prepared \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizer Prepared \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the model\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print('Generating the model')\n",
    "print('...')\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatimg embedding objects for encoder and decoder models\n",
    "EncoderEmbeddingLayerM4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=embedding_output_dimension,\n",
    "                                     input_length=max_length,\n",
    "                                     mask_zero=True,\n",
    "                                     name='encoder_embedding_layer_model4')\n",
    "\n",
    "DecoderEmbeddingLayerM4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=embedding_output_dimension,\n",
    "                                     input_length=max_length,\n",
    "                                     mask_zero=True,\n",
    "                                     name='decoder_embedding_layer_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder & decoder initial layers\n",
    "EncoderInputM4 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "encoder_vectorM4 = Vectorizer(EncoderInputM4)\n",
    "\n",
    "DecoderInputM4 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "decoder_vectorM4 = Vectorizer(DecoderInputM4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder\n",
    "encoder_embeddings_M4 = EncoderEmbeddingLayerM4(encoder_vectorM4)\n",
    "\n",
    "EncoderLstmLayer1M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, return_sequences=True, name='Encoder_LSTM_layer1_model4')\n",
    "encoder_outputs_layer1_M4, state_h_layer1_M4, state_c_layer1_M4 = EncoderLstmLayer1M4(encoder_embeddings_M4)\n",
    "encoder_states_layer1_M4 = [state_h_layer1_M4, state_c_layer1_M4]\n",
    "\n",
    "EncoderLstmLayer2M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_state=True, name='Encoder_LSTM2_layer2_model4')\n",
    "encoder_outputs_layer2_M4, state_h_layer2_M4, state_c_layer2_M4 = EncoderLstmLayer2M4(encoder_outputs_layer1_M4)\n",
    "encoder_states_layer2_M4 = [state_h_layer2_M4, state_c_layer2_M4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decoder\n",
    "decoder_embeddings_M4 = DecoderEmbeddingLayerM4(decoder_vectorM4)\n",
    "\n",
    "DecoderLstmLayer1M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer1_model4')\n",
    "decoder_outputs_layer1_M4, _, _ = DecoderLstmLayer1M4(decoder_embeddings_M4, initial_state=encoder_states_layer1_M4)\n",
    "\n",
    "DecoderLstmLayer2M4 = tf.keras.layers.LSTM(stacked_lstm_units, return_sequences=True, return_state=True, name='Decoder_LSTM2_layer2_model4')\n",
    "decoder_outputs_layer2_M4, _, _ = DecoderLstmLayer2M4(decoder_outputs_layer1_M4, initial_state=encoder_states_layer2_M4)\n",
    "\n",
    "DecoderDenseLayerM4 = tf.keras.layers.Dense(max_vocab_length, activation='softmax', name='Decoder_Dense_layer_model4')\n",
    "decoder_dense_outputs_M4 = DecoderDenseLayerM4(decoder_outputs_layer2_M4)\n",
    "\n",
    "EncDecModel4 = tf.keras.Model([EncoderInputM4, DecoderInputM4], decoder_dense_outputs_M4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "EncDecModel4.compile(loss=sparse_loss_fun,\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder at test time\n",
    "EncModel4 = tf.keras.Model(EncoderInputM4, [encoder_states_layer1_M4, encoder_states_layer2_M4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model creation done! \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the weights to the model\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print('Loading the weights to the model')\n",
    "print('...')\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x24b83f4aa30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model weights from best model\n",
    "EncDecModel4.load_weights('model_weights/2LayerLstmAllData50epochs/EncDecModel4Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are loaded into the model suceessfully \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Weights are loaded into the model suceessfully \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder function during runtime\n",
    "def decoder_model4_test(input_word: str, states: list, vocab_list: list):\n",
    "    decoder_vector_test_M4 = Vectorizer([input_word])\n",
    "    dec_embedding_test_M4 = DecoderEmbeddingLayerM4(decoder_vector_test_M4)\n",
    "    decoder_lstm_outputs_test_layer1_M4, state_h_l1_M4, state_c_l1_M4 = DecoderLstmLayer1M4(dec_embedding_test_M4, initial_state=states[0])\n",
    "    decoder_lstm_outputs_test_layer2_M4, state_h_l2_M4, state_c_l2_M4 = DecoderLstmLayer2M4(decoder_lstm_outputs_test_layer1_M4, initial_state=states[1])\n",
    "    decoder_dense_outputs_test_M4 = DecoderDenseLayerM4(decoder_lstm_outputs_test_layer2_M4)\n",
    "    decoder_output = decoder_dense_outputs_test_M4[0, 0, :].numpy()\n",
    "    word_idx = tf.argmax(decoder_output).numpy()\n",
    "    next_word = vocab_list[word_idx]\n",
    "    states[0] = [tf.constant(state_h_l1_M4), tf.constant(state_c_l1_M4)]\n",
    "    states[1] = [tf.constant(state_h_l2_M4), tf.constant(state_c_l2_M4)]\n",
    "    return decoder_output, next_word, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is ready\n",
      "You can start talking with KATTA\n",
      "For ending the conversation with KATTA use the keyword \"END_CONVO\"\n"
     ]
    }
   ],
   "source": [
    "print('Everything is ready')\n",
    "print('You can start talking with KATTA')\n",
    "print('For ending the conversation with KATTA use the keyword \"END_CONVO\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  hi\n",
      "KATTA: hello \n",
      "Human:  how are you\n",
      "KATTA: all right \n",
      "Human:  END CONVO\n",
      "KATTA: smiling \n"
     ]
    }
   ],
   "source": [
    "end_convo = True\n",
    "while end_convo:\n",
    "  human = input(\"Human: \")\n",
    "  print('Human: ', human)\n",
    "  if human == 'END_CONVO':\n",
    "    end_convo = False\n",
    "  bot_response = data_utils.make_prediction(vocab_list=vocab_list, decoder_model_function=decoder_model4_test, \n",
    "                                            encoder_model=EncModel4, input_text=human, clean_text=data_utils.clean_text, multi_layer=True,\n",
    "                                            go_beam=True)\n",
    "  print(\"KATTA:\", bot_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHuman:  hi\\nKATTA: hello \\nHuman:  hi\\nKATTA: hello \\nHuman:  hi\\nKATTA: hello \\nHuman:  hi\\nKATTA: hello what do you want \\nHuman:  how are you\\nKATTA: all right \\nHuman:  how are you\\nKATTA: okay \\nHuman:  END CONVO\\nKATTA: threatening me \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Human:  hi\n",
    "KATTA: hello \n",
    "Human:  hi\n",
    "KATTA: hello \n",
    "Human:  hi\n",
    "KATTA: hello \n",
    "Human:  hi\n",
    "KATTA: hello what do you want \n",
    "Human:  how are you\n",
    "KATTA: all right \n",
    "Human:  how are you\n",
    "KATTA: okay \n",
    "Human:  END CONVO\n",
    "KATTA: threatening me \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e3e6d922a4a2b8bdfb835fcfd6c913bd8c71bd3fc5f1ecc522edf4d040dd891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
